# Generated by o3-mini-high
# https://gist.github.com/chunhualiao/f90c48a0bdac24ba686c25c86150cca8
import math
import random
import logging
from typing import List, Dict, Optional
import openai
from openai import OpenAI
import os
import datetime
from fastapi import FastAPI, HTTPException, responses
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn
import yaml

################################################################################
# Utility Functions
################################################################################

import time

# Configure logging
def load_config(config_path: str) -> Dict:
    """Loads the configuration from the specified YAML file."""
    try:
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
            # Convert logging level string to actual level
            config["logging_level"] = getattr(logging, config["logging_level"].upper(), logging.INFO)
        return config
    except FileNotFoundError:
        print(f"Error: Configuration file not found at {config_path}")
        exit(1)
    except yaml.YAMLError as e:
        print(f"Error parsing YAML in {config_path}: {e}")
        exit(1)
    except AttributeError as e:
        print("Error: Invalid logging level in config file")
        exit(1)
    except KeyError as e:
        print(f"Error: Missing key in config file: {e}")
        exit(1)


def setup_logger(log_filename):
    logger = logging.getLogger(log_filename)  # Create a logger with the filename
    logger.setLevel(config["logging_level"])
    formatter = logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s")

    # Remove existing handlers to avoid duplicate logs
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    file_handler = logging.FileHandler(f"{config['log_file_name']}_{log_filename}")
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    return logger

# Load configuration at the start
config = load_config("config.yaml")

def call_llm(prompt: str, temperature: float = 0.7) -> str:
    """
    Calls an LLM via the OpenRouter API and returns the response.

    Args:
        prompt (str): The input prompt for the LLM.
        temperature (float, optional): The temperature setting for the LLM. Defaults to 0.7.

    Returns:
        str: The LLM's response.

    Args:
        prompt (str): The input prompt for the LLM.

    Returns:
        str: The LLM's response.
    """
    client = OpenAI(
        base_url=config["openrouter_base_url"],
        api_key=os.getenv("OPENROUTER_API_KEY"),
    )

    try:
        completion = client.chat.completions.create(
            model=config["llm_model"],
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,  # Pass temperature to the API call
        )
    except Exception as e:
        retries = config.get("max_retries", 3)
        delay = config.get("initial_retry_delay", 1)  # seconds

        if "Rate limit exceeded" in str(e):
            logger.warning(f"Rate limit exceeded: {e}")
            error_message = f"Rate limit exceeded: {e}"
        else:
            logger.error(f"API call failed with exception: {e}")
            error_message = f"API call failed with exception: {e}"

        for attempt in range(retries):
            try:
                wait_time = delay * (2 ** attempt)  # Exponential backoff
                logger.info(f"Retrying in {wait_time} seconds (attempt {attempt + 1}/{retries})")
                time.sleep(wait_time)
                completion = client.chat.completions.create(
                    model=config["llm_model"],
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,  # Pass temperature to the API call
                )
                if completion.choices and len(completion.choices) > 0:
                    return completion.choices[0].message.content
            except Exception as inner_e:
                if "Rate limit exceeded" in str(inner_e):
                    logger.warning(f"Rate limit exceeded (retry attempt {attempt + 1}): {inner_e}")
                    error_message = f"Rate limit exceeded: {inner_e}"
                else:
                    logger.error(f"API call failed with exception (retry attempt {attempt + 1}): {inner_e}")
                    error_message = f"API call failed with exception: {inner_e}"

                if attempt == retries - 1:
                    logger.error("Max retries reached. Giving up.")
                    return f"API call failed after multiple retries. Error: {error_message}" # Detailed error

        logger.error("Max retries reached without a successful response.")
        return f"API call failed after multiple retries. Error: {error_message}"  # Detailed error

    # If no exception, you can safely access attributes
    if completion.choices and len(completion.choices) > 0:
        return completion.choices[0].message.content
    else:
        logger.error("No choices in the response: %s", completion)
        return f"No choices in the response: {completion}"



###############################################################################
# Data Models and Pydantic Schemas
###############################################################################

class Hypothesis:
    def __init__(self, hypothesis_id: str, title: str, text: str):
        self.hypothesis_id = hypothesis_id
        self.title = title
        self.text = text
        self.novelty_review: Optional[str] = None   # "HIGH", "MEDIUM", "LOW"
        self.feasibility_review: Optional[str] = None
        self.elo_score: float = 1200.0      # initial Elo score
        self.review_comments: List[str] = []
        self.references: List[str] = []
        self.is_active: bool = True
        self.parent_ids: List[str] = []  # Store IDs of parent hypotheses

    def to_dict(self) -> dict:
        return {
            "id": self.hypothesis_id,
            "title": self.title,
            "text": self.text,
            "novelty_review": self.novelty_review,
            "feasibility_review": self.feasibility_review,
            "elo_score": self.elo_score,
            "review_comments": self.review_comments,
            "references": self.references,
            "is_active": self.is_active,
            "parent_ids": self.parent_ids,  # Include parent IDs
        }

class ResearchGoal:
    def __init__(self, description: str, constraints: Dict = None):
        self.description = description
        self.constraints = constraints if constraints else {}

class ContextMemory:
    """
    A simple in-memory context storage.
    """
    def __init__(self):
        self.hypotheses: Dict[str, Hypothesis] = {}  # key: hypothesis_id
        self.tournament_results: List[Dict] = []
        self.meta_review_feedback: List[Dict] = []
        self.iteration_number: int = 0

    def add_hypothesis(self, hypothesis: Hypothesis):
        self.hypotheses[hypothesis.hypothesis_id] = hypothesis
        logger.info(f"Added hypothesis {hypothesis.hypothesis_id}")

    def get_active_hypotheses(self) -> List[Hypothesis]:
        return [h for h in self.hypotheses.values() if h.is_active]


# Pydantic schemas for API endpoints.
class ResearchGoalRequest(BaseModel):
    description: str
    constraints: Optional[Dict] = {}

class HypothesisResponse(BaseModel):
    id: str
    title: str
    text: str
    novelty_review: Optional[str]
    feasibility_review: Optional[str]
    elo_score: float
    review_comments: List[str]
    references: List[str]
    is_active: bool

class OverviewResponse(BaseModel):
    iteration: int
    meta_review_critique: List[str]
    top_hypotheses: List[HypothesisResponse]
    suggested_next_steps: List[str]


###############################################################################
# Utility Functions (Placeholders for LLM Calls and Similarity Measures)
###############################################################################

def generate_unique_id(prefix="H") -> str:
    """
    Generates a unique identifier string.

    Args:
        prefix (str, optional): A prefix for the ID. Defaults to "H".

    Returns:
        str: A unique identifier string consisting of the prefix and a random 4-digit number.
    """
    return f"{prefix}{random.randint(1000, 9999)}"

import json

# --- VIS.JS INTEGRATION ---
def generate_visjs_graph(adjacency_graph: Dict) -> str:
    """
    Generates HTML and JavaScript code for a vis.js graph.

    Args:
        adjacency_graph (Dict): The adjacency graph data.

    Returns:
        str: A string containing the HTML and JavaScript code to embed the graph.
    """
    nodes = []
    edges = []

    for node_id, connections in adjacency_graph.items():
        nodes.append(f"{{id: '{node_id}', label: '{node_id}'}}")
        for connection in connections:
            if connection['similarity'] > 0.2:
                edges.append(f"{{from: '{node_id}', to: '{connection['other_id']}', label: '{connection['similarity']:.2f}', arrows: 'to'}}")

    nodes_str = ",\n".join(nodes)
    edges_str = ",\n".join(edges)

    return f"""
        <div id="mynetwork"></div>
        <p>
            <b>How to read the graph:</b><br>
            - Each node (circle) represents an item.<br>
            - Lines (edges) between nodes indicate a relationship.<br>
            - The number on each edge represents the similarity score between the connected nodes. Higher numbers mean greater similarity. Only similarities above 0.2 are shown.<br>
        </p>
        <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
        <script type="text/javascript">
            var nodes = new vis.DataSet([
                {nodes_str}
            ]);
            var edges = new vis.DataSet([
                {edges_str}
            ]);
            var container = document.getElementById('mynetwork');
            var data = {{
                nodes: nodes,
                edges: edges
            }};
            var options = {{
              edges: {{
                smooth: {{
                  enabled: true,
                  type: "dynamic",
                }},
              }},
            }};
            var network = new vis.Network(container, data, options);
        </script>
    """

def call_llm_for_generation(prompt: str, num_hypotheses: int = 3) -> List[Dict]:
    """
    Calls a Large Language Model (LLM) for generating hypotheses.

    Args:
        prompt (str): The input prompt for the LLM.
        num_hypotheses (int, optional): The number of hypotheses to generate. Defaults to 3.

    Returns:
        List[Dict]: A list of dictionaries, each representing a generated hypothesis.
                    Each dictionary contains "title" and "text" keys.
    """
    logger.info("LLM generation called with prompt: %s, num_hypotheses: %d", prompt, num_hypotheses)

    # Modify the prompt to request JSON output
    prompt += "\n\nPlease return the response as a JSON array of objects, where each object has a 'title' and 'text' key."

    # Call LLM with the appropriate temperature
    response = call_llm(prompt, temperature=config["step_temperatures"]["generation"])
    logger.info("LLM response: %s", response)

    if "API call failed" in response:
        # If the call failed, log it and return the error message
        logger.error(f"LLM call failed: {response}")
        return [{"title": "Error", "text": response}]  # Return error as a hypothesis

    try:
        # Remove potential Markdown code block formatting
        response = response.strip()
        if response.startswith("```json"):
            response = response[7:]
        if response.endswith("```"):
            response = response[:-3]
        response = response.strip()

        # Attempt to parse the response as JSON
        hypotheses = json.loads(response)
        logger.info("Parsed hypotheses: %s", hypotheses)

        # Basic validation: Check if the response is a list and each item has 'title' and 'text'
        if not isinstance(hypotheses, list) or not all(isinstance(h, dict) and "title" in h and "text" in h for h in hypotheses):
            error_message = "Invalid JSON format: Expected a list of objects with 'title' and 'text' keys."
            raise ValueError(error_message)
    except (json.JSONDecodeError, ValueError) as e:
        logger.error("Could not parse LLM response as JSON: %s", response)
        logger.error(f"Error: {e}")
        return [{"title": "Error", "text": f"Could not parse LLM response: {e}"}]  # Return error as a hypothesis

    return hypotheses

def call_llm_for_reflection(hypothesis_text: str) -> Dict:
    """
    Calls a Large Language Model (LLM) for reviewing a hypothesis.

    Args:
        hypothesis_text (str): The text of the hypothesis to be reviewed.

    Returns:
        Dict: A dictionary containing the review results, including novelty and feasibility
              assessments (HIGH, MEDIUM, or LOW), a comment, and a list of references.
    """
    prompt = (
        f"Review the following hypothesis and provide a novelty assessment (HIGH, MEDIUM, or LOW), "
        f"a feasibility assessment (HIGH, MEDIUM, or LOW), a comment, and a list of references (PMIDs) in JSON format:\n\n"
        f"Hypothesis: {hypothesis_text}\n\n"
        f"Return the response as a JSON object with the following keys: 'novelty_review', 'feasibility_review', 'comment', 'references'."

    )
    # Call LLM with the appropriate temperature
    response = call_llm(prompt, temperature=config["step_temperatures"]["reflection"])
    logger.info("LLM reflection for hypothesis: %s, response: %s", hypothesis_text, response)

    if "API call failed" in response:
        # If the call failed, log it and return the error message
        logger.error(f"LLM call failed: {response}")
        return {
            "novelty_review": "ERROR",
            "feasibility_review": "ERROR",
            "comment": response,  # Return the error message
            "references": [],
        }

    # Initialize default values
    novelty_review = "MEDIUM"
    feasibility_review = "MEDIUM"
    comment = "Could not parse LLM response."
    references = []

    try:
        # Remove potential Markdown code block formatting
        response = response.strip()
        if response.startswith("```json"):
            response = response[7:]
        if response.endswith("```"):
            response = response[:-3]
        response = response.strip()

        # Parse the JSON response
        data = json.loads(response)
        novelty_review = data.get("novelty_review", "MEDIUM")
        feasibility_review = data.get("feasibility_review", "MEDIUM")
        comment = data.get("comment", "Could not parse LLM response.")
        references = data.get("references", [])

        # Basic validation of review values
        if not any(level in novelty_review.upper() for level in ["HIGH", "MEDIUM", "LOW"]):
            logger.warning("Invalid novelty review value: %s", novelty_review)
            novelty_review = "MEDIUM"
        if not any(level in feasibility_review.upper() for level in ["HIGH", "MEDIUM", "LOW"]):
            logger.warning("Invalid feasibility review value: %s", feasibility_review)
            feasibility_review = "MEDIUM"
        if not isinstance(comment, str):
            logger.warning("Invalid comment value: %s", comment)
            comment = "Could not parse LLM response."

    except (json.JSONDecodeError, AttributeError, KeyError) as e:
        logger.warning("Error parsing LLM response: %s", e)
        logger.warning("Response: %s", response)
        comment = f"Could not parse LLM response: {e}"

    return {
        "novelty_review": novelty_review,
        "feasibility_review": feasibility_review,
        "comment": comment,
        "references": references,
    }

def run_pairwise_debate(hypoA: Hypothesis, hypoB: Hypothesis) -> Hypothesis:
    """
    Compares two hypotheses based on their novelty and feasibility review scores.

    Args:
        hypoA (Hypothesis): The first hypothesis.
        hypoB (Hypothesis): The second hypothesis.

    Returns:
        Hypothesis: The winning hypothesis. If scores are tied, a winner is chosen randomly.
    """
    def score(h: Hypothesis) -> int:
        """
        Calculates a numerical score for a hypothesis based on its novelty and feasibility reviews.

        Args:
            h (Hypothesis): The hypothesis to score.

        Returns:
            int: The calculated score.  HIGH=3, MEDIUM=2, LOW=1, None=0.  The score is the sum of
                 the novelty and feasibility scores.
        """
        mapping = {"HIGH": 3, "MEDIUM": 2, "LOW": 1, None: 0}
        score_novelty = 0
        if isinstance(h.novelty_review, str):
            score_novelty = mapping.get(h.novelty_review, 0)
        else:
            logger.error(f"Invalid novelty_review type: {type(h.novelty_review)}, value: {h.novelty_review}")

        score_feasibility = 0
        if isinstance(h.feasibility_review, str):
            score_feasibility = mapping.get(h.feasibility_review, 0)
        else:
            logger.error(f"Invalid feasibility_review type: {type(h.feasibility_review)}, value: {h.feasibility_review}")

        return score_novelty + score_feasibility
    scoreA = score(hypoA)
    scoreB = score(hypoB)
    winner = hypoA if scoreA > scoreB else hypoB if scoreB > scoreA else random.choice([hypoA, hypoB])
    logger.info("Debate: %s (score %d) vs %s (score %d) => Winner: %s",
                hypoA.hypothesis_id, scoreA, hypoB.hypothesis_id, scoreB, winner.hypothesis_id)
    return winner

def update_elo(winner: Hypothesis, loser: Hypothesis, k_factor: int = config["elo_k_factor"]):
    """
    Updates the Elo scores of two hypotheses after a pairwise comparison.

    Args:
        winner (Hypothesis): The winning hypothesis.
        loser (Hypothesis): The losing hypothesis.
        k_factor (int, optional): The K-factor used in the Elo calculation. Defaults to 32.

    Returns:
        None
    """
    ratingA = winner.elo_score
    ratingB = loser.elo_score
    expectedA = 1 / (1 + math.pow(10, (ratingB - ratingA) / 400))
    expectedB = 1 - expectedA
    winner.elo_score = ratingA + k_factor * (1 - expectedA)
    loser.elo_score = ratingB + k_factor * (0 - expectedB)
    logger.info("Updated Elo: Winner %s -> %.2f, Loser %s -> %.2f",
                winner.hypothesis_id, winner.elo_score, loser.hypothesis_id, loser.elo_score)

def combine_hypotheses(hypoA: Hypothesis, hypoB: Hypothesis) -> Hypothesis:
    """
    Combines two hypotheses into a new, evolved hypothesis.

    Args:
        hypoA (Hypothesis): The first hypothesis.
        hypoB (Hypothesis): The second hypothesis.

    Returns:
        Hypothesis: A new hypothesis combining the two input hypotheses. The new ID is prefixed with "E".
    """
    new_id = generate_unique_id("E")
    combined_title = f"Combined: {hypoA.title} & {hypoB.title}"
    combined_text = f"{hypoA.text}\n\nAdditionally, {hypoB.text}"
    logger.info("Combined hypotheses %s and %s into %s", hypoA.hypothesis_id, hypoB.hypothesis_id, new_id)
    new_hypothesis = Hypothesis(new_id, combined_title, combined_text)
    new_hypothesis.parent_ids = [hypoA.hypothesis_id, hypoB.hypothesis_id]  # Store parent IDs
    logger.info("New hypothesis parent_ids: %s", new_hypothesis.parent_ids) # Added logging
    return new_hypothesis

# Global variable to store the sentence transformer model
_sentence_transformer_model = None

def get_sentence_transformer_model():
    """
    Returns a singleton instance of the sentence transformer model.
    Loads the model only once to improve performance.
    
    Returns:
        SentenceTransformer: The sentence transformer model.
    """
    global _sentence_transformer_model
    if _sentence_transformer_model is None:
        try:
            from sentence_transformers import SentenceTransformer
            logger.info("Loading sentence transformer model...")
            # Using a smaller model for efficiency, can be replaced with larger models for better accuracy
            _sentence_transformer_model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("Sentence transformer model loaded successfully")
        except ImportError as e:
            logger.error(f"Failed to import sentence_transformers: {e}")
            raise
        except Exception as e:
            logger.error(f"Failed to load sentence transformer model: {e}")
            raise
    return _sentence_transformer_model

def similarity_score(textA: str, textB: str) -> float:
    """
    Calculates a similarity score between two text strings using sentence embeddings
    and cosine similarity.

    Args:
        textA (str): The first text string.
        textB (str): The second text string.

    Returns:
        float: A similarity score between 0 and 1 (inclusive), where 1 indicates
              identical semantic meaning and 0 indicates completely different meanings.
    """
    try:
        # Handle empty strings
        if not textA.strip() or not textB.strip():
            logger.warning("Empty string provided to similarity_score")
            return 0.0
            
        # Get the model
        model = get_sentence_transformer_model()
        
        # Generate embeddings
        embedding_a = model.encode(textA, convert_to_tensor=True)
        embedding_b = model.encode(textB, convert_to_tensor=True)
        
        # Calculate cosine similarity
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        
        # Convert to numpy arrays if they're tensors
        if hasattr(embedding_a, 'cpu') and hasattr(embedding_b, 'cpu'):
            embedding_a = embedding_a.cpu().numpy().reshape(1, -1)
            embedding_b = embedding_b.cpu().numpy().reshape(1, -1)
        
        similarity = cosine_similarity(embedding_a, embedding_b)[0][0]
        
        # Ensure the result is between 0 and 1
        similarity = float(max(0.0, min(1.0, similarity)))
        
        logger.info(f"Similarity score between texts: {similarity:.4f}")
        return similarity
    except Exception as e:
        logger.error(f"Error calculating similarity score: {e}")
        # Fallback to a default value in case of error
        return 0.5


###############################################################################
# Agent Implementations
###############################################################################

class GenerationAgent:
    def generate_new_hypotheses(self, research_goal: ResearchGoal, context: ContextMemory) -> List[Hypothesis]:
        """
        Generates new hypotheses based on the given research goal and context.

        Args:
            research_goal (ResearchGoal): The research goal.
            context (ContextMemory): The current context memory.

        Returns:
            List[Hypothesis]: A list of newly generated hypotheses.
        """
        prompt = (
            f"Research Goal: {research_goal.description}\n"
            f"Constraints: {research_goal.constraints}\n"
            f"Please propose {config['num_hypotheses']} new hypotheses with rationale.\n"
        )
        raw_output = call_llm_for_generation(prompt, num_hypotheses=config["num_hypotheses"])
        new_hypos = []
        for idea in raw_output:
            hypo_id = generate_unique_id("G")
            h = Hypothesis(hypo_id, idea["title"], idea["text"])
            logger.info("Generated hypothesis: %s", h.to_dict())
            new_hypos.append(h)
        return new_hypos

class ReflectionAgent:
    def review_hypotheses(self, hypotheses: List[Hypothesis], context: ContextMemory) -> None:
        """
        Reviews a list of hypotheses, updating their novelty, feasibility, comments, and references.

        Args:
            hypotheses (List[Hypothesis]): The list of hypotheses to review.
            context (ContextMemory): The current context memory.

        Returns:
            None
        """
        for h in hypotheses:
            result = call_llm_for_reflection(h.text)
            h.novelty_review = result["novelty_review"]
            h.feasibility_review = result["feasibility_review"]
            h.review_comments.append(result["comment"])
            h.references.extend(result["references"])
            logger.info("Reviewed hypothesis: %s, Novelty: %s, Feasibility: %s", h.hypothesis_id, h.novelty_review, h.feasibility_review)

class RankingAgent:
    def run_tournament(self, hypotheses: List[Hypothesis], context: ContextMemory) -> None:
        """
        Runs a tournament among the given hypotheses, updating their Elo scores and recording results.

        Args:
            hypotheses (List[Hypothesis]): The list of hypotheses to participate in the tournament.
            context (ContextMemory): The current context memory.

        Returns:
            None
        """
        random.shuffle(hypotheses)
        pairs = []
        for i in range(len(hypotheses)):
            for j in range(i + 1, len(hypotheses)):
                pairs.append((hypotheses[i], hypotheses[j]))
        for hA, hB in pairs:
            if hA.is_active and hB.is_active:
                winner = run_pairwise_debate(hA, hB)
                loser = hB if winner == hA else hA
                update_elo(winner, loser)
                logger.info("Ran pairwise debate between %s and %s. Winner: %s", hA.hypothesis_id, hB.hypothesis_id, winner.hypothesis_id)
                context.tournament_results.append({
                    "winner": winner.hypothesis_id,
                    "loser": loser.hypothesis_id,
                    "winner_score": winner.elo_score,
                    "loser_score": loser.elo_score
                })

class EvolutionAgent:
    def evolve_hypotheses(self, top_k: int, context: ContextMemory) -> List[Hypothesis]:
        """
        Evolves hypotheses by combining the top-k hypotheses based on Elo score.

        Args:
            top_k (int): The number of top hypotheses to consider for evolution.
            context (ContextMemory): The current context memory.

        Returns:
            List[Hypothesis]: A list of new, evolved hypotheses.  Currently, at most one
                              new hypothesis is generated by combining the top two.
        """
        active = context.get_active_hypotheses()
        sorted_by_elo = sorted(active, key=lambda h: h.elo_score, reverse=True)
        top_candidates = sorted_by_elo[:config["top_k_hypotheses"]]
        new_hypotheses = []
        if len(top_candidates) >= 2:
            new_h = combine_hypotheses(top_candidates[0], top_candidates[1])
            logger.info("Evolved hypothesis: %s", new_h.to_dict())
            logger.info("top_candidates: %s", [h.to_dict() for h in top_candidates]) # Added logging
            new_hypotheses.append(new_h)
        return new_hypotheses

class ProximityAgent:
    def build_proximity_graph(self, hypotheses: List[Hypothesis], context: ContextMemory) -> Dict:
        """
        Builds a proximity graph representing the similarity between hypotheses.

        Args:
            hypotheses (List[Hypothesis]): The list of hypotheses.
            context (ContextMemory): The current context memory.

        Returns:
            Dict: A dictionary containing:
                - "adjacency_graph": An adjacency list representing the proximity graph.
                - "graph_html": HTML and JavaScript code for visualizing the graph.
        """
        adjacency = {}
        for i in range(len(hypotheses)):
            adjacency[hypotheses[i].hypothesis_id] = []
            for j in range(len(hypotheses)):
                if i == j:
                    continue
                sim = similarity_score(hypotheses[i].text, hypotheses[j].text)
                adjacency[hypotheses[i].hypothesis_id].append({
                    "other_id": hypotheses[j].hypothesis_id,
                    "similarity": sim
                })
        
        # Generate the HTML for the graph visualization
        graph_html = generate_visjs_graph(adjacency)
        
        logger.info("Built proximity graph: %s", adjacency)
        return {
            "adjacency_graph": adjacency,
            "graph_html": graph_html
        }

class MetaReviewAgent:
    def summarize_and_feedback(self, context: ContextMemory, adjacency: Dict) -> Dict:
        """
        Summarizes the current state of research and provides feedback.

        Args:
            context (ContextMemory): The current context memory.
            adjacency (Dict): The proximity graph of hypotheses.

        Returns:
            Dict: A dictionary containing a meta-review critique, a research overview
                  (including top-ranked hypotheses and suggested next steps).
        """
        reflection_comments = []
        for h in context.get_active_hypotheses():
            reflection_comments.extend(h.review_comments)
        comment_summary = set()
        for c in reflection_comments:
            if "novelty=LOW" in c:
                comment_summary.add("Some ideas are not very novel.")
            if "feasibility=LOW" in c:
                comment_summary.add("Some ideas may be infeasible.")
        best_hypotheses = sorted(context.get_active_hypotheses(), key=lambda h: h.elo_score, reverse=True)[:3]
        logger.info("Top hypotheses: %s", [h.to_dict() for h in best_hypotheses])

        overview = {
            "meta_review_critique": list(comment_summary),
            "research_overview": {
                "top_ranked_hypotheses": [h.to_dict() for h in best_hypotheses],
                "suggested_next_steps": [
                    "Conduct further in experiments on top hypotheses.",
                    "Collect domain expert feedback and refine constraints."
                ]
            }
        }
        context.meta_review_feedback.append(overview)
        logger.info("Meta-review and feedback: %s", overview)
        return overview

class SupervisorAgent:
    def __init__(self):
        self.generation_agent = GenerationAgent()
        self.reflection_agent = ReflectionAgent()
        self.ranking_agent = RankingAgent()
        self.evolution_agent = EvolutionAgent()
        self.proximity_agent = ProximityAgent()
        self.meta_review_agent = MetaReviewAgent()

    def run_cycle(self, research_goal: ResearchGoal, context: ContextMemory) -> Dict:
        """
        Runs a single cycle of the hypothesis generation, review, ranking, and evolution process.

        Args:
            research_goal (ResearchGoal): The research goal.
            context (ContextMemory): The current context memory.

        Returns:
            Dict: A dictionary containing detailed information about each step of the cycle.
        """
        logger.info("Starting a new cycle, iteration %d", context.iteration_number + 1)

        # Initialize a dictionary to store cycle details
        cycle_details = {
            "iteration": context.iteration_number + 1,
            "steps": {},
            "meta_review": {}
        }

        # 1. Generation
        new_hypotheses = self.generation_agent.generate_new_hypotheses(research_goal, context)
        for nh in new_hypotheses:
            context.add_hypothesis(nh)
        cycle_details["steps"]["generation"] = {
            "hypotheses": [h.to_dict() for h in new_hypotheses]
        }

        # 2. Reflection
        active_hypos = context.get_active_hypotheses()
        self.reflection_agent.review_hypotheses(active_hypos, context)
        cycle_details["steps"]["reflection"] = {
            "hypotheses": [h.to_dict() for h in active_hypos]
        }

        # 3. Ranking (Tournament)
        active_hypos = context.get_active_hypotheses()
        self.ranking_agent.run_tournament(active_hypos, context)
        cycle_details["steps"]["ranking1"] = {
            "tournament_results": context.tournament_results,
            "hypotheses": [h.to_dict() for h in active_hypos]
        }

        # 4. Evolution (Improve top ideas)
        new_evolved = self.evolution_agent.evolve_hypotheses(top_k=2, context=context)
        for nh in new_evolved:
            context.add_hypothesis(nh)
        if new_evolved:
            self.reflection_agent.review_hypotheses(new_evolved, context)
        cycle_details["steps"]["evolution"] = {
            "hypotheses": [h.to_dict() for h in new_evolved]
        }

        # 5. Ranking again
        active_hypos = context.get_active_hypotheses()
        self.ranking_agent.run_tournament(active_hypos, context)
        cycle_details["steps"]["ranking2"] = {
            "tournament_results": context.tournament_results,
            "hypotheses": [h.to_dict() for h in active_hypos]

        }

        # 6. Proximity Analysis
        adjacency_result = self.proximity_agent.build_proximity_graph(active_hypos, context)
        cycle_details["steps"]["proximity"] = {
            "adjacency_graph": adjacency_result["adjacency_graph"],
            "graph_html": adjacency_result["graph_html"]
        }

        # 7. Meta-review
        overview = self.meta_review_agent.summarize_and_feedback(context, adjacency_result["adjacency_graph"])
        cycle_details["meta_review"] = overview
        context.iteration_number += 1

        logger.info("Cycle complete, iteration now %d", context.iteration_number)
        return cycle_details

###############################################################################
# FastAPI Application
###############################################################################

app = FastAPI(title="AI Co-Scientist System", version="1.0")

# Global context and supervisor (in production, consider persistent storage)
global_context = ContextMemory()
supervisor = SupervisorAgent()
current_research_goal: Optional[ResearchGoal] = None

app.mount("/static", StaticFiles(directory="static"), name="static")

@app.post("/research_goal", response_model=dict)
def set_research_goal(goal: ResearchGoalRequest):
    """
    Sets the research goal for the AI Co-Scientist.

    Args:
        goal (ResearchGoalRequest): The research goal, including a description and optional constraints.

    Returns:
        dict: A confirmation message.
    """
    global current_research_goal, global_context, logger
    current_research_goal = ResearchGoal(goal.description, goal.constraints)
    # Reset context for new research goal
    global_context = ContextMemory()

    # Create a new logger for this submission
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_filename = f"log_{timestamp}.txt"
    logger = setup_logger(log_filename)

    logger.info("Research goal set: %s", goal.description)
    return {"message": "Research goal successfully set. Please wait for results. This may take a few minutes. Please be patient."}

@app.post("/run_cycle")
def run_cycle():
    """
    Runs a single cycle of hypothesis generation, review, ranking, and evolution.

    Raises:
        HTTPException: If no research goal has been set.

    Returns:
        Dict: A dictionary containing detailed information about each step of the cycle.
    """
    global current_research_goal, global_context
    if not current_research_goal:
        raise HTTPException(status_code=400, detail="No research goal set.")
    cycle_details = supervisor.run_cycle(current_research_goal, global_context)
    logger.info("Run cycle complete. Overview: %s", cycle_details)
    return cycle_details

@app.get("/hypotheses", response_model=List[HypothesisResponse])
def list_hypotheses():
    """
    Retrieves a list of all currently active hypotheses.

    Returns:
        List[HypothesisResponse]: A list of active hypotheses, each including its ID, title, text,
                                  novelty/feasibility reviews, Elo score, comments, references,
                                  and active status.
    """
    global global_context
    return [HypothesisResponse(**h.to_dict()) for h in global_context.get_active_hypotheses()]

@app.get("/")
async def root():
    """
    Root endpoint for the API. Returns an HTML page with a form to input the research goal.
    """
    return responses.HTMLResponse(content="""
    <!DOCTYPE html>
    <html>
    <head>
        <title>AI Co-Scientist</title>
    </head>
    <body>
        <h1>Welcome to the AI Co-Scientist System</h1>
        <p>Set your research goal and run cycles to generate hypotheses.</p>

        <label for="researchGoal">Research Goal:</label><br>
        <textarea id="researchGoal" name="researchGoal" rows="4" cols="50"></textarea><br><br>
        <button onclick="submitResearchGoal()">Submit Research Goal</button>

        <h2>Results</h2>
        <div id="results"></div>

        <h2>Errors</h2>
        <div id="errors" style="color: red;"></div>

        <script>
            async function submitResearchGoal() {
                const researchGoal = document.getElementById('researchGoal').value;
                const response = await fetch('/research_goal', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ description: researchGoal })
                });

                // Clear previous errors
                document.getElementById('errors').innerHTML = '';

                if (!response.ok) {
                    const errorData = await response.json();
                    document.getElementById('errors').innerHTML = `<p>Error: ${errorData.detail}</p>`;
                    return; // Stop execution if there's an error
                }

                const data = await response.json();
                document.getElementById('results').innerHTML = `<p>${data.message}</p>`;
                runCycle(); // Automatically run a cycle after setting the goal
            }

            async function runCycle() {
                const response = await fetch('/run_cycle', { method: 'POST' });

                // Clear previous errors
                document.getElementById('errors').innerHTML = '';

                if (!response.ok) {
                    const errorData = await response.json();
                    document.getElementById('errors').innerHTML = `<p>Error: ${errorData.detail}</p>`;
                    return; // Stop execution if there's an error
                }

                const data = await response.json();

                let resultsHTML = `<h3>Iteration: ${data.iteration}</h3>`;

                // Define step explanations
                const stepExplanations = {
                    generation: "Generates new hypotheses based on the research goal and current context.",
                    reflection: "Reviews the generated hypotheses for novelty and feasibility.",
                    ranking1: "Ranks hypotheses based on a pairwise comparison (tournament).",
                    evolution: "Combines the top-ranked hypotheses to create new, evolved hypotheses.",
                    ranking2: "Ranks hypotheses again after the evolution step.",
                    proximity: "Analyzes the similarity between hypotheses.",
                };

                // Display details for each step
                for (const stepName in data.steps) {
                    if (data.steps.hasOwnProperty(stepName)) {
                        const step = data.steps[stepName];
                        resultsHTML += `<h4>Step: ${stepName}</h4>`;

                        // Add explanation if available
                        if (stepExplanations[stepName]) {
                            resultsHTML += `<p>${stepExplanations[stepName]}</p>`;
                        }

                        if (step.hypotheses) {
                            resultsHTML += `<h5>Hypotheses:</h5><ul>`;
                            step.hypotheses.sort((a, b) => b.elo_score - a.elo_score).forEach(hypo => {
                                resultsHTML += `<li>
                                    <strong>${hypo.title}</strong> (ID: ${hypo.id}, Elo: ${hypo.elo_score.toFixed(2)})<br>`;
                                if (hypo.parent_ids && hypo.parent_ids.length > 0) {
                                    resultsHTML += `<em>Parent IDs: ${hypo.parent_ids.join(', ')}</em><br>`;
                                }
                                resultsHTML += `<p>${hypo.text}</p>`;
                                if (hypo.novelty_review) {
                                    resultsHTML += `<p>Novelty: ${hypo.novelty_review}</p>`;
                                }
                                if (hypo.feasibility_review){
                                    resultsHTML += `<p>Feasibility: ${hypo.feasibility_review}</p>`;
                                }

                                if (hypo.review_comments && hypo.review_comments.length > 0) {
                                    resultsHTML += `<p>Review Comments:</p><ul>`;
                                    hypo.review_comments.forEach(comment => {
                                        resultsHTML += `<li>${comment}</li>`;
                                    });
                                    resultsHTML += `</ul>`;
                                }
                                if (hypo.references && hypo.references.length > 0) {
                                    resultsHTML += `<p>References:</p><ul>`;
                                    hypo.references.forEach(ref => {
                                        resultsHTML += `<li>${ref}</li>`;
                                    });
                                    resultsHTML += `</ul>`;
                                }
                                resultsHTML += `</li>`;

                            });
                            resultsHTML += `</ul>`;
                        }
                        if (stepName.startsWith("ranking") && step.tournament_results){
                            resultsHTML += '<h5>Ranking Results</h5>';
                            resultsHTML += '<ul>';
                            for (let i = 0; i < step.tournament_results.length; i++){
                                const result = step.tournament_results[i];
                                resultsHTML += `<li>${result.winner} beat ${result.loser}</li>`;
                            }
                            resultsHTML += '</ul>';
                        }

                        if (step.graph_html) {
                            // Display the graph visualization
                            resultsHTML += `<h5>Hypothesis Similarity Graph:</h5>`;
                            resultsHTML += `<div class="graph-container" style="height: 500px; border: 1px solid #ccc; margin-bottom: 20px;">`;
                            resultsHTML += step.graph_html;
                            resultsHTML += `</div>`;
                        } else if (step.adjacency_graph) {
                            // Fallback to displaying the raw adjacency graph if graph_html is not available
                            resultsHTML += `<p>Adjacency Graph: ${JSON.stringify(step.adjacency_graph)}</p>`;
                        }
                    }
                }

                // Display meta-review information
                if (data.meta_review.meta_review_critique && data.meta_review.meta_review_critique.length > 0) {
                    resultsHTML += `<h4>Meta-Review Critique:</h4><ul>`;
                    data.meta_review.meta_review_critique.forEach(item => {
                        resultsHTML += `<li>${item}</li>`;
                    });
                    resultsHTML += `</ul>`;
                }

                if (data.meta_review.research_overview && data.meta_review.research_overview.suggested_next_steps.length > 0) {
                    resultsHTML += `<h4>Suggested Next Steps:</h4><ul>`;
                    data.meta_review.research_overview.suggested_next_steps.forEach(item => {
                        resultsHTML += `<li>${item}</li>`;
                    });
                    resultsHTML += `</ul>`;
                }

                document.getElementById('results').innerHTML = resultsHTML;
            }
        </script>
    </body>
    </html>
    """)


###############################################################################
# Main Entrypoint
###############################################################################

if __name__ == "__main__":
    # Run with: uvicorn this_script:app --host 0.0.0.0 --port 8000
    uvicorn.run("main:app", host=config["fastapi_host"], port=config["fastapi_port"], reload=False)
