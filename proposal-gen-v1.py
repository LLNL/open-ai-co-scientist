# Generated by o3-mini-high
# https://gist.github.com/chunhualiao/f90c48a0bdac24ba686c25c86150cca8
import math
import random
import logging
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

# Configure logging for production readiness.
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
)
logger = logging.getLogger("co_scientist")

###############################################################################
# Data Models and Pydantic Schemas
###############################################################################

class Hypothesis:
    def __init__(self, hypothesis_id: str, title: str, text: str):
        self.hypothesis_id = hypothesis_id
        self.title = title
        self.text = text
        self.novelty_review: Optional[str] = None   # "HIGH", "MEDIUM", "LOW"
        self.feasibility_review: Optional[str] = None
        self.elo_score: float = 1200.0      # initial Elo score
        self.review_comments: List[str] = []
        self.references: List[str] = []
        self.is_active: bool = True

    def to_dict(self) -> dict:
        return {
            "id": self.hypothesis_id,
            "title": self.title,
            "text": self.text,
            "novelty_review": self.novelty_review,
            "feasibility_review": self.feasibility_review,
            "elo_score": self.elo_score,
            "review_comments": self.review_comments,
            "references": self.references,
            "is_active": self.is_active,
        }

class ResearchGoal:
    def __init__(self, description: str, constraints: Dict = None):
        self.description = description
        self.constraints = constraints if constraints else {}

class ContextMemory:
    """
    A simple in-memory context storage.
    """
    def __init__(self):
        self.hypotheses: Dict[str, Hypothesis] = {}  # key: hypothesis_id
        self.tournament_results: List[Dict] = []
        self.meta_review_feedback: List[Dict] = []
        self.iteration_number: int = 0

    def add_hypothesis(self, hypothesis: Hypothesis):
        self.hypotheses[hypothesis.hypothesis_id] = hypothesis
        logger.info(f"Added hypothesis {hypothesis.hypothesis_id}")

    def get_active_hypotheses(self) -> List[Hypothesis]:
        return [h for h in self.hypotheses.values() if h.is_active]


# Pydantic schemas for API endpoints.
class ResearchGoalRequest(BaseModel):
    description: str
    constraints: Optional[Dict] = {}

class HypothesisResponse(BaseModel):
    id: str
    title: str
    text: str
    novelty_review: Optional[str]
    feasibility_review: Optional[str]
    elo_score: float
    review_comments: List[str]
    references: List[str]
    is_active: bool

class OverviewResponse(BaseModel):
    iteration: int
    meta_review_critique: List[str]
    top_hypotheses: List[HypothesisResponse]
    suggested_next_steps: List[str]


###############################################################################
# Utility Functions (Placeholders for LLM Calls and Similarity Measures)
###############################################################################

def generate_unique_id(prefix="H") -> str:
    """
    Generates a unique identifier string.

    Args:
        prefix (str, optional): A prefix for the ID. Defaults to "H".

    Returns:
        str: A unique identifier string consisting of the prefix and a random 4-digit number.
    """
    return f"{prefix}{random.randint(1000, 9999)}"

def call_llm_for_generation(prompt: str) -> List[Dict]:
    """
    Simulates a call to a Large Language Model (LLM) for generating hypotheses.
    In a production environment, this would be replaced with an actual API call to an LLM.

    Args:
        prompt (str): The input prompt for the LLM.

    Returns:
        List[Dict]: A list of dictionaries, each representing a generated hypothesis.
                    Each dictionary contains "title" and "text" keys.
    """
    logger.info("LLM generation called with prompt: %s", prompt)
    return [
        {
            "title": "Potential link between epigenetic factor X and disease Y",
            "text": "We hypothesize that factor X modulates gene expression leading to disease Y."
        },
        {
            "title": "Alternative synergy with drug A for condition B",
            "text": "We propose that drug A, originally approved for condition M, might be repurposed for condition B."
        }
    ]

def call_llm_for_reflection(hypothesis_text: str) -> Dict:
    """
    Simulates a call to a Large Language Model (LLM) for reviewing a hypothesis.
    In a production environment, this would be replaced with an actual API call to an LLM.

    Args:
        hypothesis_text (str): The text of the hypothesis to be reviewed.

    Returns:
        Dict: A dictionary containing the review results, including novelty and feasibility
              assessments (HIGH, MEDIUM, or LOW), a comment, and a list of references.
    """
    outcomes = ["HIGH", "MEDIUM", "LOW"]
    novelty = random.choice(outcomes)
    feasibility = random.choice(outcomes)
    comment = f"Review: novelty={novelty}, feasibility={feasibility}"
    logger.info("LLM reflection for hypothesis: %s", hypothesis_text)
    return {
        "novelty_review": novelty,
        "feasibility_review": feasibility,
        "comment": comment,
        "references": ["PMID:123456", "PMID:789012"]
    }

def run_pairwise_debate(hypoA: Hypothesis, hypoB: Hypothesis) -> Hypothesis:
    """
    Compares two hypotheses based on their novelty and feasibility review scores.

    Args:
        hypoA (Hypothesis): The first hypothesis.
        hypoB (Hypothesis): The second hypothesis.

    Returns:
        Hypothesis: The winning hypothesis. If scores are tied, a winner is chosen randomly.
    """
    def score(h: Hypothesis) -> int:
        """
        Calculates a numerical score for a hypothesis based on its novelty and feasibility reviews.

        Args:
            h (Hypothesis): The hypothesis to score.

        Returns:
            int: The calculated score.  HIGH=3, MEDIUM=2, LOW=1, None=0.  The score is the sum of
                 the novelty and feasibility scores.
        """
        mapping = {"HIGH": 3, "MEDIUM": 2, "LOW": 1, None: 0}
        return mapping.get(h.novelty_review, 0) + mapping.get(h.feasibility_review, 0)
    scoreA = score(hypoA)
    scoreB = score(hypoB)
    winner = hypoA if scoreA > scoreB else hypoB if scoreB > scoreA else random.choice([hypoA, hypoB])
    logger.info("Debate: %s (score %d) vs %s (score %d) => Winner: %s",
                hypoA.hypothesis_id, scoreA, hypoB.hypothesis_id, scoreB, winner.hypothesis_id)
    return winner

def update_elo(winner: Hypothesis, loser: Hypothesis, k_factor: int = 32):
    """
    Updates the Elo scores of two hypotheses after a pairwise comparison.

    Args:
        winner (Hypothesis): The winning hypothesis.
        loser (Hypothesis): The losing hypothesis.
        k_factor (int, optional): The K-factor used in the Elo calculation. Defaults to 32.

    Returns:
        None
    """
    ratingA = winner.elo_score
    ratingB = loser.elo_score
    expectedA = 1 / (1 + math.pow(10, (ratingB - ratingA) / 400))
    expectedB = 1 - expectedA
    winner.elo_score = ratingA + k_factor * (1 - expectedA)
    loser.elo_score = ratingB + k_factor * (0 - expectedB)
    logger.info("Updated Elo: Winner %s -> %.2f, Loser %s -> %.2f",
                winner.hypothesis_id, winner.elo_score, loser.hypothesis_id, loser.elo_score)

def combine_hypotheses(hypoA: Hypothesis, hypoB: Hypothesis) -> Hypothesis:
    """
    Combines two hypotheses into a new, evolved hypothesis.

    Args:
        hypoA (Hypothesis): The first hypothesis.
        hypoB (Hypothesis): The second hypothesis.

    Returns:
        Hypothesis: A new hypothesis combining the two input hypotheses. The new ID is prefixed with "E".
    """
    new_id = generate_unique_id("E")
    combined_title = f"Combined: {hypoA.title} & {hypoB.title}"
    combined_text = f"{hypoA.text}\n\nAdditionally, {hypoB.text}"
    logger.info("Combined hypotheses %s and %s into %s", hypoA.hypothesis_id, hypoB.hypothesis_id, new_id)
    return Hypothesis(new_id, combined_title, combined_text)

def similarity_score(textA: str, textB: str) -> float:
    """
    Calculates a similarity score between two text strings.
    This is a placeholder. In a real implementation, this would use a more sophisticated
    similarity measure like cosine similarity or Jaccard index.

    Args:
        textA (str): The first text string.
        textB (str): The second text string.

    Returns:
        float: A similarity score between 0 and 1 (inclusive). For now, it returns a random number.
    """
    return random.uniform(0, 1)


###############################################################################
# Agent Implementations
###############################################################################

class GenerationAgent:
    def generate_new_hypotheses(self, research_goal: ResearchGoal, context: ContextMemory) -> List[Hypothesis]:
        """
        Generates new hypotheses based on the given research goal and context.

        Args:
            research_goal (ResearchGoal): The research goal.
            context (ContextMemory): The current context memory.

        Returns:
            List[Hypothesis]: A list of newly generated hypotheses.
        """
        prompt = (
            f"Research Goal: {research_goal.description}\n"
            f"Constraints: {research_goal.constraints}\n"
            "Please propose 2 new hypotheses with rationale.\n"
        )
        raw_output = call_llm_for_generation(prompt)
        new_hypos = []
        for idea in raw_output:
            hypo_id = generate_unique_id("G")
            h = Hypothesis(hypo_id, idea["title"], idea["text"])
            new_hypos.append(h)
        return new_hypos

class ReflectionAgent:
    def review_hypotheses(self, hypotheses: List[Hypothesis], context: ContextMemory) -> None:
        """
        Reviews a list of hypotheses, updating their novelty, feasibility, comments, and references.

        Args:
            hypotheses (List[Hypothesis]): The list of hypotheses to review.
            context (ContextMemory): The current context memory.

        Returns:
            None
        """
        for h in hypotheses:
            result = call_llm_for_reflection(h.text)
            h.novelty_review = result["novelty_review"]
            h.feasibility_review = result["feasibility_review"]
            h.review_comments.append(result["comment"])
            h.references.extend(result["references"])

class RankingAgent:
    def run_tournament(self, hypotheses: List[Hypothesis], context: ContextMemory) -> None:
        """
        Runs a tournament among the given hypotheses, updating their Elo scores and recording results.

        Args:
            hypotheses (List[Hypothesis]): The list of hypotheses to participate in the tournament.
            context (ContextMemory): The current context memory.

        Returns:
            None
        """
        random.shuffle(hypotheses)
        pairs = []
        for i in range(len(hypotheses)):
            for j in range(i + 1, len(hypotheses)):
                pairs.append((hypotheses[i], hypotheses[j]))
        for hA, hB in pairs:
            if hA.is_active and hB.is_active:
                winner = run_pairwise_debate(hA, hB)
                loser = hB if winner == hA else hA
                update_elo(winner, loser)
                context.tournament_results.append({
                    "winner": winner.hypothesis_id,
                    "loser": loser.hypothesis_id,
                    "winner_score": winner.elo_score,
                    "loser_score": loser.elo_score
                })

class EvolutionAgent:
    def evolve_hypotheses(self, top_k: int, context: ContextMemory) -> List[Hypothesis]:
        """
        Evolves hypotheses by combining the top-k hypotheses based on Elo score.

        Args:
            top_k (int): The number of top hypotheses to consider for evolution.
            context (ContextMemory): The current context memory.

        Returns:
            List[Hypothesis]: A list of new, evolved hypotheses.  Currently, at most one
                              new hypothesis is generated by combining the top two.
        """
        active = context.get_active_hypotheses()
        sorted_by_elo = sorted(active, key=lambda h: h.elo_score, reverse=True)
        top_candidates = sorted_by_elo[:top_k]
        new_hypotheses = []
        if len(top_candidates) >= 2:
            new_h = combine_hypotheses(top_candidates[0], top_candidates[1])
            new_hypotheses.append(new_h)
        return new_hypotheses

class ProximityAgent:
    def build_proximity_graph(self, hypotheses: List[Hypothesis], context: ContextMemory) -> Dict:
        """
        Builds a proximity graph representing the similarity between hypotheses.

        Args:
            hypotheses (List[Hypothesis]): The list of hypotheses.
            context (ContextMemory): The current context memory.

        Returns:
            Dict: An adjacency list representing the proximity graph.  Keys are hypothesis IDs,
                  and values are lists of dictionaries. Each dictionary in the list represents
                  a connection to another hypothesis and contains "other_id" (the ID of the other
                  hypothesis) and "similarity" (the similarity score between the two hypotheses).
        """
        adjacency = {}
        for i in range(len(hypotheses)):
            adjacency[hypotheses[i].hypothesis_id] = []
            for j in range(len(hypotheses)):
                if i == j:
                    continue
                sim = similarity_score(hypotheses[i].text, hypotheses[j].text)
                adjacency[hypotheses[i].hypothesis_id].append({
                    "other_id": hypotheses[j].hypothesis_id,
                    "similarity": sim
                })
        return adjacency

class MetaReviewAgent:
    def summarize_and_feedback(self, context: ContextMemory, adjacency: Dict) -> Dict:
        """
        Summarizes the current state of research and provides feedback.

        Args:
            context (ContextMemory): The current context memory.
            adjacency (Dict): The proximity graph of hypotheses.

        Returns:
            Dict: A dictionary containing a meta-review critique, a research overview
                  (including top-ranked hypotheses and suggested next steps).
        """
        reflection_comments = []
        for h in context.get_active_hypotheses():
            reflection_comments.extend(h.review_comments)
        comment_summary = set()
        for c in reflection_comments:
            if "novelty=LOW" in c:
                comment_summary.add("Some ideas are not very novel.")
            if "feasibility=LOW" in c:
                comment_summary.add("Some ideas may be infeasible.")
        best_hypotheses = sorted(context.get_active_hypotheses(), key=lambda h: h.elo_score, reverse=True)[:3]
        overview = {
            "meta_review_critique": list(comment_summary),
            "research_overview": {
                "top_ranked_hypotheses": [h.to_dict() for h in best_hypotheses],
                "suggested_next_steps": [
                    "Conduct further in vitro experiments on top hypotheses.",
                    "Collect domain expert feedback and refine constraints."
                ]
            }
        }
        context.meta_review_feedback.append(overview)
        return overview

class SupervisorAgent:
    def __init__(self):
        self.generation_agent = GenerationAgent()
        self.reflection_agent = ReflectionAgent()
        self.ranking_agent = RankingAgent()
        self.evolution_agent = EvolutionAgent()
        self.proximity_agent = ProximityAgent()
        self.meta_review_agent = MetaReviewAgent()

    def run_cycle(self, research_goal: ResearchGoal, context: ContextMemory) -> Dict:
        """
        Runs a single cycle of the hypothesis generation, review, ranking, and evolution process.

        Args:
            research_goal (ResearchGoal): The research goal.
            context (ContextMemory): The current context memory.

        Returns:
            Dict: An overview of the cycle, including meta-review critique and research overview.
        """
        logger.info("Starting a new cycle, iteration %d", context.iteration_number + 1)
        # 1. Generation
        new_hypotheses = self.generation_agent.generate_new_hypotheses(research_goal, context)
        for nh in new_hypotheses:
            context.add_hypothesis(nh)
        # 2. Reflection
        active_hypos = context.get_active_hypotheses()
        self.reflection_agent.review_hypotheses(active_hypos, context)
        # 3. Ranking (Tournament)
        active_hypos = context.get_active_hypotheses()
        self.ranking_agent.run_tournament(active_hypos, context)
        # 4. Evolution (Improve top ideas)
        new_evolved = self.evolution_agent.evolve_hypotheses(top_k=2, context=context)
        for nh in new_evolved:
            context.add_hypothesis(nh)
        if new_evolved:
            self.reflection_agent.review_hypotheses(new_evolved, context)
        # 5. Ranking again
        active_hypos = context.get_active_hypotheses()
        self.ranking_agent.run_tournament(active_hypos, context)
        # 6. Proximity Analysis
        adjacency = self.proximity_agent.build_proximity_graph(active_hypos, context)
        # 7. Meta-review
        overview = self.meta_review_agent.summarize_and_feedback(context, adjacency)
        context.iteration_number += 1
        logger.info("Cycle complete, iteration now %d", context.iteration_number)
        return overview

###############################################################################
# FastAPI Application
###############################################################################

app = FastAPI(title="AI Co-Scientist System", version="1.0")

# Global context and supervisor (in production, consider persistent storage)
global_context = ContextMemory()
supervisor = SupervisorAgent()
current_research_goal: Optional[ResearchGoal] = None

@app.post("/research_goal", response_model=dict)
def set_research_goal(goal: ResearchGoalRequest):
    """
    Sets the research goal for the AI Co-Scientist.

    Args:
        goal (ResearchGoalRequest): The research goal, including a description and optional constraints.

    Returns:
        dict: A confirmation message.
    """
    global current_research_goal, global_context
    current_research_goal = ResearchGoal(goal.description, goal.constraints)
    # Reset context for new research goal
    global_context = ContextMemory()
    logger.info("Research goal set: %s", goal.description)
    return {"message": "Research goal successfully set."}

@app.post("/run_cycle", response_model=OverviewResponse)
def run_cycle():
    """
    Runs a single cycle of hypothesis generation, review, ranking, and evolution.

    Raises:
        HTTPException: If no research goal has been set.

    Returns:
        OverviewResponse: An overview of the cycle, including the iteration number, meta-review
                          critique, top hypotheses, and suggested next steps.
    """
    global current_research_goal, global_context
    if not current_research_goal:
        raise HTTPException(status_code=400, detail="No research goal set.")
    overview_dict = supervisor.run_cycle(current_research_goal, global_context)
    # Build response using best hypotheses from meta review
    top_hypotheses = overview_dict["research_overview"]["top_ranked_hypotheses"]
    response = OverviewResponse(
        iteration=global_context.iteration_number,
        meta_review_critique=overview_dict["meta_review_critique"],
        top_hypotheses=[HypothesisResponse(**h) for h in top_hypotheses],
        suggested_next_steps=overview_dict["research_overview"]["suggested_next_steps"]
    )
    return response

@app.get("/hypotheses", response_model=List[HypothesisResponse])
def list_hypotheses():
    """
    Retrieves a list of all currently active hypotheses.

    Returns:
        List[HypothesisResponse]: A list of active hypotheses, each including its ID, title, text,
                                  novelty/feasibility reviews, Elo score, comments, references,
                                  and active status.
    """
    global global_context
    return [HypothesisResponse(**h.to_dict()) for h in global_context.get_active_hypotheses()]

@app.get("/")
def root():
    """
    Root endpoint for the API. Returns a welcome message.

    Returns:
        dict: A welcome message.
    """
    return {"message": "Welcome to the AI Co-Scientist System. Set your research goal and run cycles to generate hypotheses."}

###############################################################################
# Main Entrypoint
###############################################################################

if __name__ == "__main__":
    # Run with: uvicorn this_script:app --host 0.0.0.0 --port 8000
    uvicorn.run("co_scientist:app", host="0.0.0.0", port=8000, reload=False)
