# Generated by o3-mini-high
# https://gist.github.com/chunhualiao/f90c48a0bdac24ba686c25c86150cca8
import math
import random
import logging
from typing import List, Dict, Optional
from openai import OpenAI
import os
import datetime
from fastapi import FastAPI, HTTPException, responses
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn
import yaml

# Configure logging for production readiness.
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s %(levelname)s %(name)s: %(message)s",
#     filename="app.log",
# )
# logger = logging.getLogger("co_scientist") # global logger

def load_config(config_path: str) -> Dict:
    """Loads the configuration from the specified YAML file."""
    try:
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
            # Convert logging level string to actual level
            config["logging_level"] = getattr(logging, config["logging_level"].upper(), logging.INFO)
        return config
    except FileNotFoundError:
        print(f"Error: Configuration file not found at {config_path}")
        exit(1)
    except yaml.YAMLError as e:
        print(f"Error parsing YAML in {config_path}: {e}")
        exit(1)
    except AttributeError as e:
        print("Error: Invalid logging level in config file")
        exit(1)


def setup_logger(log_filename):
    logger = logging.getLogger(log_filename)  # Create a logger with the filename
    logger.setLevel(config["logging_level"])
    formatter = logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s")

    # Remove existing handlers to avoid duplicate logs
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    file_handler = logging.FileHandler(f"{config['log_file_name']}_{log_filename}")
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    return logger

# Load configuration at the start
config = load_config("config.yaml")

def call_llm(prompt: str) -> str:
    """
    Calls an LLM via the OpenRouter API and returns the response.

    Args:
        prompt (str): The input prompt for the LLM.

    Returns:
        str: The LLM's response.
    """
    client = OpenAI(
        base_url=config["openrouter_base_url"],
        api_key=os.getenv("OPENROUTER_API_KEY"),
    )

    try:
        completion = client.chat.completions.create(
            model=config["llm_model"],
            messages=[{"role": "user", "content": prompt}],
        )
    except Exception as e:
        # If the library raises an exception (e.g., for invalid key, rate limit, etc.)
        logger.error(f"API call failed with exception: {e}")
        return f"API call failed with exception: {e}" # Return the error message
    else:
        # If no exception, you can safely access attributes
        if completion.choices and len(completion.choices) > 0:
            return completion.choices[0].message.content
        else:
            logger.error("No choices in the response: %s", completion.dict())
            return f"No choices in the response: {completion.dict()}"

###############################################################################
# Data Models and Pydantic Schemas
###############################################################################

class Hypothesis:
    def __init__(self, hypothesis_id: str, title: str, text: str):
        self.hypothesis_id = hypothesis_id
        self.title = title
        self.text = text
        self.novelty_review: Optional[str] = None   # "HIGH", "MEDIUM", "LOW"
        self.feasibility_review: Optional[str] = None
        self.elo_score: float = 1200.0      # initial Elo score
        self.review_comments: List[str] = []
        self.references: List[str] = []
        self.is_active: bool = True
        self.parent_ids: List[str] = []  # Store IDs of parent hypotheses

    def to_dict(self) -> dict:
        return {
            "id": self.hypothesis_id,
            "title": self.title,
            "text": self.text,
            "novelty_review": self.novelty_review,
            "feasibility_review": self.feasibility_review,
            "elo_score": self.elo_score,
            "review_comments": self.review_comments,
            "references": self.references,
            "is_active": self.is_active,
            "parent_ids": self.parent_ids,  # Include parent IDs
        }

class ResearchGoal:
    def __init__(self, description: str, constraints: Dict = None):
        self.description = description
        self.constraints = constraints if constraints else {}

class ContextMemory:
    """
    A simple in-memory context storage.
    """
    def __init__(self):
        self.hypotheses: Dict[str, Hypothesis] = {}  # key: hypothesis_id
        self.tournament_results: List[Dict] = []
        self.meta_review_feedback: List[Dict] = []
        self.iteration_number: int = 0

    def add_hypothesis(self, hypothesis: Hypothesis):
        self.hypotheses[hypothesis.hypothesis_id] = hypothesis
        logger.info(f"Added hypothesis {hypothesis.hypothesis_id}")

    def get_active_hypotheses(self) -> List[Hypothesis]:
        return [h for h in self.hypotheses.values() if h.is_active]


# Pydantic schemas for API endpoints.
class ResearchGoalRequest(BaseModel):
    description: str
    constraints: Optional[Dict] = {}

class HypothesisResponse(BaseModel):
    id: str
    title: str
    text: str
    novelty_review: Optional[str]
    feasibility_review: Optional[str]
    elo_score: float
    review_comments: List[str]
    references: List[str]
    is_active: bool

class OverviewResponse(BaseModel):
    iteration: int
    meta_review_critique: List[str]
    top_hypotheses: List[HypothesisResponse]
    suggested_next_steps: List[str]


###############################################################################
# Utility Functions (Placeholders for LLM Calls and Similarity Measures)
###############################################################################

def generate_unique_id(prefix="H") -> str:
    """
    Generates a unique identifier string.

    Args:
        prefix (str, optional): A prefix for the ID. Defaults to "H".

    Returns:
        str: A unique identifier string consisting of the prefix and a random 4-digit number.
    """
    return f"{prefix}{random.randint(1000, 9999)}"

import json

def call_llm_for_generation(prompt: str, num_hypotheses: int = 3) -> List[Dict]:
    """
    Calls a Large Language Model (LLM) for generating hypotheses.

    Args:
        prompt (str): The input prompt for the LLM.
        num_hypotheses (int, optional): The number of hypotheses to generate. Defaults to 3.

    Returns:
        List[Dict]: A list of dictionaries, each representing a generated hypothesis.
                    Each dictionary contains "title" and "text" keys.
    """
    logger.info("LLM generation called with prompt: %s, num_hypotheses: %d", prompt, num_hypotheses)

    # Modify the prompt to request JSON output
    prompt += "\n\nPlease return the response as a JSON array of objects, where each object has a 'title' and 'text' key."

    response = call_llm(prompt)
    logger.info("LLM response: %s", response)

    if "API call failed" in response:
        # If the call failed, log it and return an empty list
        logger.error(f"LLM call failed: {response}")
        return []

    try:
        # Remove potential Markdown code block formatting
        response = response.strip()
        if response.startswith("```json"):
            response = response[7:]
        if response.endswith("```"):
            response = response[:-3]
        response = response.strip()

        # Attempt to parse the response as JSON
        hypotheses = json.loads(response)
        logger.info("Parsed hypotheses: %s", hypotheses)

        # Basic validation: Check if the response is a list and each item has 'title' and 'text'
        if not isinstance(hypotheses, list) or not all(isinstance(h, dict) and "title" in h and "text" in h for h in hypotheses):
          raise ValueError("Invalid JSON format: Expected a list of objects with 'title' and 'text' keys.")
    except (json.JSONDecodeError, ValueError) as e:
        logger.error("Could not parse LLM response as JSON: %s", response)
        logger.error(f"Error: {e}")
        return []  # Return an empty list in case of parsing failure

    return hypotheses

def call_llm_for_reflection(hypothesis_text: str) -> Dict:
    """
    Calls a Large Language Model (LLM) for reviewing a hypothesis.

    Args:
        hypothesis_text (str): The text of the hypothesis to be reviewed.

    Returns:
        Dict: A dictionary containing the review results, including novelty and feasibility
              assessments (HIGH, MEDIUM, or LOW), a comment, and a list of references.
    """
    prompt = (
        f"Review the following hypothesis and provide a novelty assessment (HIGH, MEDIUM, or LOW), "
        f"a feasibility assessment (HIGH, MEDIUM, or LOW), a comment, and a list of references (PMIDs) in JSON format:\n\n"
        f"Hypothesis: {hypothesis_text}\n\n"
        f"Return the response as a JSON object with the following keys: 'novelty_review', 'feasibility_review', 'comment', 'references'."

    )
    response = call_llm(prompt)
    logger.info("LLM reflection for hypothesis: %s, response: %s", hypothesis_text, response)

     # Initialize default values
    novelty_review = "MEDIUM"
    feasibility_review = "MEDIUM"
    comment = "Could not parse LLM response."
    references = []

    try:
        if "API call failed" not in response:
            # Remove potential Markdown code block formatting
            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]
            if response.endswith("```"):
                response = response[:-3]
            response = response.strip()

            # Parse the JSON response
            data = json.loads(response)
            novelty_review = data.get("novelty_review", "MEDIUM")
            feasibility_review = data.get("feasibility_review", "MEDIUM")
            comment = data.get("comment", "Could not parse LLM response.")
            references = data.get("references", [])

            # Basic validation of review values
            if not any(level in novelty_review.upper() for level in ["HIGH", "MEDIUM", "LOW"]):
                logger.warning("Invalid novelty review value: %s", novelty_review)
                novelty_review = "MEDIUM"
            if not any(level in feasibility_review.upper() for level in ["HIGH", "MEDIUM", "LOW"]):
                logger.warning("Invalid feasibility review value: %s", feasibility_review)
                feasibility_review = "MEDIUM"
            if not isinstance(comment, str):
                logger.warning("Invalid comment value: %s", comment)
                comment = "Could not parse LLM response."

    except (json.JSONDecodeError, AttributeError, KeyError) as e:
        logger.warning("Error parsing LLM response: %s", e)
        logger.warning("Response: %s", response)
    return {
        "novelty_review": novelty_review,
        "feasibility_review": feasibility_review,
        "comment": comment,
        "references": references,
    }

def run_pairwise_debate(hypoA: Hypothesis, hypoB: Hypothesis) -> Hypothesis:
    """
    Compares two hypotheses based on their novelty and feasibility review scores.

    Args:
        hypoA (Hypothesis): The first hypothesis.
        hypoB (Hypothesis): The second hypothesis.

    Returns:
        Hypothesis: The winning hypothesis. If scores are tied, a winner is chosen randomly.
    """
    def score(h: Hypothesis) -> int:
        """
        Calculates a numerical score for a hypothesis based on its novelty and feasibility reviews.

        Args:
            h (Hypothesis): The hypothesis to score.

        Returns:
            int: The calculated score.  HIGH=3, MEDIUM=2, LOW=1, None=0.  The score is the sum of
                 the novelty and feasibility scores.
        """
        mapping = {"HIGH": 3, "MEDIUM": 2, "LOW": 1, None: 0}
        return mapping.get(h.novelty_review, 0) + mapping.get(h.feasibility_review, 0)
    scoreA = score(hypoA)
    scoreB = score(hypoB)
    winner = hypoA if scoreA > scoreB else hypoB if scoreB > scoreA else random.choice([hypoA, hypoB])
    logger.info("Debate: %s (score %d) vs %s (score %d) => Winner: %s",
                hypoA.hypothesis_id, scoreA, hypoB.hypothesis_id, scoreB, winner.hypothesis_id)
    return winner

def update_elo(winner: Hypothesis, loser: Hypothesis, k_factor: int = config["elo_k_factor"]):
    """
    Updates the Elo scores of two hypotheses after a pairwise comparison.

    Args:
        winner (Hypothesis): The winning hypothesis.
        loser (Hypothesis): The losing hypothesis.
        k_factor (int, optional): The K-factor used in the Elo calculation. Defaults to 32.

    Returns:
        None
    """
    ratingA = winner.elo_score
    ratingB = loser.elo_score
    expectedA = 1 / (1 + math.pow(10, (ratingB - ratingA) / 400))
    expectedB = 1 - expectedA
    winner.elo_score = ratingA + k_factor * (1 - expectedA)
    loser.elo_score = ratingB + k_factor * (0 - expectedB)
    logger.info("Updated Elo: Winner %s -> %.2f, Loser %s -> %.2f",
                winner.hypothesis_id, winner.elo_score, loser.hypothesis_id, loser.elo_score)

def combine_hypotheses(hypoA: Hypothesis, hypoB: Hypothesis) -> Hypothesis:
    """
    Combines two hypotheses into a new, evolved hypothesis.

    Args:
        hypoA (Hypothesis): The first hypothesis.
        hypoB (Hypothesis): The second hypothesis.

    Returns:
        Hypothesis: A new hypothesis combining the two input hypotheses. The new ID is prefixed with "E".
    """
    new_id = generate_unique_id("E")
    combined_title = f"Combined: {hypoA.title} & {hypoB.title}"
    combined_text = f"{hypoA.text}\n\nAdditionally, {hypoB.text}"
    logger.info("Combined hypotheses %s and %s into %s", hypoA.hypothesis_id, hypoB.hypothesis_id, new_id)
    new_hypothesis = Hypothesis(new_id, combined_title, combined_text)
    new_hypothesis.parent_ids = [hypoA.hypothesis_id, hypoB.hypothesis_id]  # Store parent IDs
    logger.info("New hypothesis parent_ids: %s", new_hypothesis.parent_ids) # Added logging
    return new_hypothesis

def similarity_score(textA: str, textB: str) -> float:
    """
    Calculates a similarity score between two text strings.
    This is a placeholder. In a real implementation, this would use a more sophisticated
    similarity measure like cosine similarity or Jaccard index.

    Args:
        textA (str): The first text string.
        textB (str): The second text string.

    Returns:
        float: A similarity score between 0 and 1 (inclusive). For now, it returns a random number.
    """
    logger.info("Similarity score between %s and %s: %f (placeholder)", textA, textB, random.uniform(0,1))
    return random.uniform(0, 1)


###############################################################################
# Agent Implementations
###############################################################################

class GenerationAgent:
    def generate_new_hypotheses(self, research_goal: ResearchGoal, context: ContextMemory) -> List[Hypothesis]:
        """
        Generates new hypotheses based on the given research goal and context.

        Args:
            research_goal (ResearchGoal): The research goal.
            context (ContextMemory): The current context memory.

        Returns:
            List[Hypothesis]: A list of newly generated hypotheses.
        """
        prompt = (
            f"Research Goal: {research_goal.description}\n"
            f"Constraints: {research_goal.constraints}\n"
            f"Please propose {config['num_hypotheses']} new hypotheses with rationale.\n"
        )
        raw_output = call_llm_for_generation(prompt, num_hypotheses=config["num_hypotheses"])
        new_hypos = []
        for idea in raw_output:
            hypo_id = generate_unique_id("G")
            h = Hypothesis(hypo_id, idea["title"], idea["text"])
            logger.info("Generated hypothesis: %s", h.to_dict())
            new_hypos.append(h)
        return new_hypos

class ReflectionAgent:
    def review_hypotheses(self, hypotheses: List[Hypothesis], context: ContextMemory) -> None:
        """
        Reviews a list of hypotheses, updating their novelty, feasibility, comments, and references.

        Args:
            hypotheses (List[Hypothesis]): The list of hypotheses to review.
            context (ContextMemory): The current context memory.

        Returns:
            None
        """
        for h in hypotheses:
            result = call_llm_for_reflection(h.text)
            h.novelty_review = result["novelty_review"]
            h.feasibility_review = result["feasibility_review"]
            h.review_comments.append(result["comment"])
            h.references.extend(result["references"])
            logger.info("Reviewed hypothesis: %s, Novelty: %s, Feasibility: %s", h.hypothesis_id, h.novelty_review, h.feasibility_review)

class RankingAgent:
    def run_tournament(self, hypotheses: List[Hypothesis], context: ContextMemory) -> None:
        """
        Runs a tournament among the given hypotheses, updating their Elo scores and recording results.

        Args:
            hypotheses (List[Hypothesis]): The list of hypotheses to participate in the tournament.
            context (ContextMemory): The current context memory.

        Returns:
            None
        """
        random.shuffle(hypotheses)
        pairs = []
        for i in range(len(hypotheses)):
            for j in range(i + 1, len(hypotheses)):
                pairs.append((hypotheses[i], hypotheses[j]))
        for hA, hB in pairs:
            if hA.is_active and hB.is_active:
                winner = run_pairwise_debate(hA, hB)
                loser = hB if winner == hA else hA
                update_elo(winner, loser)
                logger.info("Ran pairwise debate between %s and %s. Winner: %s", hA.hypothesis_id, hB.hypothesis_id, winner.hypothesis_id)
                context.tournament_results.append({
                    "winner": winner.hypothesis_id,
                    "loser": loser.hypothesis_id,
                    "winner_score": winner.elo_score,
                    "loser_score": loser.elo_score
                })

class EvolutionAgent:
    def evolve_hypotheses(self, top_k: int, context: ContextMemory) -> List[Hypothesis]:
        """
        Evolves hypotheses by combining the top-k hypotheses based on Elo score.

        Args:
            top_k (int): The number of top hypotheses to consider for evolution.
            context (ContextMemory): The current context memory.

        Returns:
            List[Hypothesis]: A list of new, evolved hypotheses.  Currently, at most one
                              new hypothesis is generated by combining the top two.
        """
        active = context.get_active_hypotheses()
        sorted_by_elo = sorted(active, key=lambda h: h.elo_score, reverse=True)
        top_candidates = sorted_by_elo[:config["top_k_hypotheses"]]
        new_hypotheses = []
        if len(top_candidates) >= 2:
            new_h = combine_hypotheses(top_candidates[0], top_candidates[1])
            logger.info("Evolved hypothesis: %s", new_h.to_dict())
            logger.info("top_candidates: %s", [h.to_dict() for h in top_candidates]) # Added logging
            new_hypotheses.append(new_h)
        return new_hypotheses

class ProximityAgent:
    def build_proximity_graph(self, hypotheses: List[Hypothesis], context: ContextMemory) -> Dict:
        """
        Builds a proximity graph representing the similarity between hypotheses.

        Args:
            hypotheses (List[Hypothesis]): The list of hypotheses.
            context (ContextMemory): The current context memory.

        Returns:
            Dict: An adjacency list representing the proximity graph.  Keys are hypothesis IDs,
                  and values are lists of dictionaries. Each dictionary in the list represents
                  a connection to another hypothesis and contains "other_id" (the ID of the other
                  hypothesis) and "similarity" (the similarity score between the two hypotheses).
        """
        adjacency = {}
        for i in range(len(hypotheses)):
            adjacency[hypotheses[i].hypothesis_id] = []
            for j in range(len(hypotheses)):
                if i == j:
                    continue
                sim = similarity_score(hypotheses[i].text, hypotheses[j].text)
                adjacency[hypotheses[i].hypothesis_id].append({
                    "other_id": hypotheses[j].hypothesis_id,
                    "similarity": sim
                })
        logger.info("Built proximity graph: %s", adjacency)
        return adjacency

class MetaReviewAgent:
    def summarize_and_feedback(self, context: ContextMemory, adjacency: Dict) -> Dict:
        """
        Summarizes the current state of research and provides feedback.

        Args:
            context (ContextMemory): The current context memory.
            adjacency (Dict): The proximity graph of hypotheses.

        Returns:
            Dict: A dictionary containing a meta-review critique, a research overview
                  (including top-ranked hypotheses and suggested next steps).
        """
        reflection_comments = []
        for h in context.get_active_hypotheses():
            reflection_comments.extend(h.review_comments)
        comment_summary = set()
        for c in reflection_comments:
            if "novelty=LOW" in c:
                comment_summary.add("Some ideas are not very novel.")
            if "feasibility=LOW" in c:
                comment_summary.add("Some ideas may be infeasible.")
        best_hypotheses = sorted(context.get_active_hypotheses(), key=lambda h: h.elo_score, reverse=True)[:3]
        logger.info("Top hypotheses: %s", [h.to_dict() for h in best_hypotheses])

        overview = {
            "meta_review_critique": list(comment_summary),
            "research_overview": {
                "top_ranked_hypotheses": [h.to_dict() for h in best_hypotheses],
                "suggested_next_steps": [
                    "Conduct further in vitro experiments on top hypotheses.",
                    "Collect domain expert feedback and refine constraints."
                ]
            }
        }
        context.meta_review_feedback.append(overview)
        logger.info("Meta-review and feedback: %s", overview)
        return overview

class SupervisorAgent:
    def __init__(self):
        self.generation_agent = GenerationAgent()
        self.reflection_agent = ReflectionAgent()
        self.ranking_agent = RankingAgent()
        self.evolution_agent = EvolutionAgent()
        self.proximity_agent = ProximityAgent()
        self.meta_review_agent = MetaReviewAgent()

    def run_cycle(self, research_goal: ResearchGoal, context: ContextMemory) -> Dict:
        """
        Runs a single cycle of the hypothesis generation, review, ranking, and evolution process.

        Args:
            research_goal (ResearchGoal): The research goal.
            context (ContextMemory): The current context memory.

        Returns:
            Dict: An overview of the cycle, including meta-review critique and research overview.
        """
        logger.info("Starting a new cycle, iteration %d", context.iteration_number + 1)
        logger.info("Starting a new cycle, iteration %d", context.iteration_number + 1)
        # 1. Generation
        new_hypotheses = self.generation_agent.generate_new_hypotheses(research_goal, context)
        for nh in new_hypotheses:
            context.add_hypothesis(nh)
        # 2. Reflection
        active_hypos = context.get_active_hypotheses()
        self.reflection_agent.review_hypotheses(active_hypos, context)
        # 3. Ranking (Tournament)
        active_hypos = context.get_active_hypotheses()
        self.ranking_agent.run_tournament(active_hypos, context)
        # 4. Evolution (Improve top ideas)
        new_evolved = self.evolution_agent.evolve_hypotheses(top_k=2, context=context)
        for nh in new_evolved:
            context.add_hypothesis(nh)
        if new_evolved:
            self.reflection_agent.review_hypotheses(new_evolved, context)
        # 5. Ranking again
        active_hypos = context.get_active_hypotheses()
        self.ranking_agent.run_tournament(active_hypos, context)
        # 6. Proximity Analysis
        adjacency = self.proximity_agent.build_proximity_graph(active_hypos, context)
        # 7. Meta-review
        overview = self.meta_review_agent.summarize_and_feedback(context, adjacency)
        context.iteration_number += 1
        logger.info("Cycle complete, iteration now %d", context.iteration_number)
        return overview

###############################################################################
# FastAPI Application
###############################################################################

app = FastAPI(title="AI Co-Scientist System", version="1.0")

# Global context and supervisor (in production, consider persistent storage)
global_context = ContextMemory()
supervisor = SupervisorAgent()
current_research_goal: Optional[ResearchGoal] = None

app.mount("/static", StaticFiles(directory="static"), name="static")

@app.post("/research_goal", response_model=dict)
def set_research_goal(goal: ResearchGoalRequest):
    """
    Sets the research goal for the AI Co-Scientist.

    Args:
        goal (ResearchGoalRequest): The research goal, including a description and optional constraints.

    Returns:
        dict: A confirmation message.
    """
    global current_research_goal, global_context, logger
    current_research_goal = ResearchGoal(goal.description, goal.constraints)
    # Reset context for new research goal
    global_context = ContextMemory()

    # Create a new logger for this submission
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_filename = f"log_{timestamp}.txt"
    logger = setup_logger(log_filename)

    logger.info("Research goal set: %s", goal.description)
    return {"message": "Research goal successfully set. Please wait for results. This may take a few minutes. Please be patient."}

@app.post("/run_cycle", response_model=OverviewResponse)
def run_cycle():
    """
    Runs a single cycle of hypothesis generation, review, ranking, and evolution.

    Raises:
        HTTPException: If no research goal has been set.

    Returns:
        OverviewResponse: An overview of the cycle, including the iteration number, meta-review
                          critique, top hypotheses, and suggested next steps.
    """
    global current_research_goal, global_context
    if not current_research_goal:
        raise HTTPException(status_code=400, detail="No research goal set.")
    overview_dict = supervisor.run_cycle(current_research_goal, global_context)
    logger.info("Run cycle complete. Overview: %s", overview_dict)
    # Build response using best hypotheses from meta review
    top_hypotheses = overview_dict["research_overview"]["top_ranked_hypotheses"]
    response = OverviewResponse(
        iteration=global_context.iteration_number,
        meta_review_critique=overview_dict["meta_review_critique"],
        top_hypotheses=[HypothesisResponse(**h) for h in top_hypotheses],
        suggested_next_steps=overview_dict["research_overview"]["suggested_next_steps"]
    )
    return response

@app.get("/hypotheses", response_model=List[HypothesisResponse])
def list_hypotheses():
    """
    Retrieves a list of all currently active hypotheses.

    Returns:
        List[HypothesisResponse]: A list of active hypotheses, each including its ID, title, text,
                                  novelty/feasibility reviews, Elo score, comments, references,
                                  and active status.
    """
    global global_context
    return [HypothesisResponse(**h.to_dict()) for h in global_context.get_active_hypotheses()]

@app.get("/")
async def root():
    """
    Root endpoint for the API. Returns an HTML page with a form to input the research goal.
    """
    return responses.HTMLResponse(content="""
    <!DOCTYPE html>
    <html>
    <head>
        <title>AI Co-Scientist</title>
    </head>
    <body>
        <h1>Welcome to the AI Co-Scientist System</h1>
        <p>Set your research goal and run cycles to generate hypotheses.</p>

        <label for="researchGoal">Research Goal:</label><br>
        <textarea id="researchGoal" name="researchGoal" rows="4" cols="50"></textarea><br><br>
        <button onclick="submitResearchGoal()">Submit Research Goal</button>

        <h2>Results</h2>
        <div id="results"></div>

        <script>
            async function submitResearchGoal() {
                const researchGoal = document.getElementById('researchGoal').value;
                const response = await fetch('/research_goal', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ description: researchGoal })
                });
                const data = await response.json();
                document.getElementById('results').innerHTML = `<p>${data.message}</p>`;
                runCycle(); // Automatically run a cycle after setting the goal
            }

            async function runCycle() {
                const response = await fetch('/run_cycle', { method: 'POST' });
                const data = await response.json();

                let resultsHTML = `<h3>Iteration: ${data.iteration}</h3>`;

                if (data.meta_review_critique.length > 0) {
                    resultsHTML += `<h4>Meta-Review Critique:</h4><ul>`;
                    data.meta_review_critique.forEach(item => {
                        resultsHTML += `<li>${item}</li>`;
                    });
                    resultsHTML += `</ul>`;
                }

                resultsHTML += `<h4>Top Hypotheses:</h4>`;
                data.top_hypotheses.forEach(hypo => {
                    resultsHTML += `<ul><li>
                        <strong>${hypo.title}</strong> (ID: ${hypo.id}, Elo: ${hypo.elo_score.toFixed(2)})<br>`;

                    // Display parent IDs if they exist (for combined hypotheses)
                    if (hypo.parent_ids && hypo.parent_ids.length > 0) {
                        resultsHTML += `<em>Parent IDs: ${hypo.parent_ids.join(', ')}</em><br>`;
                    }

                    // Debug: Display the hypothesis object as a string
                    resultsHTML += `<pre>DEBUG: ${JSON.stringify(hypo)}</pre><br>`;

                    resultsHTML += `<pre>${hypo.text}</pre><br>
                        Novelty: ${hypo.novelty_review}, Feasibility: ${hypo.feasibility_review}
                    </li></ul>`;
                });

                if (data.suggested_next_steps.length > 0){
                    resultsHTML += `<h4>Suggested Next Steps:</h4><ul>`;
                    data.suggested_next_steps.forEach(item => {
                        resultsHTML += `<li>${item}</li>`;
                    });
                    resultsHTML += `</ul>`;
                }

                document.getElementById('results').innerHTML = resultsHTML;
            }
        </script>
    </body>
    </html>
    """)


###############################################################################
# Main Entrypoint
###############################################################################

if __name__ == "__main__":
    # Run with: uvicorn this_script:app --host 0.0.0.0 --port 8000
    uvicorn.run("proposal-gen-v1:app", host=config["fastapi_host"], port=config["fastapi_port"], reload=False)
