2025-05-31 19:22:04,085 INFO aicoscientist: Logging for this goal directed to: results/app_log_2025-05-31_19-22-04.txt
2025-05-31 19:22:04,085 INFO aicoscientist: Logging setup complete. File: results/app_log_2025-05-31_19-22-04.txt, Database: Session session_20250531_192155
2025-05-31 19:22:04,085 INFO aicoscientist: Auto-running first cycle with literature context...
2025-05-31 19:22:04,086 INFO aicoscientist: --- Starting Cycle 1 ---
2025-05-31 19:22:04,086 INFO aicoscientist: Step 1: Generation
2025-05-31 19:22:04,086 INFO aicoscientist: LLM generation called with prompt: Research Goal: make self-improving ai systems
Constraints: {}
Existing Hypothesis IDs: []
Please propose 6 novel and feasible hypotheses with rationale, avoiding duplication with existing IDs. Consider how your hypotheses can advance beyond the current literature while addressing gaps or unexplored directions.
, num_hypotheses: 6, temperature: 0.70
2025-05-31 19:22:10,634 INFO aicoscientist: LLM call completed - Type: generation, Model: google/gemini-2.0-flash-001, Success: True, Time: 6541.63ms, Tokens: 80/920, Retries: 0
2025-05-31 19:22:10,634 INFO aicoscientist: LLM generation response: ```json
[
  {
    "title": "Meta-Learning Curriculum Design for Autonomous Exploration",
    "text": "Hypothesis: A meta-learning agent can learn to design its own learning curriculum for exploration in novel, initially unknown environments. Rationale: Current curriculum learning often relies on hand-crafted curricula or pre-defined task distributions. This hypothesis explores autonomous curriculum design using meta-learning, allowing the agent to actively select tasks and environments that maximize its learning progress and exploration efficiency. This goes beyond existing work by focusing on *true* autonomy in curriculum creation, not just task selection from a fixed set. It addresses the gap of adapting exploration strategies to completely novel and unpredictable environments."
  },
  {
    "title": "Hierarchical Intrinsic Motivation with Predictive Models for Skill Discovery",
    "text": "Hypothesis: A hierarchical reinforcement learning agent, driven by intrinsic motivation based on prediction error reduction at multiple levels of abstraction, can autonomously discover and learn a useful repertoire of skills. Rationale: Existing intrinsic motivation often focuses on immediate prediction error. This hypothesis proposes a hierarchical approach where higher levels predict lower-level actions and outcomes, driving the agent to learn skills that improve its ability to model its own behavior and the environment at different granularities. This differs from existing hierarchical RL by explicitly linking the hierarchy to intrinsic motivation and learning predictive models for skill abstraction, leading to more robust and adaptable skill discovery."
  },
  {
    "title": "Lifelong Learning via Continual Model Merging and Knowledge Distillation",
    "text": "Hypothesis: Continual learning can be improved by dynamically merging pre-trained models trained on different tasks and distilling their knowledge into a single, unified model while preserving task-specific performance and mitigating catastrophic forgetting. Rationale: Current continual learning methods often struggle with catastrophic forgetting and scaling to a large number of tasks. This hypothesis explores a model merging approach where models trained on individual tasks are combined into a larger model using techniques like parameter averaging or attention-based merging. Knowledge distillation is then used to refine the merged model and prevent interference between tasks. This goes beyond current distillation techniques by focusing on the efficient merging of multiple pre-trained models, addressing the scalability limitations of existing continual learning methods."
  },
  {
    "title": "Uncertainty-Aware Exploration using Bayesian Neural Networks and Active Learning",
    "text": "Hypothesis: An agent using a Bayesian Neural Network (BNN) to represent its environment model, combined with active learning techniques to query the environment based on uncertainty estimates, can achieve more efficient exploration and learning compared to agents using deterministic models. Rationale: Traditional exploration methods often rely on heuristics or random exploration. This hypothesis leverages the uncertainty estimates provided by BNNs to guide exploration, focusing on regions of the environment where the agent's knowledge is most uncertain. Active learning is used to select the most informative actions based on these uncertainty estimates. This extends current BNN-based RL by explicitly using the uncertainty estimates for *active* exploration, leading to more targeted data acquisition and faster learning."
  },
  {
    "title": "Emergent Communication Protocols for Collaborative Self-Improvement",
    "text": "Hypothesis: A population of agents can develop emergent communication protocols that facilitate collaborative self-improvement through the sharing of learned strategies, experiences, and environmental models. Rationale: Current multi-agent learning often focuses on competition or pre-defined communication channels. This hypothesis explores the emergence of communication protocols that enable agents to share knowledge and accelerate their individual learning processes. The agents could share successful strategies, learned environmental models, or even raw experiences. This moves beyond current emergent communication by explicitly linking it to self-improvement, enabling agents to collectively learn and surpass the capabilities of individual agents."
  },
  {
    "title": "Symbolic Abstraction and Reasoning for Generalization in Reinforcement Learning",
    "text": "Hypothesis: Integrating symbolic abstraction and reasoning into a reinforcement learning agent's architecture allows for more robust generalization to unseen environments and tasks by enabling the agent to reason about high-level concepts and relationships. Rationale: Current RL agents often struggle to generalize to even slightly different environments. This hypothesis proposes combining RL with symbolic AI techniques, where the agent learns to abstract the environment into symbolic representations and use logical reasoning to plan and make decisions. This goes beyond end-to-end RL by incorporating symbolic reasoning, enabling the agent to transfer knowledge and generalize to new situations more effectively by reasoning about the underlying principles and relationships."
  }
]
```
2025-05-31 19:22:10,635 INFO aicoscientist: Parsed generated hypotheses: [{'title': 'Meta-Learning Curriculum Design for Autonomous Exploration', 'text': 'Hypothesis: A meta-learning agent can learn to design its own learning curriculum for exploration in novel, initially unknown environments. Rationale: Current curriculum learning often relies on hand-crafted curricula or pre-defined task distributions. This hypothesis explores autonomous curriculum design using meta-learning, allowing the agent to actively select tasks and environments that maximize its learning progress and exploration efficiency. This goes beyond existing work by focusing on *true* autonomy in curriculum creation, not just task selection from a fixed set. It addresses the gap of adapting exploration strategies to completely novel and unpredictable environments.'}, {'title': 'Hierarchical Intrinsic Motivation with Predictive Models for Skill Discovery', 'text': 'Hypothesis: A hierarchical reinforcement learning agent, driven by intrinsic motivation based on prediction error reduction at multiple levels of abstraction, can autonomously discover and learn a useful repertoire of skills. Rationale: Existing intrinsic motivation often focuses on immediate prediction error. This hypothesis proposes a hierarchical approach where higher levels predict lower-level actions and outcomes, driving the agent to learn skills that improve its ability to model its own behavior and the environment at different granularities. This differs from existing hierarchical RL by explicitly linking the hierarchy to intrinsic motivation and learning predictive models for skill abstraction, leading to more robust and adaptable skill discovery.'}, {'title': 'Lifelong Learning via Continual Model Merging and Knowledge Distillation', 'text': 'Hypothesis: Continual learning can be improved by dynamically merging pre-trained models trained on different tasks and distilling their knowledge into a single, unified model while preserving task-specific performance and mitigating catastrophic forgetting. Rationale: Current continual learning methods often struggle with catastrophic forgetting and scaling to a large number of tasks. This hypothesis explores a model merging approach where models trained on individual tasks are combined into a larger model using techniques like parameter averaging or attention-based merging. Knowledge distillation is then used to refine the merged model and prevent interference between tasks. This goes beyond current distillation techniques by focusing on the efficient merging of multiple pre-trained models, addressing the scalability limitations of existing continual learning methods.'}, {'title': 'Uncertainty-Aware Exploration using Bayesian Neural Networks and Active Learning', 'text': "Hypothesis: An agent using a Bayesian Neural Network (BNN) to represent its environment model, combined with active learning techniques to query the environment based on uncertainty estimates, can achieve more efficient exploration and learning compared to agents using deterministic models. Rationale: Traditional exploration methods often rely on heuristics or random exploration. This hypothesis leverages the uncertainty estimates provided by BNNs to guide exploration, focusing on regions of the environment where the agent's knowledge is most uncertain. Active learning is used to select the most informative actions based on these uncertainty estimates. This extends current BNN-based RL by explicitly using the uncertainty estimates for *active* exploration, leading to more targeted data acquisition and faster learning."}, {'title': 'Emergent Communication Protocols for Collaborative Self-Improvement', 'text': 'Hypothesis: A population of agents can develop emergent communication protocols that facilitate collaborative self-improvement through the sharing of learned strategies, experiences, and environmental models. Rationale: Current multi-agent learning often focuses on competition or pre-defined communication channels. This hypothesis explores the emergence of communication protocols that enable agents to share knowledge and accelerate their individual learning processes. The agents could share successful strategies, learned environmental models, or even raw experiences. This moves beyond current emergent communication by explicitly linking it to self-improvement, enabling agents to collectively learn and surpass the capabilities of individual agents.'}, {'title': 'Symbolic Abstraction and Reasoning for Generalization in Reinforcement Learning', 'text': "Hypothesis: Integrating symbolic abstraction and reasoning into a reinforcement learning agent's architecture allows for more robust generalization to unseen environments and tasks by enabling the agent to reason about high-level concepts and relationships. Rationale: Current RL agents often struggle to generalize to even slightly different environments. This hypothesis proposes combining RL with symbolic AI techniques, where the agent learns to abstract the environment into symbolic representations and use logical reasoning to plan and make decisions. This goes beyond end-to-end RL by incorporating symbolic reasoning, enabling the agent to transfer knowledge and generalize to new situations more effectively by reasoning about the underlying principles and relationships."}]
2025-05-31 19:22:10,636 INFO aicoscientist: Generated hypothesis: {'id': 'G1351', 'title': 'Meta-Learning Curriculum Design for Autonomous Exploration', 'text': 'Hypothesis: A meta-learning agent can learn to design its own learning curriculum for exploration in novel, initially unknown environments. Rationale: Current curriculum learning often relies on hand-crafted curricula or pre-defined task distributions. This hypothesis explores autonomous curriculum design using meta-learning, allowing the agent to actively select tasks and environments that maximize its learning progress and exploration efficiency. This goes beyond existing work by focusing on *true* autonomy in curriculum creation, not just task selection from a fixed set. It addresses the gap of adapting exploration strategies to completely novel and unpredictable environments.', 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:10,636 INFO aicoscientist: Generated hypothesis: {'id': 'G7295', 'title': 'Hierarchical Intrinsic Motivation with Predictive Models for Skill Discovery', 'text': 'Hypothesis: A hierarchical reinforcement learning agent, driven by intrinsic motivation based on prediction error reduction at multiple levels of abstraction, can autonomously discover and learn a useful repertoire of skills. Rationale: Existing intrinsic motivation often focuses on immediate prediction error. This hypothesis proposes a hierarchical approach where higher levels predict lower-level actions and outcomes, driving the agent to learn skills that improve its ability to model its own behavior and the environment at different granularities. This differs from existing hierarchical RL by explicitly linking the hierarchy to intrinsic motivation and learning predictive models for skill abstraction, leading to more robust and adaptable skill discovery.', 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:10,637 INFO aicoscientist: Generated hypothesis: {'id': 'G4199', 'title': 'Lifelong Learning via Continual Model Merging and Knowledge Distillation', 'text': 'Hypothesis: Continual learning can be improved by dynamically merging pre-trained models trained on different tasks and distilling their knowledge into a single, unified model while preserving task-specific performance and mitigating catastrophic forgetting. Rationale: Current continual learning methods often struggle with catastrophic forgetting and scaling to a large number of tasks. This hypothesis explores a model merging approach where models trained on individual tasks are combined into a larger model using techniques like parameter averaging or attention-based merging. Knowledge distillation is then used to refine the merged model and prevent interference between tasks. This goes beyond current distillation techniques by focusing on the efficient merging of multiple pre-trained models, addressing the scalability limitations of existing continual learning methods.', 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:10,637 INFO aicoscientist: Generated hypothesis: {'id': 'G3657', 'title': 'Uncertainty-Aware Exploration using Bayesian Neural Networks and Active Learning', 'text': "Hypothesis: An agent using a Bayesian Neural Network (BNN) to represent its environment model, combined with active learning techniques to query the environment based on uncertainty estimates, can achieve more efficient exploration and learning compared to agents using deterministic models. Rationale: Traditional exploration methods often rely on heuristics or random exploration. This hypothesis leverages the uncertainty estimates provided by BNNs to guide exploration, focusing on regions of the environment where the agent's knowledge is most uncertain. Active learning is used to select the most informative actions based on these uncertainty estimates. This extends current BNN-based RL by explicitly using the uncertainty estimates for *active* exploration, leading to more targeted data acquisition and faster learning.", 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:10,637 INFO aicoscientist: Generated hypothesis: {'id': 'G8633', 'title': 'Emergent Communication Protocols for Collaborative Self-Improvement', 'text': 'Hypothesis: A population of agents can develop emergent communication protocols that facilitate collaborative self-improvement through the sharing of learned strategies, experiences, and environmental models. Rationale: Current multi-agent learning often focuses on competition or pre-defined communication channels. This hypothesis explores the emergence of communication protocols that enable agents to share knowledge and accelerate their individual learning processes. The agents could share successful strategies, learned environmental models, or even raw experiences. This moves beyond current emergent communication by explicitly linking it to self-improvement, enabling agents to collectively learn and surpass the capabilities of individual agents.', 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:10,637 INFO aicoscientist: Generated hypothesis: {'id': 'G7402', 'title': 'Symbolic Abstraction and Reasoning for Generalization in Reinforcement Learning', 'text': "Hypothesis: Integrating symbolic abstraction and reasoning into a reinforcement learning agent's architecture allows for more robust generalization to unseen environments and tasks by enabling the agent to reason about high-level concepts and relationships. Rationale: Current RL agents often struggle to generalize to even slightly different environments. This hypothesis proposes combining RL with symbolic AI techniques, where the agent learns to abstract the environment into symbolic representations and use logical reasoning to plan and make decisions. This goes beyond end-to-end RL by incorporating symbolic reasoning, enabling the agent to transfer knowledge and generalize to new situations more effectively by reasoning about the underlying principles and relationships.", 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:10,648 INFO aicoscientist: Step 2: Reflection
2025-05-31 19:22:10,648 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:22:14,009 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3354.16ms, Tokens: 246/474, Retries: 0
2025-05-31 19:22:14,010 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis explores a promising direction in autonomous curriculum learning. While meta-learning for curriculum design is not entirely new, the emphasis on *true* autonomy and adaptation to *completely novel* environments increases its novelty. The feasibility hinges on the complexity of the environments and the efficiency of the meta-learning algorithm. Scaling this approach to high-dimensional, realistic environments will be a significant challenge. Careful consideration of the reward function and exploration strategy within the meta-learning loop is crucial. The distinction between task selection and curriculum design needs to be clearly defined and demonstrated in the experimental setup.",
  "references": [
    {
      "type": "arXiv",
      "id": "1905.10976",
      "title": "Meta-Curriculum Learning"
    },
    {
      "type": "arXiv",
      "id": "1707.04140",
      "title": "Teacher-Student Curriculum Learning"
    },
    {
      "type": "DOI",
      "id": "10.1109/ICRA.2018.8460635",
      "title": "Automatic Curriculum Learning for Deep RL: A Case Study on Robotic Manipulation",
      "venue": "ICRA 2018"
    },
    {
      "type": "Paper Title",
      "title": "POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Curricula via Paired Open-Ended Trailblazing",
      "venue": "GECCO 2019"
    },
    {
      "type": "arXiv",
      "id": "2010.08453",
      "title": "Self-Supervised Curriculum Learning"
    },
    {
      "type": "Paper Title",
      "title": "Curriculum Learning by Transferring Latent Task Structures",
      "venue": "NeurIPS 2020"
    }
  ]
}
```
2025-05-31 19:22:14,010 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The hypothesis explores a promising direction in autonomous curriculum learning. While meta-learning for curriculum design is not entirely new, the emphasis on *true* autonomy and adaptation to *completely novel* environments increases its novelty. The feasibility hinges on the complexity of the environments and the efficiency of the meta-learning algorithm. Scaling this approach to high-dimensional, realistic environments will be a significant challenge. Careful consideration of the reward function and exploration strategy within the meta-learning loop is crucial. The distinction between task selection and curriculum design needs to be clearly defined and demonstrated in the experimental setup.', 'references': [{'type': 'arXiv', 'id': '1905.10976', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04140', 'title': 'Teacher-Student Curriculum Learning'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460635', 'title': 'Automatic Curriculum Learning for Deep RL: A Case Study on Robotic Manipulation', 'venue': 'ICRA 2018'}, {'type': 'Paper Title', 'title': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Curricula via Paired Open-Ended Trailblazing', 'venue': 'GECCO 2019'}, {'type': 'arXiv', 'id': '2010.08453', 'title': 'Self-Supervised Curriculum Learning'}, {'type': 'Paper Title', 'title': 'Curriculum Learning by Transferring Latent Task Structures', 'venue': 'NeurIPS 2020'}]}
2025-05-31 19:22:14,010 INFO aicoscientist: Reviewed hypothesis: G1351, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:22:14,010 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:22:17,593 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3576.02ms, Tokens: 252/468, Retries: 0
2025-05-31 19:22:17,593 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis builds upon existing work in hierarchical reinforcement learning and intrinsic motivation, but the explicit connection between hierarchical levels, prediction error reduction at *multiple* levels, and autonomous skill discovery makes it reasonably novel. The feasibility is moderate because implementing and training such a system, especially with multiple levels of abstraction, can be computationally expensive and require careful design of the reward functions and state representations. The success hinges on the specific implementation details and the ability to effectively learn predictive models at each level of the hierarchy.",
  "references": [
    {
      "type": "arXiv",
      "id": "1604.00772",
      "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation"
    },
    {
      "type": "DOI",
      "id": "10.1109/ICDL-EpiRob.2013.6673656",
      "title": "Intrinsic Motivation for Hierarchical Skill Acquisition"
    },
    {
      "type": "Paper",
      "title": "HAC: Hierarchical Actor-Critic",
      "venue": "NeurIPS 2018"
    },
    {
      "type": "Paper",
      "title": "Variational Option Discovery using Hierarchical Intrinsic Rewards",
      "venue": "ICML 2018"
    },
    {
      "type": "Paper",
      "title": "Learning Options via Spectral Clustering",
      "venue": "NeurIPS 2007"
    },
    {
      "type": "Paper",
      "title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
      "venue": "ICML 2017"
    },
     {
      "type": "Paper",
      "title": "Self-Supervised Hindsight Planning",
      "venue": "ICLR 2020"
    }
  ]
}
```
2025-05-31 19:22:17,594 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The hypothesis builds upon existing work in hierarchical reinforcement learning and intrinsic motivation, but the explicit connection between hierarchical levels, prediction error reduction at *multiple* levels, and autonomous skill discovery makes it reasonably novel. The feasibility is moderate because implementing and training such a system, especially with multiple levels of abstraction, can be computationally expensive and require careful design of the reward functions and state representations. The success hinges on the specific implementation details and the ability to effectively learn predictive models at each level of the hierarchy.', 'references': [{'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}, {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}, {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}, {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}]}
2025-05-31 19:22:17,594 INFO aicoscientist: Reviewed hypothesis: G7295, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:22:17,594 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:22:21,451 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3854.52ms, Tokens: 270/488, Retries: 0
2025-05-31 19:22:21,451 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis presents a reasonable approach to continual learning by combining model merging and knowledge distillation. While the individual components (model merging and knowledge distillation) are well-established, the specific combination and focus on dynamically merging *pre-trained* models for continual learning offers some novelty. The success hinges on the effectiveness of the merging strategy and the ability of knowledge distillation to truly mitigate catastrophic forgetting in the merged model. A key challenge will be defining a robust merging strategy that generalizes well across various task domains and model architectures. The dynamic aspect of merging adds complexity but also potential for improved performance compared to static merging approaches. The rationale is well-articulated.",
  "references": [
    "arXiv:1801.10127 (Elastic Weight Consolidation)",
    "arXiv:1606.07141 (Learning without Forgetting)",
    "arXiv:1602.04902 (Distilling the Knowledge in a Neural Network)",
    "arXiv:1705.07129 (PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning)",
    "arXiv:1812.00424 (Synaptic Intelligence: Continual Learning with Synaptic Intelligence)",
    "arXiv:2002.00810 (Gradient Episodic Memory for Continual Learning)",
    "arXiv:2007.00894 (Continual Learning via Neural Pruning)",
    "arXiv:2106.04497 (Model Merging via Uniform Weight Averaging)",
    "arXiv:2202.02012 (Task-Agnostic Continual Learning via Meta-Learning)",
    "Tevfik Koşar, Mustafa Sert, and Pinar Duygulu. 2023. Model Merging: A Survey. ACM Comput. Surv. 55, 11, Article 223 (December 2023), 33 pages. DOI: https://doi.org/10.1145/3574243"
  ]
}
```
2025-05-31 19:22:21,452 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The hypothesis presents a reasonable approach to continual learning by combining model merging and knowledge distillation. While the individual components (model merging and knowledge distillation) are well-established, the specific combination and focus on dynamically merging *pre-trained* models for continual learning offers some novelty. The success hinges on the effectiveness of the merging strategy and the ability of knowledge distillation to truly mitigate catastrophic forgetting in the merged model. A key challenge will be defining a robust merging strategy that generalizes well across various task domains and model architectures. The dynamic aspect of merging adds complexity but also potential for improved performance compared to static merging approaches. The rationale is well-articulated.', 'references': ['arXiv:1801.10127 (Elastic Weight Consolidation)', 'arXiv:1606.07141 (Learning without Forgetting)', 'arXiv:1602.04902 (Distilling the Knowledge in a Neural Network)', 'arXiv:1705.07129 (PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning)', 'arXiv:1812.00424 (Synaptic Intelligence: Continual Learning with Synaptic Intelligence)', 'arXiv:2002.00810 (Gradient Episodic Memory for Continual Learning)', 'arXiv:2007.00894 (Continual Learning via Neural Pruning)', 'arXiv:2106.04497 (Model Merging via Uniform Weight Averaging)', 'arXiv:2202.02012 (Task-Agnostic Continual Learning via Meta-Learning)', 'Tevfik Koşar, Mustafa Sert, and Pinar Duygulu. 2023. Model Merging: A Survey. ACM Comput. Surv. 55, 11, Article 223 (December 2023), 33 pages. DOI: https://doi.org/10.1145/3574243']}
2025-05-31 19:22:32,426 INFO aicoscientist: Reviewed hypothesis: G4199, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:22:32,426 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:22:35,104 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 2670.20ms, Tokens: 272/373, Retries: 0
2025-05-31 19:22:35,104 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis is moderately novel. While BNNs have been used in reinforcement learning to represent uncertainty, the explicit combination with active learning for targeted exploration, leveraging BNN uncertainty estimates as the *primary* driver for action selection, is a valuable extension. The success hinges on the effectiveness of the active learning strategy and the computational cost of BNN inference and training. Feasibility is moderate because BNNs can be computationally expensive, and designing a robust active learning strategy that balances exploration and exploitation can be challenging. The performance will likely depend on the complexity of the environment.",
  "references": [
    {
      "title": "Bayesian Deep Reinforcement Learning",
      "venue": "ICML 2016"
    },
    {
      "title": "Active Learning for Reinforcement Learning",
      "venue": "Machine Learning 2005"
    },
    {
      "arXiv_ID": "1802.06866" ,
      "title": "Exploration with Bayesian Neural Networks for Reinforcement Learning"
    },
    {
      "arXiv_ID": "1901.08753",
      "title": "Deep Exploration via Bootstrapped DQN"
    },
    {
      "title": "Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning",
      "venue": "NeurIPS 2019"
    },
    {
      "title": "Variational Bayesian Reinforcement Learning",
      "venue": "AISTATS 2018"
    }
  ]
}
```
2025-05-31 19:22:35,104 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The hypothesis is moderately novel. While BNNs have been used in reinforcement learning to represent uncertainty, the explicit combination with active learning for targeted exploration, leveraging BNN uncertainty estimates as the *primary* driver for action selection, is a valuable extension. The success hinges on the effectiveness of the active learning strategy and the computational cost of BNN inference and training. Feasibility is moderate because BNNs can be computationally expensive, and designing a robust active learning strategy that balances exploration and exploitation can be challenging. The performance will likely depend on the complexity of the environment.', 'references': [{'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Active Learning for Reinforcement Learning', 'venue': 'Machine Learning 2005'}, {'arXiv_ID': '1802.06866', 'title': 'Exploration with Bayesian Neural Networks for Reinforcement Learning'}, {'arXiv_ID': '1901.08753', 'title': 'Deep Exploration via Bootstrapped DQN'}, {'title': 'Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Variational Bayesian Reinforcement Learning', 'venue': 'AISTATS 2018'}]}
2025-05-31 19:22:35,105 INFO aicoscientist: Reviewed hypothesis: G3657, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:22:35,105 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:22:38,341 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3230.10ms, Tokens: 247/433, Retries: 0
2025-05-31 19:22:38,342 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis builds upon existing work in emergent communication and multi-agent learning, but the explicit focus on collaborative self-improvement through knowledge sharing and its impact on individual agent performance adds a novel dimension. The feasibility depends heavily on the complexity of the environment and the learning algorithms used. Specifically, designing appropriate reward structures and communication mechanisms that incentivize knowledge sharing without exploitation or free-riding will be challenging. The success also hinges on the agents' ability to effectively process and integrate the shared information into their individual learning processes. Further specification of the agent architecture, communication constraints, and learning algorithms would strengthen the hypothesis.",
  "references": [
    {
      "title": "Learning to Communicate with Deep Multi-Agent Reinforcement Learning",
      "venue": "NeurIPS 2016"
    },
    {
      "title": "Emergent Communication in Multi-Agent Systems",
      "venue": "Foundations and Trends in Machine Learning, 2020"
    },
    {
      "title": "Communication-Efficient Distributed Deep Reinforcement Learning",
      "arXiv_ID": "1802.01528"
    },
    {
      "title": "TarMAC: Targeted Multi-Agent Communication",
      "venue": "ICML 2018"
    },
    {
      "title": "Learning to Share in Multi-Agent Reinforcement Learning",
      "venue": "AAMAS 2020"
    },
    {
      "title": "Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning",
      "venue": "ICML 2018"
    },
    {
      "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
      "venue": "NeurIPS 2017"
    }
  ]
}
```
2025-05-31 19:22:38,342 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': "The hypothesis builds upon existing work in emergent communication and multi-agent learning, but the explicit focus on collaborative self-improvement through knowledge sharing and its impact on individual agent performance adds a novel dimension. The feasibility depends heavily on the complexity of the environment and the learning algorithms used. Specifically, designing appropriate reward structures and communication mechanisms that incentivize knowledge sharing without exploitation or free-riding will be challenging. The success also hinges on the agents' ability to effectively process and integrate the shared information into their individual learning processes. Further specification of the agent architecture, communication constraints, and learning algorithms would strengthen the hypothesis.", 'references': [{'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems', 'venue': 'Foundations and Trends in Machine Learning, 2020'}, {'title': 'Communication-Efficient Distributed Deep Reinforcement Learning', 'arXiv_ID': '1802.01528'}, {'title': 'TarMAC: Targeted Multi-Agent Communication', 'venue': 'ICML 2018'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'AAMAS 2020'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning', 'venue': 'ICML 2018'}, {'title': 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'venue': 'NeurIPS 2017'}]}
2025-05-31 19:22:38,342 INFO aicoscientist: Reviewed hypothesis: G8633, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:22:38,342 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:22:41,452 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3102.58ms, Tokens: 255/423, Retries: 0
2025-05-31 19:22:41,453 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The integration of symbolic reasoning and reinforcement learning is a well-explored area, but the specific implementation details and the extent to which the agent can truly 'reason' about high-level concepts will determine the actual novelty and impact. The success hinges on effectively bridging the gap between continuous RL control and discrete symbolic reasoning. The rationale is sound, but the challenge lies in creating a system that is both computationally efficient and capable of handling the complexity of real-world environments. The robustness claim needs to be carefully validated through rigorous experimentation across a variety of unseen environments and tasks.",
  "references": [
    {
      "title": "Neuro-Symbolic Reinforcement Learning: A Survey",
      "venue": "arXiv preprint arXiv:2010.06278"
    },
    {
      "title": "Learning Programs for Abstraction and Reasoning",
      "venue": "Advances in Neural Information Processing Systems 31 (NeurIPS 2018)"
    },
    {
      "title": "Grounded Language Learning in a Simulated 3D World",
      "venue": "ICLR 2017"
    },
    {
      "title": "SRL: Symbolic Reinforcement Learning",
      "venue": "International Conference on Automated Planning and Scheduling (ICAPS) 2018"
    },
    {
      "arXiv_ID": "2106.09689"
    },
    {
      "DOI": "10.1109/ICRA46639.2022.9811713"
    },
    {
      "title": "Differentiable Reasoning over Symbolic Knowledge",
      "venue": "NeurIPS 2017"
    }
  ]
}
```
2025-05-31 19:22:41,453 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': "The integration of symbolic reasoning and reinforcement learning is a well-explored area, but the specific implementation details and the extent to which the agent can truly 'reason' about high-level concepts will determine the actual novelty and impact. The success hinges on effectively bridging the gap between continuous RL control and discrete symbolic reasoning. The rationale is sound, but the challenge lies in creating a system that is both computationally efficient and capable of handling the complexity of real-world environments. The robustness claim needs to be carefully validated through rigorous experimentation across a variety of unseen environments and tasks.", 'references': [{'title': 'Neuro-Symbolic Reinforcement Learning: A Survey', 'venue': 'arXiv preprint arXiv:2010.06278'}, {'title': 'Learning Programs for Abstraction and Reasoning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'ICLR 2017'}, {'title': 'SRL: Symbolic Reinforcement Learning', 'venue': 'International Conference on Automated Planning and Scheduling (ICAPS) 2018'}, {'arXiv_ID': '2106.09689'}, {'DOI': '10.1109/ICRA46639.2022.9811713'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'NeurIPS 2017'}]}
2025-05-31 19:22:41,453 INFO aicoscientist: Reviewed hypothesis: G7402, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:22:41,453 INFO aicoscientist: Step 3: Ranking 1
2025-05-31 19:22:41,453 INFO aicoscientist: Running tournament with 15 pairs.
2025-05-31 19:22:41,454 INFO aicoscientist: Debate: G1351 (score 4) vs G8633 (score 4) => Winner: G1351
2025-05-31 19:22:41,454 INFO aicoscientist: Updated Elo: Winner G1351 -> 1216.00, Loser G8633 -> 1184.00
2025-05-31 19:22:41,457 INFO aicoscientist: Debate: G1351 (score 4) vs G4199 (score 4) => Winner: G1351
2025-05-31 19:22:41,457 INFO aicoscientist: Updated Elo: Winner G1351 -> 1231.26, Loser G4199 -> 1184.74
2025-05-31 19:22:41,460 INFO aicoscientist: Debate: G1351 (score 4) vs G7402 (score 4) => Winner: G1351
2025-05-31 19:22:41,460 INFO aicoscientist: Updated Elo: Winner G1351 -> 1245.83, Loser G7402 -> 1185.44
2025-05-31 19:22:41,462 INFO aicoscientist: Debate: G1351 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:22:41,462 INFO aicoscientist: Updated Elo: Winner G3657 -> 1218.10, Loser G1351 -> 1227.73
2025-05-31 19:22:41,463 INFO aicoscientist: Debate: G1351 (score 4) vs G7295 (score 4) => Winner: G7295
2025-05-31 19:22:41,463 INFO aicoscientist: Updated Elo: Winner G7295 -> 1217.27, Loser G1351 -> 1210.46
2025-05-31 19:22:41,464 INFO aicoscientist: Debate: G8633 (score 4) vs G4199 (score 4) => Winner: G4199
2025-05-31 19:22:41,464 INFO aicoscientist: Updated Elo: Winner G4199 -> 1200.70, Loser G8633 -> 1168.03
2025-05-31 19:22:41,465 INFO aicoscientist: Debate: G8633 (score 4) vs G7402 (score 4) => Winner: G8633
2025-05-31 19:22:41,465 INFO aicoscientist: Updated Elo: Winner G8633 -> 1184.83, Loser G7402 -> 1168.64
2025-05-31 19:22:41,467 INFO aicoscientist: Debate: G8633 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:22:41,467 INFO aicoscientist: Updated Elo: Winner G3657 -> 1232.57, Loser G8633 -> 1170.36
2025-05-31 19:22:41,468 INFO aicoscientist: Debate: G8633 (score 4) vs G7295 (score 4) => Winner: G8633
2025-05-31 19:22:41,468 INFO aicoscientist: Updated Elo: Winner G8633 -> 1188.51, Loser G7295 -> 1199.13
2025-05-31 19:22:41,469 INFO aicoscientist: Debate: G4199 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:22:41,469 INFO aicoscientist: Updated Elo: Winner G7402 -> 1186.11, Loser G4199 -> 1183.23
2025-05-31 19:22:41,470 INFO aicoscientist: Debate: G4199 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:22:41,470 INFO aicoscientist: Updated Elo: Winner G3657 -> 1246.31, Loser G4199 -> 1169.49
2025-05-31 19:22:41,471 INFO aicoscientist: Debate: G4199 (score 4) vs G7295 (score 4) => Winner: G4199
2025-05-31 19:22:41,471 INFO aicoscientist: Updated Elo: Winner G4199 -> 1186.85, Loser G7295 -> 1181.77
2025-05-31 19:22:41,472 INFO aicoscientist: Debate: G7402 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:22:41,472 INFO aicoscientist: Updated Elo: Winner G3657 -> 1259.57, Loser G7402 -> 1172.85
2025-05-31 19:22:41,473 INFO aicoscientist: Debate: G7402 (score 4) vs G7295 (score 4) => Winner: G7402
2025-05-31 19:22:41,473 INFO aicoscientist: Updated Elo: Winner G7402 -> 1189.26, Loser G7295 -> 1165.35
2025-05-31 19:22:41,473 INFO aicoscientist: Debate: G3657 (score 4) vs G7295 (score 4) => Winner: G7295
2025-05-31 19:22:41,473 INFO aicoscientist: Updated Elo: Winner G7295 -> 1185.59, Loser G3657 -> 1239.33
2025-05-31 19:22:41,474 INFO aicoscientist: Step 4: Evolution
2025-05-31 19:22:41,474 INFO aicoscientist: Combining hypotheses G3657 and G1351 into E5336
2025-05-31 19:22:41,474 INFO aicoscientist: Evolved hypothesis created: E5336 from parents ['G3657', 'G1351']
2025-05-31 19:22:41,475 INFO aicoscientist: Step 4a: Reviewing Evolved Hypotheses
2025-05-31 19:22:41,475 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:22:45,344 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3861.47ms, Tokens: 393/595, Retries: 0
2025-05-31 19:22:45,344 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The combination of BNN-based active exploration and meta-learning for curriculum design is promising but presents significant challenges. While both components have been explored individually, their synergistic integration for autonomous exploration in novel environments is less common. The feasibility hinges on the computational cost of BNNs, the stability of meta-learning algorithms, and the design of a suitable reward structure for the meta-learner to incentivize effective curriculum design. The success will also depend on the complexity of the target environments. Demonstrating a clear advantage over simpler exploration strategies (e.g., epsilon-greedy, Thompson sampling) in sufficiently complex environments is crucial.",
  "references": [
    {
      "title": "Bayesian Neural Networks",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "year": 1991
    },
    {
      "title": "Active Learning Literature Survey",
      "venue": "ACM Computing Surveys",
      "year": 2009
    },
    {
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "venue": "International Conference on Machine Learning (ICML)",
      "year": 2017
    },
    {
      "title": "Curriculum Learning",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "year": 2009
    },
    {
      "title": "Learning to Explore via Meta-Reinforcement Learning",
      "venue": "International Conference on Learning Representations (ICLR)",
      "year": 2017
    },
    {
      "title": "Efficient Exploration through Bayesian Deep Reinforcement Learning",
      "venue": "Conference on Robot Learning (CoRL)",
      "year": 2018
    },
    {
      "title": "Meta-Curriculum Learning for Domain Adaptation",
      "venue": "International Conference on Computer Vision (ICCV)",
      "year": 2021
    },
    {
      "arXiv_ID": "1802.04849",
      "title": "Variational Information Bottleneck for Reinforcement Learning"
    },
    {
      "arXiv_ID": "2006.05961",
      "title": "Uncertainty-Aware Reinforcement Learning for Safe Exploration"
    },
    {
      "arXiv_ID": "2106.02762",
      "title": "Meta-Learning for Curriculum Design"
    }
  ]
}
```
2025-05-31 19:22:45,345 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The combination of BNN-based active exploration and meta-learning for curriculum design is promising but presents significant challenges. While both components have been explored individually, their synergistic integration for autonomous exploration in novel environments is less common. The feasibility hinges on the computational cost of BNNs, the stability of meta-learning algorithms, and the design of a suitable reward structure for the meta-learner to incentivize effective curriculum design. The success will also depend on the complexity of the target environments. Demonstrating a clear advantage over simpler exploration strategies (e.g., epsilon-greedy, Thompson sampling) in sufficiently complex environments is crucial.', 'references': [{'title': 'Bayesian Neural Networks', 'venue': 'Advances in Neural Information Processing Systems (NeurIPS)', 'year': 1991}, {'title': 'Active Learning Literature Survey', 'venue': 'ACM Computing Surveys', 'year': 2009}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML)', 'year': 2017}, {'title': 'Curriculum Learning', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'year': 2009}, {'title': 'Learning to Explore via Meta-Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR)', 'year': 2017}, {'title': 'Efficient Exploration through Bayesian Deep Reinforcement Learning', 'venue': 'Conference on Robot Learning (CoRL)', 'year': 2018}, {'title': 'Meta-Curriculum Learning for Domain Adaptation', 'venue': 'International Conference on Computer Vision (ICCV)', 'year': 2021}, {'arXiv_ID': '1802.04849', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'arXiv_ID': '2006.05961', 'title': 'Uncertainty-Aware Reinforcement Learning for Safe Exploration'}, {'arXiv_ID': '2106.02762', 'title': 'Meta-Learning for Curriculum Design'}]}
2025-05-31 19:22:45,345 INFO aicoscientist: Reviewed hypothesis: E5336, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:22:45,345 INFO aicoscientist: Step 5: Ranking 2
2025-05-31 19:22:45,346 INFO aicoscientist: Running tournament with 21 pairs.
2025-05-31 19:22:45,346 INFO aicoscientist: Debate: E5336 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:22:45,346 INFO aicoscientist: Updated Elo: Winner G7402 -> 1205.76, Loser E5336 -> 1183.51
2025-05-31 19:22:45,348 INFO aicoscientist: Debate: E5336 (score 4) vs G7295 (score 4) => Winner: G7295
2025-05-31 19:22:45,349 INFO aicoscientist: Updated Elo: Winner G7295 -> 1201.49, Loser E5336 -> 1167.60
2025-05-31 19:22:45,352 INFO aicoscientist: Debate: E5336 (score 4) vs G1351 (score 4) => Winner: G1351
2025-05-31 19:22:45,352 INFO aicoscientist: Updated Elo: Winner G1351 -> 1224.49, Loser E5336 -> 1153.57
2025-05-31 19:22:45,354 INFO aicoscientist: Debate: E5336 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:22:45,354 INFO aicoscientist: Updated Elo: Winner G3657 -> 1251.46, Loser E5336 -> 1141.44
2025-05-31 19:22:45,355 INFO aicoscientist: Debate: E5336 (score 4) vs G8633 (score 4) => Winner: G8633
2025-05-31 19:22:45,355 INFO aicoscientist: Updated Elo: Winner G8633 -> 1202.35, Loser E5336 -> 1127.59
2025-05-31 19:22:45,356 INFO aicoscientist: Debate: E5336 (score 4) vs G4199 (score 4) => Winner: E5336
2025-05-31 19:22:45,356 INFO aicoscientist: Updated Elo: Winner E5336 -> 1146.29, Loser G4199 -> 1168.15
2025-05-31 19:22:45,357 INFO aicoscientist: Debate: G7402 (score 4) vs G7295 (score 4) => Winner: G7402
2025-05-31 19:22:45,357 INFO aicoscientist: Updated Elo: Winner G7402 -> 1221.56, Loser G7295 -> 1185.69
2025-05-31 19:22:45,358 INFO aicoscientist: Debate: G7402 (score 4) vs G1351 (score 4) => Winner: G7402
2025-05-31 19:22:45,358 INFO aicoscientist: Updated Elo: Winner G7402 -> 1237.70, Loser G1351 -> 1208.36
2025-05-31 19:22:45,359 INFO aicoscientist: Debate: G7402 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:22:45,360 INFO aicoscientist: Updated Elo: Winner G3657 -> 1266.83, Loser G7402 -> 1222.33
2025-05-31 19:22:45,361 INFO aicoscientist: Debate: G7402 (score 4) vs G8633 (score 4) => Winner: G8633
2025-05-31 19:22:45,361 INFO aicoscientist: Updated Elo: Winner G8633 -> 1219.27, Loser G7402 -> 1205.41
2025-05-31 19:22:45,361 INFO aicoscientist: Debate: G7402 (score 4) vs G4199 (score 4) => Winner: G4199
2025-05-31 19:22:45,362 INFO aicoscientist: Updated Elo: Winner G4199 -> 1185.86, Loser G7402 -> 1187.70
2025-05-31 19:22:45,362 INFO aicoscientist: Debate: G7295 (score 4) vs G1351 (score 4) => Winner: G7295
2025-05-31 19:22:45,362 INFO aicoscientist: Updated Elo: Winner G7295 -> 1202.73, Loser G1351 -> 1191.31
2025-05-31 19:22:45,363 INFO aicoscientist: Debate: G7295 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:22:45,363 INFO aicoscientist: Updated Elo: Winner G3657 -> 1279.91, Loser G7295 -> 1189.65
2025-05-31 19:22:45,364 INFO aicoscientist: Debate: G7295 (score 4) vs G8633 (score 4) => Winner: G7295
2025-05-31 19:22:45,364 INFO aicoscientist: Updated Elo: Winner G7295 -> 1207.01, Loser G8633 -> 1201.91
2025-05-31 19:22:45,365 INFO aicoscientist: Debate: G7295 (score 4) vs G4199 (score 4) => Winner: G4199
2025-05-31 19:22:45,365 INFO aicoscientist: Updated Elo: Winner G4199 -> 1202.83, Loser G7295 -> 1190.04
2025-05-31 19:22:45,365 INFO aicoscientist: Debate: G1351 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:22:45,365 INFO aicoscientist: Updated Elo: Winner G3657 -> 1291.92, Loser G1351 -> 1179.31
2025-05-31 19:22:45,366 INFO aicoscientist: Debate: G1351 (score 4) vs G8633 (score 4) => Winner: G8633
2025-05-31 19:22:45,366 INFO aicoscientist: Updated Elo: Winner G8633 -> 1216.87, Loser G1351 -> 1164.35
2025-05-31 19:22:45,367 INFO aicoscientist: Debate: G1351 (score 4) vs G4199 (score 4) => Winner: G4199
2025-05-31 19:22:45,367 INFO aicoscientist: Updated Elo: Winner G4199 -> 1217.06, Loser G1351 -> 1150.11
2025-05-31 19:22:45,368 INFO aicoscientist: Debate: G3657 (score 4) vs G8633 (score 4) => Winner: G3657
2025-05-31 19:22:45,368 INFO aicoscientist: Updated Elo: Winner G3657 -> 1304.51, Loser G8633 -> 1204.28
2025-05-31 19:22:45,369 INFO aicoscientist: Debate: G3657 (score 4) vs G4199 (score 4) => Winner: G4199
2025-05-31 19:22:45,369 INFO aicoscientist: Updated Elo: Winner G4199 -> 1237.01, Loser G3657 -> 1284.57
2025-05-31 19:22:45,370 INFO aicoscientist: Debate: G8633 (score 4) vs G4199 (score 4) => Winner: G4199
2025-05-31 19:22:45,370 INFO aicoscientist: Updated Elo: Winner G4199 -> 1251.50, Loser G8633 -> 1189.78
2025-05-31 19:22:45,370 INFO aicoscientist: Step 6: Proximity Analysis
2025-05-31 19:22:45,370 INFO aicoscientist: Loading sentence transformer model: all-MiniLM-L6-v2...
2025-05-31 19:22:47,048 INFO aicoscientist: Sentence transformer model loaded successfully.
2025-05-31 19:22:48,046 INFO aicoscientist: Built proximity graph adjacency with 7 nodes.
2025-05-31 19:22:48,046 INFO aicoscientist: Step 7: Meta-Review
2025-05-31 19:22:48,046 INFO aicoscientist: Top hypotheses for meta-review: ['G3657', 'G4199', 'G7295']
2025-05-31 19:22:48,048 INFO aicoscientist: Meta-review complete: {'meta_review_critique': ['Overall hypothesis quality seems reasonable based on automated review.'], 'research_overview': {'top_ranked_hypotheses': [{'id': 'G3657', 'title': 'Uncertainty-Aware Exploration using Bayesian Neural Networks and Active Learning', 'text': "Hypothesis: An agent using a Bayesian Neural Network (BNN) to represent its environment model, combined with active learning techniques to query the environment based on uncertainty estimates, can achieve more efficient exploration and learning compared to agents using deterministic models. Rationale: Traditional exploration methods often rely on heuristics or random exploration. This hypothesis leverages the uncertainty estimates provided by BNNs to guide exploration, focusing on regions of the environment where the agent's knowledge is most uncertain. Active learning is used to select the most informative actions based on these uncertainty estimates. This extends current BNN-based RL by explicitly using the uncertainty estimates for *active* exploration, leading to more targeted data acquisition and faster learning.", 'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'elo_score': 1284.568601100527, 'review_comments': ['The hypothesis is moderately novel. While BNNs have been used in reinforcement learning to represent uncertainty, the explicit combination with active learning for targeted exploration, leveraging BNN uncertainty estimates as the *primary* driver for action selection, is a valuable extension. The success hinges on the effectiveness of the active learning strategy and the computational cost of BNN inference and training. Feasibility is moderate because BNNs can be computationally expensive, and designing a robust active learning strategy that balances exploration and exploitation can be challenging. The performance will likely depend on the complexity of the environment.'], 'references': [{'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Active Learning for Reinforcement Learning', 'venue': 'Machine Learning 2005'}, {'arXiv_ID': '1802.06866', 'title': 'Exploration with Bayesian Neural Networks for Reinforcement Learning'}, {'arXiv_ID': '1901.08753', 'title': 'Deep Exploration via Bootstrapped DQN'}, {'title': 'Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Variational Bayesian Reinforcement Learning', 'venue': 'AISTATS 2018'}], 'is_active': True, 'parent_ids': []}, {'id': 'G4199', 'title': 'Lifelong Learning via Continual Model Merging and Knowledge Distillation', 'text': 'Hypothesis: Continual learning can be improved by dynamically merging pre-trained models trained on different tasks and distilling their knowledge into a single, unified model while preserving task-specific performance and mitigating catastrophic forgetting. Rationale: Current continual learning methods often struggle with catastrophic forgetting and scaling to a large number of tasks. This hypothesis explores a model merging approach where models trained on individual tasks are combined into a larger model using techniques like parameter averaging or attention-based merging. Knowledge distillation is then used to refine the merged model and prevent interference between tasks. This goes beyond current distillation techniques by focusing on the efficient merging of multiple pre-trained models, addressing the scalability limitations of existing continual learning methods.', 'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'elo_score': 1251.5049734518261, 'review_comments': ['The hypothesis presents a reasonable approach to continual learning by combining model merging and knowledge distillation. While the individual components (model merging and knowledge distillation) are well-established, the specific combination and focus on dynamically merging *pre-trained* models for continual learning offers some novelty. The success hinges on the effectiveness of the merging strategy and the ability of knowledge distillation to truly mitigate catastrophic forgetting in the merged model. A key challenge will be defining a robust merging strategy that generalizes well across various task domains and model architectures. The dynamic aspect of merging adds complexity but also potential for improved performance compared to static merging approaches. The rationale is well-articulated.'], 'references': ['arXiv:1801.10127 (Elastic Weight Consolidation)', 'arXiv:1606.07141 (Learning without Forgetting)', 'arXiv:1602.04902 (Distilling the Knowledge in a Neural Network)', 'arXiv:1705.07129 (PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning)', 'arXiv:1812.00424 (Synaptic Intelligence: Continual Learning with Synaptic Intelligence)', 'arXiv:2002.00810 (Gradient Episodic Memory for Continual Learning)', 'arXiv:2007.00894 (Continual Learning via Neural Pruning)', 'arXiv:2106.04497 (Model Merging via Uniform Weight Averaging)', 'arXiv:2202.02012 (Task-Agnostic Continual Learning via Meta-Learning)', 'Tevfik Koşar, Mustafa Sert, and Pinar Duygulu. 2023. Model Merging: A Survey. ACM Comput. Surv. 55, 11, Article 223 (December 2023), 33 pages. DOI: https://doi.org/10.1145/3574243'], 'is_active': True, 'parent_ids': []}, {'id': 'G7295', 'title': 'Hierarchical Intrinsic Motivation with Predictive Models for Skill Discovery', 'text': 'Hypothesis: A hierarchical reinforcement learning agent, driven by intrinsic motivation based on prediction error reduction at multiple levels of abstraction, can autonomously discover and learn a useful repertoire of skills. Rationale: Existing intrinsic motivation often focuses on immediate prediction error. This hypothesis proposes a hierarchical approach where higher levels predict lower-level actions and outcomes, driving the agent to learn skills that improve its ability to model its own behavior and the environment at different granularities. This differs from existing hierarchical RL by explicitly linking the hierarchy to intrinsic motivation and learning predictive models for skill abstraction, leading to more robust and adaptable skill discovery.', 'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'elo_score': 1190.0393610457472, 'review_comments': ['The hypothesis builds upon existing work in hierarchical reinforcement learning and intrinsic motivation, but the explicit connection between hierarchical levels, prediction error reduction at *multiple* levels, and autonomous skill discovery makes it reasonably novel. The feasibility is moderate because implementing and training such a system, especially with multiple levels of abstraction, can be computationally expensive and require careful design of the reward functions and state representations. The success hinges on the specific implementation details and the ability to effectively learn predictive models at each level of the hierarchy.'], 'references': [{'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}, {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}, {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}, {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}], 'is_active': True, 'parent_ids': []}], 'suggested_next_steps': ['Refine top hypotheses based on review comments.', 'Consider exploring areas with fewer, less connected hypotheses (if any).', 'Seek external expert feedback on top candidates.']}}
2025-05-31 19:22:48,048 INFO aicoscientist: --- Cycle 1 Complete ---
2025-05-31 19:22:48,048 INFO aicoscientist: Auto-run cycle completed successfully
2025-05-31 19:22:48,048 INFO aicoscientist: --- Endpoint /research_goal END ---
2025-05-31 19:22:48,155 INFO aicoscientist: --- Endpoint /run_cycle START ---
2025-05-31 19:22:48,156 INFO aicoscientist: Attempting to run cycle 2 for goal: make self-improving ai systems
2025-05-31 19:22:48,156 INFO aicoscientist: Calling supervisor.run_cycle...
2025-05-31 19:22:48,156 INFO aicoscientist: --- Starting Cycle 2 ---
2025-05-31 19:22:48,156 INFO aicoscientist: Step 1: Generation
2025-05-31 19:22:48,156 INFO aicoscientist: LLM generation called with prompt: Research Goal: make self-improving ai systems
Constraints: {}
Existing Hypothesis IDs: ['G1351', 'G7295', 'G4199', 'G3657', 'G8633', 'G7402', 'E5336']
Please propose 6 novel and feasible hypotheses with rationale, avoiding duplication with existing IDs. Consider how your hypotheses can advance beyond the current literature while addressing gaps or unexplored directions.
, num_hypotheses: 6, temperature: 0.70
2025-05-31 19:22:54,896 INFO aicoscientist: LLM call completed - Type: generation, Model: google/gemini-2.0-flash-001, Success: True, Time: 6732.60ms, Tokens: 128/1014, Retries: 0
2025-05-31 19:22:54,896 INFO aicoscientist: LLM generation response: ```json
[
  {
    "title": "Hierarchical Meta-Learning for Curriculum Discovery in Self-Improving Agents",
    "text": "Hypothesis: A hierarchical meta-learning architecture, where a higher-level meta-learner dynamically generates curricula for a lower-level learning agent, will result in more efficient and robust self-improvement compared to fixed curricula or single-level meta-learning. Rationale: Current meta-learning often relies on pre-defined or hand-crafted curricula, limiting adaptation to novel environments or tasks. A hierarchical approach allows the system to discover optimal learning trajectories by meta-learning the curriculum itself, allowing faster convergence and better generalization. This moves beyond current literature by explicitly addressing the curriculum design bottleneck in self-improving AI."
  },
  {
    "title": "Intrinsically Motivated Exploration via Prediction Disagreement in Multi-Modal Embeddings",
    "text": "Hypothesis: Agents driven by intrinsic motivation derived from prediction disagreement across multiple learned, diverse embedding spaces of the environment will exhibit more effective exploration and faster learning of complex tasks. Rationale: Traditional intrinsic motivation often focuses on prediction error or novelty. However, relying on a single prediction model can lead to getting 'stuck' in local optima. Using multiple, diverse embedding spaces (e.g., visual, auditory, proprioceptive) and rewarding disagreement between their predictions encourages exploration of regions where the agent's understanding is incomplete or inconsistent, leading to a more comprehensive world model. This builds on existing intrinsic motivation research by introducing multi-modal embeddings and disagreement as a novel exploration signal."
  },
  {
    "title": "Self-Distillation with Adaptive Noise for Enhanced Robustness and Generalization in Self-Improving Systems",
    "text": "Hypothesis: Training a self-improving agent via self-distillation, where the agent learns to mimic its own consistently improving but potentially noisy predictions, while adaptively adjusting the level of noise added to the teacher's predictions, will improve the agent's robustness to adversarial examples and generalization to unseen environments. Rationale: Self-distillation can transfer knowledge from a teacher (the agent at an earlier stage) to a student (the agent at a later stage). However, the teacher's predictions might be overconfident or biased. By adding adaptive noise, the student learns to be robust to these imperfections and generalize better. The adaptive noise dynamically adjusts based on the student's performance, preventing overfitting to the teacher's specific outputs. This extends current self-distillation techniques by introducing adaptive noise specifically for self-improving agents."
  },
  {
    "title": "Composable Skill Discovery with Task-Agnostic Pre-training and Hierarchical Reinforcement Learning",
    "text": "Hypothesis: Combining task-agnostic pre-training to learn a set of diverse, reusable skills with a hierarchical reinforcement learning framework will enable faster adaptation to new tasks and improved sample efficiency in self-improving agents. Rationale: Learning skills from scratch for each new task is inefficient. Pre-training on unsupervised data can discover useful behavioral primitives. Hierarchical reinforcement learning then allows the agent to learn how to compose these skills to solve high-level tasks. The pre-trained skills act as a foundation, accelerating learning in new environments. This hypothesis aims to bridge the gap between pre-training and hierarchical RL for creating adaptable, self-improving agents."
  },
  {
    "title": "Lifelong Learning via Meta-Reinforcement Learning with Memory Replay and Task Similarity Regularization",
    "text": "Hypothesis: A meta-reinforcement learning agent augmented with a memory replay mechanism and task similarity regularization will exhibit improved lifelong learning capabilities, mitigating catastrophic forgetting and promoting positive transfer between tasks. Rationale: Catastrophic forgetting is a major challenge in lifelong learning. Memory replay helps retain knowledge from previous tasks. Task similarity regularization encourages the agent to learn representations that are similar for related tasks, promoting positive transfer. Meta-reinforcement learning provides a framework for rapidly adapting to new tasks while leveraging past experience. This combines existing lifelong learning techniques in a novel way to enhance the performance of self-improving agents across a sequence of tasks."
  },
  {
    "title": "Explainable Self-Improvement through Interpretable Policy Gradient Methods and Policy Distillation",
    "text": "Hypothesis: Using interpretable policy gradient methods to train a self-improving agent, followed by policy distillation into a simpler, more interpretable policy, will allow for understanding the agent's learned strategies and identifying potential biases or weaknesses. Rationale: Understanding how a self-improving agent makes decisions is crucial for trust and safety. Interpretable policy gradient methods provide some insight into the agent's reasoning. Policy distillation can then create a simplified, more transparent policy that mimics the behavior of the original agent, making its decisions easier to analyze. This addresses the black-box nature of many AI systems and enables debugging of self-improvement processes."
  }
]
```
2025-05-31 19:22:54,897 INFO aicoscientist: Parsed generated hypotheses: [{'title': 'Hierarchical Meta-Learning for Curriculum Discovery in Self-Improving Agents', 'text': 'Hypothesis: A hierarchical meta-learning architecture, where a higher-level meta-learner dynamically generates curricula for a lower-level learning agent, will result in more efficient and robust self-improvement compared to fixed curricula or single-level meta-learning. Rationale: Current meta-learning often relies on pre-defined or hand-crafted curricula, limiting adaptation to novel environments or tasks. A hierarchical approach allows the system to discover optimal learning trajectories by meta-learning the curriculum itself, allowing faster convergence and better generalization. This moves beyond current literature by explicitly addressing the curriculum design bottleneck in self-improving AI.'}, {'title': 'Intrinsically Motivated Exploration via Prediction Disagreement in Multi-Modal Embeddings', 'text': "Hypothesis: Agents driven by intrinsic motivation derived from prediction disagreement across multiple learned, diverse embedding spaces of the environment will exhibit more effective exploration and faster learning of complex tasks. Rationale: Traditional intrinsic motivation often focuses on prediction error or novelty. However, relying on a single prediction model can lead to getting 'stuck' in local optima. Using multiple, diverse embedding spaces (e.g., visual, auditory, proprioceptive) and rewarding disagreement between their predictions encourages exploration of regions where the agent's understanding is incomplete or inconsistent, leading to a more comprehensive world model. This builds on existing intrinsic motivation research by introducing multi-modal embeddings and disagreement as a novel exploration signal."}, {'title': 'Self-Distillation with Adaptive Noise for Enhanced Robustness and Generalization in Self-Improving Systems', 'text': "Hypothesis: Training a self-improving agent via self-distillation, where the agent learns to mimic its own consistently improving but potentially noisy predictions, while adaptively adjusting the level of noise added to the teacher's predictions, will improve the agent's robustness to adversarial examples and generalization to unseen environments. Rationale: Self-distillation can transfer knowledge from a teacher (the agent at an earlier stage) to a student (the agent at a later stage). However, the teacher's predictions might be overconfident or biased. By adding adaptive noise, the student learns to be robust to these imperfections and generalize better. The adaptive noise dynamically adjusts based on the student's performance, preventing overfitting to the teacher's specific outputs. This extends current self-distillation techniques by introducing adaptive noise specifically for self-improving agents."}, {'title': 'Composable Skill Discovery with Task-Agnostic Pre-training and Hierarchical Reinforcement Learning', 'text': 'Hypothesis: Combining task-agnostic pre-training to learn a set of diverse, reusable skills with a hierarchical reinforcement learning framework will enable faster adaptation to new tasks and improved sample efficiency in self-improving agents. Rationale: Learning skills from scratch for each new task is inefficient. Pre-training on unsupervised data can discover useful behavioral primitives. Hierarchical reinforcement learning then allows the agent to learn how to compose these skills to solve high-level tasks. The pre-trained skills act as a foundation, accelerating learning in new environments. This hypothesis aims to bridge the gap between pre-training and hierarchical RL for creating adaptable, self-improving agents.'}, {'title': 'Lifelong Learning via Meta-Reinforcement Learning with Memory Replay and Task Similarity Regularization', 'text': 'Hypothesis: A meta-reinforcement learning agent augmented with a memory replay mechanism and task similarity regularization will exhibit improved lifelong learning capabilities, mitigating catastrophic forgetting and promoting positive transfer between tasks. Rationale: Catastrophic forgetting is a major challenge in lifelong learning. Memory replay helps retain knowledge from previous tasks. Task similarity regularization encourages the agent to learn representations that are similar for related tasks, promoting positive transfer. Meta-reinforcement learning provides a framework for rapidly adapting to new tasks while leveraging past experience. This combines existing lifelong learning techniques in a novel way to enhance the performance of self-improving agents across a sequence of tasks.'}, {'title': 'Explainable Self-Improvement through Interpretable Policy Gradient Methods and Policy Distillation', 'text': "Hypothesis: Using interpretable policy gradient methods to train a self-improving agent, followed by policy distillation into a simpler, more interpretable policy, will allow for understanding the agent's learned strategies and identifying potential biases or weaknesses. Rationale: Understanding how a self-improving agent makes decisions is crucial for trust and safety. Interpretable policy gradient methods provide some insight into the agent's reasoning. Policy distillation can then create a simplified, more transparent policy that mimics the behavior of the original agent, making its decisions easier to analyze. This addresses the black-box nature of many AI systems and enables debugging of self-improvement processes."}]
2025-05-31 19:22:54,898 INFO aicoscientist: Generated hypothesis: {'id': 'G7452', 'title': 'Hierarchical Meta-Learning for Curriculum Discovery in Self-Improving Agents', 'text': 'Hypothesis: A hierarchical meta-learning architecture, where a higher-level meta-learner dynamically generates curricula for a lower-level learning agent, will result in more efficient and robust self-improvement compared to fixed curricula or single-level meta-learning. Rationale: Current meta-learning often relies on pre-defined or hand-crafted curricula, limiting adaptation to novel environments or tasks. A hierarchical approach allows the system to discover optimal learning trajectories by meta-learning the curriculum itself, allowing faster convergence and better generalization. This moves beyond current literature by explicitly addressing the curriculum design bottleneck in self-improving AI.', 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:54,898 INFO aicoscientist: Generated hypothesis: {'id': 'G5307', 'title': 'Intrinsically Motivated Exploration via Prediction Disagreement in Multi-Modal Embeddings', 'text': "Hypothesis: Agents driven by intrinsic motivation derived from prediction disagreement across multiple learned, diverse embedding spaces of the environment will exhibit more effective exploration and faster learning of complex tasks. Rationale: Traditional intrinsic motivation often focuses on prediction error or novelty. However, relying on a single prediction model can lead to getting 'stuck' in local optima. Using multiple, diverse embedding spaces (e.g., visual, auditory, proprioceptive) and rewarding disagreement between their predictions encourages exploration of regions where the agent's understanding is incomplete or inconsistent, leading to a more comprehensive world model. This builds on existing intrinsic motivation research by introducing multi-modal embeddings and disagreement as a novel exploration signal.", 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:54,899 INFO aicoscientist: Generated hypothesis: {'id': 'G2823', 'title': 'Self-Distillation with Adaptive Noise for Enhanced Robustness and Generalization in Self-Improving Systems', 'text': "Hypothesis: Training a self-improving agent via self-distillation, where the agent learns to mimic its own consistently improving but potentially noisy predictions, while adaptively adjusting the level of noise added to the teacher's predictions, will improve the agent's robustness to adversarial examples and generalization to unseen environments. Rationale: Self-distillation can transfer knowledge from a teacher (the agent at an earlier stage) to a student (the agent at a later stage). However, the teacher's predictions might be overconfident or biased. By adding adaptive noise, the student learns to be robust to these imperfections and generalize better. The adaptive noise dynamically adjusts based on the student's performance, preventing overfitting to the teacher's specific outputs. This extends current self-distillation techniques by introducing adaptive noise specifically for self-improving agents.", 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:54,899 INFO aicoscientist: Generated hypothesis: {'id': 'G3758', 'title': 'Composable Skill Discovery with Task-Agnostic Pre-training and Hierarchical Reinforcement Learning', 'text': 'Hypothesis: Combining task-agnostic pre-training to learn a set of diverse, reusable skills with a hierarchical reinforcement learning framework will enable faster adaptation to new tasks and improved sample efficiency in self-improving agents. Rationale: Learning skills from scratch for each new task is inefficient. Pre-training on unsupervised data can discover useful behavioral primitives. Hierarchical reinforcement learning then allows the agent to learn how to compose these skills to solve high-level tasks. The pre-trained skills act as a foundation, accelerating learning in new environments. This hypothesis aims to bridge the gap between pre-training and hierarchical RL for creating adaptable, self-improving agents.', 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:54,899 INFO aicoscientist: Generated hypothesis: {'id': 'G2583', 'title': 'Lifelong Learning via Meta-Reinforcement Learning with Memory Replay and Task Similarity Regularization', 'text': 'Hypothesis: A meta-reinforcement learning agent augmented with a memory replay mechanism and task similarity regularization will exhibit improved lifelong learning capabilities, mitigating catastrophic forgetting and promoting positive transfer between tasks. Rationale: Catastrophic forgetting is a major challenge in lifelong learning. Memory replay helps retain knowledge from previous tasks. Task similarity regularization encourages the agent to learn representations that are similar for related tasks, promoting positive transfer. Meta-reinforcement learning provides a framework for rapidly adapting to new tasks while leveraging past experience. This combines existing lifelong learning techniques in a novel way to enhance the performance of self-improving agents across a sequence of tasks.', 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:54,899 INFO aicoscientist: Generated hypothesis: {'id': 'G1604', 'title': 'Explainable Self-Improvement through Interpretable Policy Gradient Methods and Policy Distillation', 'text': "Hypothesis: Using interpretable policy gradient methods to train a self-improving agent, followed by policy distillation into a simpler, more interpretable policy, will allow for understanding the agent's learned strategies and identifying potential biases or weaknesses. Rationale: Understanding how a self-improving agent makes decisions is crucial for trust and safety. Interpretable policy gradient methods provide some insight into the agent's reasoning. Policy distillation can then create a simplified, more transparent policy that mimics the behavior of the original agent, making its decisions easier to analyze. This addresses the black-box nature of many AI systems and enables debugging of self-improvement processes.", 'novelty_review': None, 'feasibility_review': None, 'elo_score': 1200.0, 'review_comments': [], 'references': [], 'is_active': True, 'parent_ids': []}
2025-05-31 19:22:54,911 INFO aicoscientist: Step 2: Reflection
2025-05-31 19:22:54,911 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:22:58,049 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3130.70ms, Tokens: 246/455, Retries: 0
2025-05-31 19:22:58,049 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis is moderately novel. While meta-learning for curriculum learning is an active area, the emphasis on *true* autonomy in curriculum creation for *novel, initially unknown* environments differentiates it from much of the existing work. The feasibility is also medium. The success hinges on the meta-learning agent's ability to effectively model and predict the learning progress in diverse and unpredictable environments, which is a significant challenge. Careful consideration of the representation of environments and tasks, as well as the exploration strategy within the meta-learning loop, will be crucial.",
  "references": [
    {
      "type": "arXiv",
      "id": "1810.11903",
      "title": "Meta-Curriculum Learning"
    },
    {
      "type": "arXiv",
      "id": "1707.04170",
      "title": "Curriculum Learning by Prediction-Error Minimization"
    },
    {
      "type": "DOI",
      "id": "10.1109/ICDL-EpiRob.2017.8204463",
      "title": "Autonomous Curriculum Generation for Reinforcement Learning"
    },
    {
      "type": "Paper Title",
      "title": "POET: End-to-End Parametric Optimization through Evolution" ,
      "venue": "Genetic and Evolutionary Computation Conference (GECCO) 2019"
    },
    {
      "type": "Paper Title",
      "title": "Teacher-Student Curriculum Learning",
      "venue": "Neural Information Processing Systems (NeurIPS) 2016"
    },
    {
      "type": "arXiv",
      "id": "1802.09562",
      "title": "Learning to Learn by Gradient Descent by Gradient Descent"
    }
  ]
}
```
2025-05-31 19:22:58,050 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': "The hypothesis is moderately novel. While meta-learning for curriculum learning is an active area, the emphasis on *true* autonomy in curriculum creation for *novel, initially unknown* environments differentiates it from much of the existing work. The feasibility is also medium. The success hinges on the meta-learning agent's ability to effectively model and predict the learning progress in diverse and unpredictable environments, which is a significant challenge. Careful consideration of the representation of environments and tasks, as well as the exploration strategy within the meta-learning loop, will be crucial.", 'references': [{'type': 'arXiv', 'id': '1810.11903', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04170', 'title': 'Curriculum Learning by Prediction-Error Minimization'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204463', 'title': 'Autonomous Curriculum Generation for Reinforcement Learning'}, {'type': 'Paper Title', 'title': 'POET: End-to-End Parametric Optimization through Evolution', 'venue': 'Genetic and Evolutionary Computation Conference (GECCO) 2019'}, {'type': 'Paper Title', 'title': 'Teacher-Student Curriculum Learning', 'venue': 'Neural Information Processing Systems (NeurIPS) 2016'}, {'type': 'arXiv', 'id': '1802.09562', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}]}
2025-05-31 19:22:58,050 INFO aicoscientist: Reviewed hypothesis: G1351, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:22:58,050 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:01,285 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3229.89ms, Tokens: 252/496, Retries: 0
2025-05-31 19:23:01,285 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis has merit. While hierarchical reinforcement learning and intrinsic motivation are established fields, the specific combination of hierarchical prediction error reduction across multiple levels of abstraction for autonomous skill discovery is not entirely novel, but has potential for significant advancement. The success hinges on the effective design of the hierarchical architecture, the intrinsic reward function, and the scalability of the learning algorithm. The claim of 'more robust and adaptable skill discovery' needs strong empirical validation. The connection between prediction error and skill abstraction is a key element that needs to be well-defined and implemented.",
  "references": [
    {
      "type": "arXiv",
      "id": "1604.00077",
      "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation"
    },
    {
      "type": "DOI",
      "id": "10.1109/ICDL-EpiRob.2017.8204503",
      "title": "Intrinsic Motivation for Hierarchical Reinforcement Learning"
    },
    {
      "type": "Paper",
      "title": "HAC: Hierarchical Actor-Critic",
      "venue": "NeurIPS 2018"
    },
    {
      "type": "Paper",
      "title": "Variational Option Discovery using Hierarchical Intrinsic Rewards",
      "venue": "ICML 2019"
    },
    {
      "type": "arXiv",
      "id": "1511.09292",
      "title": "Learning to Predict the Internal State of Agents"
    },
    {
      "type": "Paper",
      "title": "Unsupervised Meta-Learning for Few-Shot Image Classification",
      "venue": "ICLR 2017"
    },
    {
      "type": "Paper",
      "title": "Intrinsic Motivation via Prediction Gain",
      "venue": "International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011"
    }
  ]
}
```
2025-05-31 19:23:01,285 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': "The hypothesis has merit. While hierarchical reinforcement learning and intrinsic motivation are established fields, the specific combination of hierarchical prediction error reduction across multiple levels of abstraction for autonomous skill discovery is not entirely novel, but has potential for significant advancement. The success hinges on the effective design of the hierarchical architecture, the intrinsic reward function, and the scalability of the learning algorithm. The claim of 'more robust and adaptable skill discovery' needs strong empirical validation. The connection between prediction error and skill abstraction is a key element that needs to be well-defined and implemented.", 'references': [{'type': 'arXiv', 'id': '1604.00077', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204503', 'title': 'Intrinsic Motivation for Hierarchical Reinforcement Learning'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2019'}, {'type': 'arXiv', 'id': '1511.09292', 'title': 'Learning to Predict the Internal State of Agents'}, {'type': 'Paper', 'title': 'Unsupervised Meta-Learning for Few-Shot Image Classification', 'venue': 'ICLR 2017'}, {'type': 'Paper', 'title': 'Intrinsic Motivation via Prediction Gain', 'venue': 'International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011'}]}
2025-05-31 19:23:01,285 INFO aicoscientist: Reviewed hypothesis: G7295, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:23:01,285 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:04,828 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3535.98ms, Tokens: 270/493, Retries: 0
2025-05-31 19:23:04,828 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis presents a reasonable approach to continual learning, combining model merging and knowledge distillation. While the individual components (model merging and distillation) are not novel, the specific combination of dynamically merging *pre-trained* models from different tasks, followed by distillation to mitigate forgetting and interference, has potential novelty. The success hinges on the effectiveness of the merging strategy and the distillation process in preserving task-specific knowledge. The dynamic aspect of the merging is also key. The feasibility depends on the computational resources required for training multiple pre-trained models and the complexity of the merging and distillation procedures. A crucial aspect to consider is the similarity/dissimilarity of the tasks. Highly dissimilar tasks might pose a challenge for effective merging and distillation.",
  "references": [
    {
      "title": "Model Soups: Averaging Weights Leads to Wider Optima and Better Generalization",
      "venue": "NeurIPS 2022"
    },
    {
      "title": "Knowledge Distillation",
      "venue": "NIPS 2015"
    },
    {
      "arXiv_ID": "1812.00424" ,
      "title": "Elastic Weight Consolidation"
    },
    {
      "arXiv_ID": "1606.07792",
      "title": "Learning without Forgetting"
    },
    {
      "title": "Continual Learning via Modular Networks with Overlapping Representation",
      "venue": "ICLR 2020"
    },
    {
      "arXiv_ID": "2012.00822",
      "title": "Task-Agnostic Continual Learning via Meta-Learning"
    },
    {
      "arXiv_ID": "2106.08943",
      "title": "MERLIN: Parameter-Efficient Transfer Learning for NLP"
    },
    {
      "title": "Distilling Knowledge from Ensembles of Neural Networks",
      "venue": "CVPR 2014"
    }
  ]
}
```
2025-05-31 19:23:04,828 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The hypothesis presents a reasonable approach to continual learning, combining model merging and knowledge distillation. While the individual components (model merging and distillation) are not novel, the specific combination of dynamically merging *pre-trained* models from different tasks, followed by distillation to mitigate forgetting and interference, has potential novelty. The success hinges on the effectiveness of the merging strategy and the distillation process in preserving task-specific knowledge. The dynamic aspect of the merging is also key. The feasibility depends on the computational resources required for training multiple pre-trained models and the complexity of the merging and distillation procedures. A crucial aspect to consider is the similarity/dissimilarity of the tasks. Highly dissimilar tasks might pose a challenge for effective merging and distillation.', 'references': [{'title': 'Model Soups: Averaging Weights Leads to Wider Optima and Better Generalization', 'venue': 'NeurIPS 2022'}, {'title': 'Knowledge Distillation', 'venue': 'NIPS 2015'}, {'arXiv_ID': '1812.00424', 'title': 'Elastic Weight Consolidation'}, {'arXiv_ID': '1606.07792', 'title': 'Learning without Forgetting'}, {'title': 'Continual Learning via Modular Networks with Overlapping Representation', 'venue': 'ICLR 2020'}, {'arXiv_ID': '2012.00822', 'title': 'Task-Agnostic Continual Learning via Meta-Learning'}, {'arXiv_ID': '2106.08943', 'title': 'MERLIN: Parameter-Efficient Transfer Learning for NLP'}, {'title': 'Distilling Knowledge from Ensembles of Neural Networks', 'venue': 'CVPR 2014'}]}
2025-05-31 19:23:04,829 INFO aicoscientist: Reviewed hypothesis: G4199, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:23:04,829 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:07,790 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 2955.41ms, Tokens: 272/425, Retries: 0
2025-05-31 19:23:07,791 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis is moderately novel. While BNNs have been used in reinforcement learning for uncertainty estimation, the explicit combination of BNNs, active learning based on uncertainty estimates, and a focus on *active* exploration to improve learning efficiency represents a valuable and potentially impactful direction. The novelty hinges on the specific active learning algorithm and how effectively it leverages the BNN's uncertainty. The feasibility is also medium. Training BNNs can be computationally expensive, and designing an effective active learning strategy that balances exploration and exploitation is challenging. Success depends on careful selection of the BNN architecture, the active learning query strategy, and the environment.",
  "references": [
    {
      "title": "Bayesian Deep Reinforcement Learning",
      "venue": "ICML 2016"
    },
    {
      "title": "Deep Exploration via Bootstrapped DQN",
      "venue": "NeurIPS 2016"
    },
    {
      "title": "Variational Information Bottleneck for Reinforcement Learning",
      "venue": "ICLR 2019"
    },
    {
      "title": "Active Model Learning",
      "venue": "arXiv:1802.08941"
    },
    {
      "title": "Efficient Exploration through Bayesian Deep Q-Networks",
      "venue": "arXiv:1606.04670"
    },
    {
      "title": "Active Learning for Deep Reinforcement Learning",
      "venue": "arXiv:1804.08474"
    },
    {
      "title": "Model-Based Active Exploration",
      "venue": "arXiv:1703.07604"
    }
  ]
}
```
2025-05-31 19:23:07,791 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': "The hypothesis is moderately novel. While BNNs have been used in reinforcement learning for uncertainty estimation, the explicit combination of BNNs, active learning based on uncertainty estimates, and a focus on *active* exploration to improve learning efficiency represents a valuable and potentially impactful direction. The novelty hinges on the specific active learning algorithm and how effectively it leverages the BNN's uncertainty. The feasibility is also medium. Training BNNs can be computationally expensive, and designing an effective active learning strategy that balances exploration and exploitation is challenging. Success depends on careful selection of the BNN architecture, the active learning query strategy, and the environment.", 'references': [{'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Deep Exploration via Bootstrapped DQN', 'venue': 'NeurIPS 2016'}, {'title': 'Variational Information Bottleneck for Reinforcement Learning', 'venue': 'ICLR 2019'}, {'title': 'Active Model Learning', 'venue': 'arXiv:1802.08941'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'arXiv:1606.04670'}, {'title': 'Active Learning for Deep Reinforcement Learning', 'venue': 'arXiv:1804.08474'}, {'title': 'Model-Based Active Exploration', 'venue': 'arXiv:1703.07604'}]}
2025-05-31 19:23:07,791 INFO aicoscientist: Reviewed hypothesis: G3657, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:23:07,791 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:11,249 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3452.70ms, Tokens: 247/464, Retries: 0
2025-05-31 19:23:11,250 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis builds upon existing research in emergent communication and multi-agent learning. The novelty lies in the explicit focus on collaborative self-improvement through the shared knowledge (strategies, experiences, models) facilitated by the emergent communication protocol. While emergent communication is well-studied, the specific application to collaborative self-improvement and the types of knowledge shared (beyond simple symbols) adds a layer of complexity and potential novelty. Feasibility depends heavily on the complexity of the agents, the environment, and the learning algorithms used. Scaling this to complex environments with many agents could be challenging. Careful design of the reward structure and communication mechanisms will be crucial.",
  "references": [
    {
      "title": "Learning to Communicate with Deep Multi-Agent Reinforcement Learning",
      "venue": "NeurIPS 2016"
    },
    {
      "title": "Emergent Communication in Multi-Agent Systems: A Survey",
      "venue": "Journal of Artificial Intelligence Research 72 (2021) 1-93"
    },
    {
      "title": "Learning to Share in Multi-Agent Reinforcement Learning",
      "venue": "ICML 2018"
    },
    {
      "arXiv_ID": "1605.06673" ,
      "comment": "Foerster et al. - Learning to Communicate with Deep Multi-Agent Reinforcement Learning"
    },
    {
      "arXiv_ID": "1802.08780",
      "comment": "Lazaridou et al. - Emergence of Linguistic Communication from Referential Games"
    },
    {
      "title": "Social Influence as Intrinsic Motivation for Multi-Agent Exploration",
      "venue": "ICLR 2019"
    },
    {
      "title": "Model-Based Communication for Multi-Agent Cooperation",
      "venue": "AAMAS 2020"
    }
  ]
}
```
2025-05-31 19:23:11,250 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The hypothesis builds upon existing research in emergent communication and multi-agent learning. The novelty lies in the explicit focus on collaborative self-improvement through the shared knowledge (strategies, experiences, models) facilitated by the emergent communication protocol. While emergent communication is well-studied, the specific application to collaborative self-improvement and the types of knowledge shared (beyond simple symbols) adds a layer of complexity and potential novelty. Feasibility depends heavily on the complexity of the agents, the environment, and the learning algorithms used. Scaling this to complex environments with many agents could be challenging. Careful design of the reward structure and communication mechanisms will be crucial.', 'references': [{'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems: A Survey', 'venue': 'Journal of Artificial Intelligence Research 72 (2021) 1-93'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'ICML 2018'}, {'arXiv_ID': '1605.06673', 'comment': 'Foerster et al. - Learning to Communicate with Deep Multi-Agent Reinforcement Learning'}, {'arXiv_ID': '1802.08780', 'comment': 'Lazaridou et al. - Emergence of Linguistic Communication from Referential Games'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Exploration', 'venue': 'ICLR 2019'}, {'title': 'Model-Based Communication for Multi-Agent Cooperation', 'venue': 'AAMAS 2020'}]}
2025-05-31 19:23:11,250 INFO aicoscientist: Reviewed hypothesis: G8633, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:23:11,250 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:14,521 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3262.97ms, Tokens: 255/471, Retries: 0
2025-05-31 19:23:14,521 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The idea of integrating symbolic reasoning with reinforcement learning to improve generalization is not entirely new, but the specific implementation details and the extent to which it improves robustness in unseen environments are important factors. The success hinges on effectively learning the symbolic abstractions and ensuring the reasoning process is computationally tractable. The challenge lies in bridging the gap between the continuous state space of RL and the discrete symbolic world. The specific architecture and learning algorithm used to achieve this integration will significantly impact the novelty and feasibility.",
  "references": [
    {
      "title": "Neuro-Symbolic Reinforcement Learning",
      "venue": "Advances in Neural Information Processing Systems 31 (NeurIPS 2018)"
    },
    {
      "title": "Learning Symbolic Representations for Generalization in Reinforcement Learning",
      "venue": "International Conference on Learning Representations (ICLR) 2018"
    },
    {
      "title": "Grounded Language Learning in a Simulated 3D World",
      "venue": "Association for Computational Linguistics (ACL) 2017"
    },
    {
      "title": "Differentiable Reasoning over Symbolic Knowledge",
      "venue": "International Conference on Learning Representations (ICLR) 2017"
    },
    {
      "title": "Relational Reinforcement Learning",
      "venue": "Machine Learning, 57(1-3), 5-43, 2004"
    },
    {
      "arXiv_ID": "2106.06275",
      "title": "Symbolic Planning with Neural Reward Functions"
    },
    {
      "arXiv_ID": "2203.02071",
      "title": "Learning to Abstract and Reason with Symbolic Planning"
    },
    {
      "title": "STRIPS Learning through Reinforcement",
      "venue": "AAAI Conference on Artificial Intelligence (AAAI), 2023"
    }
  ]
}
```
2025-05-31 19:23:14,522 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The idea of integrating symbolic reasoning with reinforcement learning to improve generalization is not entirely new, but the specific implementation details and the extent to which it improves robustness in unseen environments are important factors. The success hinges on effectively learning the symbolic abstractions and ensuring the reasoning process is computationally tractable. The challenge lies in bridging the gap between the continuous state space of RL and the discrete symbolic world. The specific architecture and learning algorithm used to achieve this integration will significantly impact the novelty and feasibility.', 'references': [{'title': 'Neuro-Symbolic Reinforcement Learning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Learning Symbolic Representations for Generalization in Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR) 2018'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'Association for Computational Linguistics (ACL) 2017'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'International Conference on Learning Representations (ICLR) 2017'}, {'title': 'Relational Reinforcement Learning', 'venue': 'Machine Learning, 57(1-3), 5-43, 2004'}, {'arXiv_ID': '2106.06275', 'title': 'Symbolic Planning with Neural Reward Functions'}, {'arXiv_ID': '2203.02071', 'title': 'Learning to Abstract and Reason with Symbolic Planning'}, {'title': 'STRIPS Learning through Reinforcement', 'venue': 'AAAI Conference on Artificial Intelligence (AAAI), 2023'}]}
2025-05-31 19:23:14,522 INFO aicoscientist: Reviewed hypothesis: G7402, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:23:14,522 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:17,850 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3320.30ms, Tokens: 393/500, Retries: 0
2025-05-31 19:23:17,850 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "HIGH",
  "feasibility_review": "MEDIUM",
  "comment": "This hypothesis combines two relatively novel ideas: using BNN uncertainty for active exploration and meta-learning for autonomous curriculum design. While both components have been explored individually, their combination, especially in the context of *completely* novel environments, represents a significant step forward. The feasibility is medium because implementing and training BNNs, active learning, and meta-learning algorithms simultaneously is computationally expensive and requires careful tuning. Success hinges on effectively integrating the uncertainty estimates from the BNN into the meta-learning framework for curriculum design.",
  "references": [
    {
      "title": "Bayesian Neural Networks",
      "venue": "Review Article, e.g., Journal of Machine Learning Research"
    },
    {
      "title": "Active Learning for Regression",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "venue": "ICML 2017"
    },
    {
      "title": "Meta-Curriculum Learning",
      "venue": "NeurIPS 2019"
    },
    {
      "title": "Exploration by Random Network Distillation",
      "venue": "ICLR 2019"
    },
    {
      "arXiv_ID": "1802.09471",
      "title": "Variational Information Bottleneck for Reinforcement Learning"
    },
    {
      "title": "Uncertainty-Aware Deep Reinforcement Learning",
      "venue": "AAAI 2018"
    },
    {
      "title": "Efficient Exploration through Bayesian Deep Q-Networks",
      "venue": "AISTATS 2016"
    },
    {
      "title": "Curriculum Learning",
      "venue": "A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "title": "Self-Paced Learning for Latent Variable Models",
      "venue": "NIPS 2010"
    }
  ]
}
```
2025-05-31 19:23:17,851 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'HIGH', 'feasibility_review': 'MEDIUM', 'comment': 'This hypothesis combines two relatively novel ideas: using BNN uncertainty for active exploration and meta-learning for autonomous curriculum design. While both components have been explored individually, their combination, especially in the context of *completely* novel environments, represents a significant step forward. The feasibility is medium because implementing and training BNNs, active learning, and meta-learning algorithms simultaneously is computationally expensive and requires careful tuning. Success hinges on effectively integrating the uncertainty estimates from the BNN into the meta-learning framework for curriculum design.', 'references': [{'title': 'Bayesian Neural Networks', 'venue': 'Review Article, e.g., Journal of Machine Learning Research'}, {'title': 'Active Learning for Regression', 'venue': 'Journal of Machine Learning Research'}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'ICML 2017'}, {'title': 'Meta-Curriculum Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Exploration by Random Network Distillation', 'venue': 'ICLR 2019'}, {'arXiv_ID': '1802.09471', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'title': 'Uncertainty-Aware Deep Reinforcement Learning', 'venue': 'AAAI 2018'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'AISTATS 2016'}, {'title': 'Curriculum Learning', 'venue': 'A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems'}, {'title': 'Self-Paced Learning for Latent Variable Models', 'venue': 'NIPS 2010'}]}
2025-05-31 19:23:17,851 INFO aicoscientist: Reviewed hypothesis: E5336, Novelty: HIGH, Feasibility: MEDIUM
2025-05-31 19:23:17,851 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:21,085 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3226.57ms, Tokens: 254/412, Retries: 0
2025-05-31 19:23:21,085 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis is interesting and builds upon existing meta-learning research. The hierarchical meta-learning approach for curriculum generation has potential, but the novelty is somewhat limited as hierarchical meta-learning and automated curriculum learning are active research areas. The feasibility depends heavily on the complexity of the environment and the learning agents. Scaling such a system to complex real-world scenarios would be challenging. The success hinges on the ability to efficiently explore the curriculum space and avoid getting stuck in suboptimal curricula. Defining appropriate reward signals for the higher-level meta-learner is also a key consideration.",
  "references": [
    {
      "title": "Meta-Curriculum Learning",
      "venue": "ICML 2020"
    },
    {
      "title": "Hierarchical Meta-Reinforcement Learning",
      "venue": "NeurIPS 2018"
    },
    {
      "title": "Learning to Teach with Dynamic Loss Functions",
      "venue": "ICLR 2018"
    },
    {
      "arXiv_ID": "1705.10410" ,
      "title": "Learning to Learn by Gradient Descent by Gradient Descent"
    },
    {
      "arXiv_ID": "1606.04474",
      "title": "Optimization as a Model for Few-Shot Learning"
    },
    {
      "title": "Automatic Curriculum Learning for Deep RL: A Survey",
      "venue": "Journal of Artificial Intelligence Research 2022"
    },
    {
        "title": "Self-Supervised Curriculum Learning",
        "venue": "ICLR 2020"
    }
  ]
}
```
2025-05-31 19:23:21,085 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The hypothesis is interesting and builds upon existing meta-learning research. The hierarchical meta-learning approach for curriculum generation has potential, but the novelty is somewhat limited as hierarchical meta-learning and automated curriculum learning are active research areas. The feasibility depends heavily on the complexity of the environment and the learning agents. Scaling such a system to complex real-world scenarios would be challenging. The success hinges on the ability to efficiently explore the curriculum space and avoid getting stuck in suboptimal curricula. Defining appropriate reward signals for the higher-level meta-learner is also a key consideration.', 'references': [{'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}, {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}, {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}, {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}, {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}, {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}]}
2025-05-31 19:23:21,086 INFO aicoscientist: Reviewed hypothesis: G7452, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:23:21,086 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:24,290 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3201.33ms, Tokens: 267/448, Retries: 0
2025-05-31 19:23:24,290 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis is interesting and builds upon existing intrinsic motivation research. The use of multiple, diverse embedding spaces and prediction disagreement is a reasonable approach to improve exploration. However, the novelty is somewhat limited as disagreement-based intrinsic motivation and multi-modal learning are individually established concepts. The challenge lies in effectively combining them and demonstrating a significant improvement over existing methods. Feasibility depends on the complexity of the environment and the computational resources available to train multiple embedding spaces and their prediction models. Careful design of the embedding spaces and disagreement metric is crucial for success. The hypothesis could be strengthened by specifying the types of tasks where this approach is expected to be most beneficial.",
  "references": [
    {
      "title": "Curiosity-driven Exploration by Self-supervised Prediction",
      "venue": "ICML 2017"
    },
    {
      "title": "Exploration via Disagreement",
      "venue": "NeurIPS 2018"
    },
    {
      "title": "Variational Intrinsic Control",
      "venue": "ICLR 2016"
    },
    {
      "title": "Multi-Modal Deep Reinforcement Learning",
      "venue": "IJCAI 2016"
    },
    {
      "title": "Learning to Explore via Meta-Curiosity",
      "venue": "ICLR 2018"
    },
    {
      "title": "A Survey of Exploration in Reinforcement Learning",
      "venue": "Journal of Artificial Intelligence Research 2020"
    },
    {
      "title": "Diversity is All You Need: Learning Skills without Rewards",
      "venue": "ICLR 2019"
    },
    {
      "title": "Episodic Curiosity through Reachability",
      "venue": "ICLR 2020"
    }
  ]
}
```
2025-05-31 19:23:24,291 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The hypothesis is interesting and builds upon existing intrinsic motivation research. The use of multiple, diverse embedding spaces and prediction disagreement is a reasonable approach to improve exploration. However, the novelty is somewhat limited as disagreement-based intrinsic motivation and multi-modal learning are individually established concepts. The challenge lies in effectively combining them and demonstrating a significant improvement over existing methods. Feasibility depends on the complexity of the environment and the computational resources available to train multiple embedding spaces and their prediction models. Careful design of the embedding spaces and disagreement metric is crucial for success. The hypothesis could be strengthened by specifying the types of tasks where this approach is expected to be most beneficial.', 'references': [{'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}, {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}, {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}, {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}, {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}, {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}, {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}, {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}]}
2025-05-31 19:23:24,291 INFO aicoscientist: Reviewed hypothesis: G5307, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:23:24,291 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:27,422 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3124.90ms, Tokens: 301/460, Retries: 0
2025-05-31 19:23:27,422 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis combines self-distillation with adaptive noise injection, which is a plausible approach to improve robustness and generalization. While self-distillation and adversarial training/noise injection are established techniques, the specific combination of *adaptive* noise within a *self-improving agent* context adds a layer of novelty. The feasibility depends on the complexity of the environment and the computational resources required for training the self-improving agent. The adaptive noise mechanism needs careful design to avoid destabilizing the training process. The success hinges on effectively balancing the benefits of noise regularization with the potential drawbacks of introducing too much uncertainty.",
  "references": [
    {
      "type": "arXiv",
      "id": "1511.00421",
      "title": "Distilling the Knowledge in a Neural Network"
    },
    {
      "type": "DOI",
      "id": "10.48550/arXiv.1706.08974",
      "title": "Adversarial Training Methods for Semi-Supervised Text Classification",
      "venue": "ICLR 2017"
    },
    {
      "type": "arXiv",
      "id": "2106.05237",
      "title": "Self-Distillation as Instance-Specific Label Smoothing"
    },
    {
      "type": "Paper",
      "title": "Regularizing Neural Networks with Parameter-Dependent Noise",
      "venue": "NIPS 2016"
    },
    {
      "type": "Paper",
      "title": "Improving Generalization via Scalable Neighborhood Mixing",
      "venue": "ICML 2020"
    },
    {
      "type": "Paper",
      "title": "Curriculum by Smoothing",
      "venue": "ICML 2020"
    }
  ]
}
```
2025-05-31 19:23:27,423 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The hypothesis combines self-distillation with adaptive noise injection, which is a plausible approach to improve robustness and generalization. While self-distillation and adversarial training/noise injection are established techniques, the specific combination of *adaptive* noise within a *self-improving agent* context adds a layer of novelty. The feasibility depends on the complexity of the environment and the computational resources required for training the self-improving agent. The adaptive noise mechanism needs careful design to avoid destabilizing the training process. The success hinges on effectively balancing the benefits of noise regularization with the potential drawbacks of introducing too much uncertainty.', 'references': [{'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}, {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}, {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}, {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}, {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}, {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}]}
2025-05-31 19:23:27,423 INFO aicoscientist: Reviewed hypothesis: G2823, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:23:27,423 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:31,425 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3995.50ms, Tokens: 261/542, Retries: 0
2025-05-31 19:23:31,425 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis is grounded in existing research trends but the specific combination of task-agnostic pre-training for diverse skill discovery with hierarchical RL for faster adaptation presents a potentially novel and valuable approach. The novelty lies in the specific *type* of pre-training and how it's integrated within the hierarchical RL framework. Feasibility depends on the complexity of the pre-training data and the scalability of the hierarchical RL algorithm. Success hinges on effectively discovering diverse and reusable skills during pre-training and designing a hierarchical RL architecture that can leverage these skills efficiently. Defining 'faster adaptation' and 'improved sample efficiency' with specific metrics is crucial for evaluating the hypothesis.",
  "references": [
    {
      "type": "arXiv",
      "id": "1602.06070",
      "title": "Hierarchical Reinforcement Learning"
    },
    {
      "type": "arXiv",
      "id": "1802.09403",
      "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks"
    },
    {
      "type": "Paper Title",
      "title": "Hindsight Experience Replay",
      "venue": "NeurIPS 2017"
    },
    {
      "type": "Paper Title",
      "title": "Data-Efficient Hierarchical Reinforcement Learning",
      "venue": "ICML 2019"
    },
    {
      "type": "Paper Title",
      "title": "Learning latent options",
      "venue": "ICML 2016"
    },
    {
      "type": "arXiv",
      "id": "2007.14950",
      "title": "Contrastive unsupervised representation learning with noise for reinforcement learning"
    },
    {
      "type": "arXiv",
      "id": "2106.04487",
      "title": "Pre-training Representations for Data-Efficient Reinforcement Learning"
    },
    {
      "type": "DOI",
      "id": "10.1109/LRA.2021.3051378",
      "title": "Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks"
    }
  ]
}
```
2025-05-31 19:23:31,425 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': "The hypothesis is grounded in existing research trends but the specific combination of task-agnostic pre-training for diverse skill discovery with hierarchical RL for faster adaptation presents a potentially novel and valuable approach. The novelty lies in the specific *type* of pre-training and how it's integrated within the hierarchical RL framework. Feasibility depends on the complexity of the pre-training data and the scalability of the hierarchical RL algorithm. Success hinges on effectively discovering diverse and reusable skills during pre-training and designing a hierarchical RL architecture that can leverage these skills efficiently. Defining 'faster adaptation' and 'improved sample efficiency' with specific metrics is crucial for evaluating the hypothesis.", 'references': [{'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}, {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}, {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}, {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}, {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}, {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}, {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}, {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}]}
2025-05-31 19:23:31,425 INFO aicoscientist: Reviewed hypothesis: G3758, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:23:31,425 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:34,597 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3165.83ms, Tokens: 256/431, Retries: 0
2025-05-31 19:23:34,597 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis combines several well-established techniques in lifelong learning (meta-RL, memory replay, task similarity regularization). The novelty lies in the specific combination and the potential for improved performance compared to using each technique in isolation. However, the individual components are not novel. Feasibility is moderate as each component has been implemented before, but the integration and hyperparameter tuning across multiple tasks can be computationally expensive and challenging.",
  "references": [
    {
      "type": "arXiv",
      "id": "1703.03400",
      "comment": "Meta-Learning with Memory-Augmented Neural Networks"
    },
    {
      "type": "arXiv",
      "id": "1606.01545",
      "comment": "Overcoming Catastrophic Forgetting in Neural Networks"
    },
    {
      "type": "DOI",
      "id": "10.1109/ICRA.2018.8460811",
      "comment": "Lifelong Robotic Reinforcement Learning"
    },
    {
      "type": "Paper Title & Venue",
      "title": "Learning without Forgetting",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017"
    },
    {
      "type": "Paper Title & Venue",
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "venue": "International Conference on Machine Learning (ICML), 2017"
    },
    {
      "type": "arXiv",
      "id": "1802.07592",
      "comment": "Regularizing Representations to Generalize in Reinforcement Learning"
    }
  ]
}
```
2025-05-31 19:23:34,598 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The hypothesis combines several well-established techniques in lifelong learning (meta-RL, memory replay, task similarity regularization). The novelty lies in the specific combination and the potential for improved performance compared to using each technique in isolation. However, the individual components are not novel. Feasibility is moderate as each component has been implemented before, but the integration and hyperparameter tuning across multiple tasks can be computationally expensive and challenging.', 'references': [{'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}, {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}, {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}, {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}, {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}]}
2025-05-31 19:23:34,598 INFO aicoscientist: Reviewed hypothesis: G2583, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:23:34,598 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:37,772 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3166.84ms, Tokens: 260/418, Retries: 0
2025-05-31 19:23:37,773 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "The hypothesis combines existing techniques (interpretable policy gradients, policy distillation) in a relatively straightforward manner. The novelty lies primarily in the specific application to understanding self-improving agents and identifying biases. The feasibility is reasonable, given the availability of existing implementations of the constituent methods. However, the success of policy distillation in preserving interpretability while maintaining performance is not guaranteed and depends heavily on the complexity of the original policy and the distillation process.",
  "references": [
    {
      "title": "Trust Region Policy Optimization",
      "venue": "International Conference on Machine Learning (ICML), 2015"
    },
    {
      "title": "Proximal Policy Optimization Algorithms",
      "venue": "arXiv preprint arXiv:1707.06347"
    },
    {
      "title": "Policy Distillation",
      "venue": "International Conference on Knowledge Discovery and Data Mining (KDD), 2006"
    },
    {
      "arXiv_ID": "1606.05256",
      "title": "Explaining and Harnessing Adversarial Examples"
    },
    {
      "arXiv_ID": "1602.04938",
      "title": "Learning to Explain: An Information-Theoretic Perspective on Model Interpretation"
    },
    {
      "arXiv_ID": "1810.00633",
      "title": "Interpretable Policies for Reinforcement Learning by Extracting Decision Trees"
    },
        {
      "title": "Explainable Reinforcement Learning: A Survey",
      "venue": "Journal of Artificial Intelligence Research (JAIR), 2022"
    }

  ]
}
```
2025-05-31 19:23:37,773 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'The hypothesis combines existing techniques (interpretable policy gradients, policy distillation) in a relatively straightforward manner. The novelty lies primarily in the specific application to understanding self-improving agents and identifying biases. The feasibility is reasonable, given the availability of existing implementations of the constituent methods. However, the success of policy distillation in preserving interpretability while maintaining performance is not guaranteed and depends heavily on the complexity of the original policy and the distillation process.', 'references': [{'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}, {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}, {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}, {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}, {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}, {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}, {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}]}
2025-05-31 19:23:37,773 INFO aicoscientist: Reviewed hypothesis: G1604, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:23:37,774 INFO aicoscientist: Step 3: Ranking 1
2025-05-31 19:23:37,774 INFO aicoscientist: Running tournament with 78 pairs.
2025-05-31 19:23:37,774 INFO aicoscientist: Debate: G5307 (score 4) vs G7295 (score 4) => Winner: G5307
2025-05-31 19:23:37,774 INFO aicoscientist: Updated Elo: Winner G5307 -> 1215.54, Loser G7295 -> 1174.50
2025-05-31 19:23:37,776 INFO aicoscientist: Debate: G5307 (score 4) vs E5336 (score 5) => Winner: E5336
2025-05-31 19:23:37,776 INFO aicoscientist: Updated Elo: Winner E5336 -> 1165.44, Loser G5307 -> 1196.39
2025-05-31 19:23:37,780 INFO aicoscientist: Debate: G5307 (score 4) vs G1604 (score 4) => Winner: G1604
2025-05-31 19:23:37,780 INFO aicoscientist: Updated Elo: Winner G1604 -> 1215.83, Loser G5307 -> 1180.56
2025-05-31 19:23:37,782 INFO aicoscientist: Debate: G5307 (score 4) vs G1351 (score 4) => Winner: G1351
2025-05-31 19:23:37,782 INFO aicoscientist: Updated Elo: Winner G1351 -> 1167.51, Loser G5307 -> 1163.16
2025-05-31 19:23:37,784 INFO aicoscientist: Debate: G5307 (score 4) vs G3758 (score 4) => Winner: G5307
2025-05-31 19:23:37,784 INFO aicoscientist: Updated Elo: Winner G5307 -> 1180.85, Loser G3758 -> 1182.31
2025-05-31 19:23:37,785 INFO aicoscientist: Debate: G5307 (score 4) vs G2583 (score 4) => Winner: G5307
2025-05-31 19:23:37,785 INFO aicoscientist: Updated Elo: Winner G5307 -> 1197.73, Loser G2583 -> 1183.12
2025-05-31 19:23:37,787 INFO aicoscientist: Debate: G5307 (score 4) vs G8633 (score 4) => Winner: G8633
2025-05-31 19:23:37,787 INFO aicoscientist: Updated Elo: Winner G8633 -> 1206.15, Loser G5307 -> 1181.37
2025-05-31 19:23:37,788 INFO aicoscientist: Debate: G5307 (score 4) vs G7452 (score 4) => Winner: G7452
2025-05-31 19:23:37,788 INFO aicoscientist: Updated Elo: Winner G7452 -> 1215.14, Loser G5307 -> 1166.22
2025-05-31 19:23:37,788 INFO aicoscientist: Debate: G5307 (score 4) vs G2823 (score 4) => Winner: G5307
2025-05-31 19:23:37,788 INFO aicoscientist: Updated Elo: Winner G5307 -> 1183.77, Loser G2823 -> 1182.45
2025-05-31 19:23:37,789 INFO aicoscientist: Debate: G5307 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:23:37,789 INFO aicoscientist: Updated Elo: Winner G7402 -> 1203.52, Loser G5307 -> 1167.96
2025-05-31 19:23:37,790 INFO aicoscientist: Debate: G5307 (score 4) vs G4199 (score 4) => Winner: G4199
2025-05-31 19:23:37,790 INFO aicoscientist: Updated Elo: Winner G4199 -> 1263.73, Loser G5307 -> 1155.73
2025-05-31 19:23:37,791 INFO aicoscientist: Debate: G5307 (score 4) vs G3657 (score 4) => Winner: G5307
2025-05-31 19:23:37,791 INFO aicoscientist: Updated Elo: Winner G5307 -> 1177.41, Loser G3657 -> 1262.89
2025-05-31 19:23:37,792 INFO aicoscientist: Debate: G7295 (score 4) vs E5336 (score 5) => Winner: E5336
2025-05-31 19:23:37,792 INFO aicoscientist: Updated Elo: Winner E5336 -> 1181.86, Loser G7295 -> 1158.08
2025-05-31 19:23:37,793 INFO aicoscientist: Debate: G7295 (score 4) vs G1604 (score 4) => Winner: G1604
2025-05-31 19:23:37,793 INFO aicoscientist: Updated Elo: Winner G1604 -> 1229.20, Loser G7295 -> 1144.72
2025-05-31 19:23:37,793 INFO aicoscientist: Debate: G7295 (score 4) vs G1351 (score 4) => Winner: G1351
2025-05-31 19:23:37,793 INFO aicoscientist: Updated Elo: Winner G1351 -> 1182.46, Loser G7295 -> 1129.76
2025-05-31 19:23:37,794 INFO aicoscientist: Debate: G7295 (score 4) vs G3758 (score 4) => Winner: G3758
2025-05-31 19:23:37,794 INFO aicoscientist: Updated Elo: Winner G3758 -> 1195.91, Loser G7295 -> 1116.17
2025-05-31 19:23:37,795 INFO aicoscientist: Debate: G7295 (score 4) vs G2583 (score 4) => Winner: G7295
2025-05-31 19:23:37,795 INFO aicoscientist: Updated Elo: Winner G7295 -> 1135.21, Loser G2583 -> 1164.07
2025-05-31 19:23:37,796 INFO aicoscientist: Debate: G7295 (score 4) vs G8633 (score 4) => Winner: G7295
2025-05-31 19:23:37,796 INFO aicoscientist: Updated Elo: Winner G7295 -> 1154.43, Loser G8633 -> 1186.92
2025-05-31 19:23:37,796 INFO aicoscientist: Debate: G7295 (score 4) vs G7452 (score 4) => Winner: G7295
2025-05-31 19:23:37,796 INFO aicoscientist: Updated Elo: Winner G7295 -> 1173.20, Loser G7452 -> 1196.38
2025-05-31 19:23:37,797 INFO aicoscientist: Debate: G7295 (score 4) vs G2823 (score 4) => Winner: G2823
2025-05-31 19:23:37,797 INFO aicoscientist: Updated Elo: Winner G2823 -> 1198.02, Loser G7295 -> 1157.63
2025-05-31 19:23:37,797 INFO aicoscientist: Debate: G7295 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:23:37,798 INFO aicoscientist: Updated Elo: Winner G7402 -> 1217.42, Loser G7295 -> 1143.73
2025-05-31 19:23:37,798 INFO aicoscientist: Debate: G7295 (score 4) vs G4199 (score 4) => Winner: G7295
2025-05-31 19:23:37,798 INFO aicoscientist: Updated Elo: Winner G7295 -> 1165.04, Loser G4199 -> 1242.41
2025-05-31 19:23:37,799 INFO aicoscientist: Debate: G7295 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:23:37,799 INFO aicoscientist: Updated Elo: Winner G3657 -> 1274.50, Loser G7295 -> 1153.44
2025-05-31 19:23:37,799 INFO aicoscientist: Debate: E5336 (score 5) vs G1604 (score 4) => Winner: E5336
2025-05-31 19:23:37,799 INFO aicoscientist: Updated Elo: Winner E5336 -> 1200.03, Loser G1604 -> 1211.03
2025-05-31 19:23:37,800 INFO aicoscientist: Debate: E5336 (score 5) vs G1351 (score 4) => Winner: E5336
2025-05-31 19:23:37,800 INFO aicoscientist: Updated Elo: Winner E5336 -> 1215.22, Loser G1351 -> 1167.27
2025-05-31 19:23:37,801 INFO aicoscientist: Debate: E5336 (score 5) vs G3758 (score 4) => Winner: E5336
2025-05-31 19:23:37,801 INFO aicoscientist: Updated Elo: Winner E5336 -> 1230.33, Loser G3758 -> 1180.80
2025-05-31 19:23:37,801 INFO aicoscientist: Debate: E5336 (score 5) vs G2583 (score 4) => Winner: E5336
2025-05-31 19:23:37,802 INFO aicoscientist: Updated Elo: Winner E5336 -> 1243.31, Loser G2583 -> 1151.09
2025-05-31 19:23:37,802 INFO aicoscientist: Debate: E5336 (score 5) vs G8633 (score 4) => Winner: E5336
2025-05-31 19:23:37,802 INFO aicoscientist: Updated Elo: Winner E5336 -> 1256.74, Loser G8633 -> 1173.50
2025-05-31 19:23:37,803 INFO aicoscientist: Debate: E5336 (score 5) vs G7452 (score 4) => Winner: E5336
2025-05-31 19:23:37,803 INFO aicoscientist: Updated Elo: Winner E5336 -> 1269.99, Loser G7452 -> 1183.13
2025-05-31 19:23:37,803 INFO aicoscientist: Debate: E5336 (score 5) vs G2823 (score 4) => Winner: E5336
2025-05-31 19:23:37,803 INFO aicoscientist: Updated Elo: Winner E5336 -> 1282.72, Loser G2823 -> 1185.29
2025-05-31 19:23:37,804 INFO aicoscientist: Debate: E5336 (score 5) vs G7402 (score 4) => Winner: E5336
2025-05-31 19:23:37,804 INFO aicoscientist: Updated Elo: Winner E5336 -> 1295.75, Loser G7402 -> 1204.39
2025-05-31 19:23:37,804 INFO aicoscientist: Debate: E5336 (score 5) vs G4199 (score 4) => Winner: E5336
2025-05-31 19:23:37,804 INFO aicoscientist: Updated Elo: Winner E5336 -> 1309.31, Loser G4199 -> 1228.85
2025-05-31 19:23:37,805 INFO aicoscientist: Debate: E5336 (score 5) vs G3657 (score 4) => Winner: E5336
2025-05-31 19:23:37,805 INFO aicoscientist: Updated Elo: Winner E5336 -> 1323.71, Loser G3657 -> 1260.10
2025-05-31 19:23:37,805 INFO aicoscientist: Debate: G1604 (score 4) vs G1351 (score 4) => Winner: G1604
2025-05-31 19:23:37,805 INFO aicoscientist: Updated Elo: Winner G1604 -> 1225.03, Loser G1351 -> 1153.28
2025-05-31 19:23:37,806 INFO aicoscientist: Debate: G1604 (score 4) vs G3758 (score 4) => Winner: G3758
2025-05-31 19:23:37,806 INFO aicoscientist: Updated Elo: Winner G3758 -> 1198.82, Loser G1604 -> 1207.00
2025-05-31 19:23:37,806 INFO aicoscientist: Debate: G1604 (score 4) vs G2583 (score 4) => Winner: G2583
2025-05-31 19:23:37,807 INFO aicoscientist: Updated Elo: Winner G2583 -> 1169.64, Loser G1604 -> 1188.45
2025-05-31 19:23:37,807 INFO aicoscientist: Debate: G1604 (score 4) vs G8633 (score 4) => Winner: G8633
2025-05-31 19:23:37,807 INFO aicoscientist: Updated Elo: Winner G8633 -> 1190.19, Loser G1604 -> 1171.76
2025-05-31 19:23:37,808 INFO aicoscientist: Debate: G1604 (score 4) vs G7452 (score 4) => Winner: G7452
2025-05-31 19:23:37,808 INFO aicoscientist: Updated Elo: Winner G7452 -> 1198.60, Loser G1604 -> 1156.28
2025-05-31 19:23:37,808 INFO aicoscientist: Debate: G1604 (score 4) vs G2823 (score 4) => Winner: G1604
2025-05-31 19:23:37,808 INFO aicoscientist: Updated Elo: Winner G1604 -> 1173.62, Loser G2823 -> 1167.96
2025-05-31 19:23:37,809 INFO aicoscientist: Debate: G1604 (score 4) vs G7402 (score 4) => Winner: G1604
2025-05-31 19:23:37,809 INFO aicoscientist: Updated Elo: Winner G1604 -> 1191.03, Loser G7402 -> 1186.98
2025-05-31 19:23:37,809 INFO aicoscientist: Debate: G1604 (score 4) vs G4199 (score 4) => Winner: G4199
2025-05-31 19:23:37,809 INFO aicoscientist: Updated Elo: Winner G4199 -> 1243.12, Loser G1604 -> 1176.76
2025-05-31 19:23:37,810 INFO aicoscientist: Debate: G1604 (score 4) vs G3657 (score 4) => Winner: G1604
2025-05-31 19:23:37,810 INFO aicoscientist: Updated Elo: Winner G1604 -> 1196.53, Loser G3657 -> 1240.33
2025-05-31 19:23:37,810 INFO aicoscientist: Debate: G1351 (score 4) vs G3758 (score 4) => Winner: G3758
2025-05-31 19:23:37,810 INFO aicoscientist: Updated Elo: Winner G3758 -> 1212.74, Loser G1351 -> 1139.36
2025-05-31 19:23:37,811 INFO aicoscientist: Debate: G1351 (score 4) vs G2583 (score 4) => Winner: G1351
2025-05-31 19:23:37,811 INFO aicoscientist: Updated Elo: Winner G1351 -> 1156.75, Loser G2583 -> 1152.25
2025-05-31 19:23:37,811 INFO aicoscientist: Debate: G1351 (score 4) vs G8633 (score 4) => Winner: G8633
2025-05-31 19:23:37,811 INFO aicoscientist: Updated Elo: Winner G8633 -> 1204.65, Loser G1351 -> 1142.29
2025-05-31 19:23:37,812 INFO aicoscientist: Debate: G1351 (score 4) vs G7452 (score 4) => Winner: G7452
2025-05-31 19:23:37,812 INFO aicoscientist: Updated Elo: Winner G7452 -> 1212.03, Loser G1351 -> 1128.86
2025-05-31 19:23:37,812 INFO aicoscientist: Debate: G1351 (score 4) vs G2823 (score 4) => Winner: G1351
2025-05-31 19:23:37,812 INFO aicoscientist: Updated Elo: Winner G1351 -> 1146.65, Loser G2823 -> 1150.17
2025-05-31 19:23:37,813 INFO aicoscientist: Debate: G1351 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:23:37,813 INFO aicoscientist: Updated Elo: Winner G7402 -> 1201.13, Loser G1351 -> 1132.50
2025-05-31 19:23:37,813 INFO aicoscientist: Debate: G1351 (score 4) vs G4199 (score 4) => Winner: G1351
2025-05-31 19:23:37,813 INFO aicoscientist: Updated Elo: Winner G1351 -> 1153.43, Loser G4199 -> 1222.19
2025-05-31 19:23:37,814 INFO aicoscientist: Debate: G1351 (score 4) vs G3657 (score 4) => Winner: G1351
2025-05-31 19:23:37,814 INFO aicoscientist: Updated Elo: Winner G1351 -> 1173.35, Loser G3657 -> 1220.41
2025-05-31 19:23:37,814 INFO aicoscientist: Debate: G3758 (score 4) vs G2583 (score 4) => Winner: G2583
2025-05-31 19:23:37,814 INFO aicoscientist: Updated Elo: Winner G2583 -> 1171.01, Loser G3758 -> 1193.98
2025-05-31 19:23:37,814 INFO aicoscientist: Debate: G3758 (score 4) vs G8633 (score 4) => Winner: G3758
2025-05-31 19:23:37,814 INFO aicoscientist: Updated Elo: Winner G3758 -> 1210.47, Loser G8633 -> 1188.16
2025-05-31 19:23:37,815 INFO aicoscientist: Debate: G3758 (score 4) vs G7452 (score 4) => Winner: G3758
2025-05-31 19:23:37,815 INFO aicoscientist: Updated Elo: Winner G3758 -> 1226.54, Loser G7452 -> 1195.96
2025-05-31 19:23:37,815 INFO aicoscientist: Debate: G3758 (score 4) vs G2823 (score 4) => Winner: G3758
2025-05-31 19:23:37,815 INFO aicoscientist: Updated Elo: Winner G3758 -> 1239.08, Loser G2823 -> 1137.63
2025-05-31 19:23:37,816 INFO aicoscientist: Debate: G3758 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:23:37,816 INFO aicoscientist: Updated Elo: Winner G7402 -> 1218.87, Loser G3758 -> 1221.34
2025-05-31 19:23:37,817 INFO aicoscientist: Debate: G3758 (score 4) vs G4199 (score 4) => Winner: G3758
2025-05-31 19:23:37,817 INFO aicoscientist: Updated Elo: Winner G3758 -> 1237.38, Loser G4199 -> 1206.15
2025-05-31 19:23:37,817 INFO aicoscientist: Debate: G3758 (score 4) vs G3657 (score 4) => Winner: G3758
2025-05-31 19:23:37,817 INFO aicoscientist: Updated Elo: Winner G3758 -> 1252.60, Loser G3657 -> 1205.19
2025-05-31 19:23:37,818 INFO aicoscientist: Debate: G2583 (score 4) vs G8633 (score 4) => Winner: G2583
2025-05-31 19:23:37,818 INFO aicoscientist: Updated Elo: Winner G2583 -> 1187.80, Loser G8633 -> 1171.37
2025-05-31 19:23:37,818 INFO aicoscientist: Debate: G2583 (score 4) vs G7452 (score 4) => Winner: G2583
2025-05-31 19:23:37,818 INFO aicoscientist: Updated Elo: Winner G2583 -> 1204.17, Loser G7452 -> 1179.59
2025-05-31 19:23:37,818 INFO aicoscientist: Debate: G2583 (score 4) vs G2823 (score 4) => Winner: G2823
2025-05-31 19:23:37,819 INFO aicoscientist: Updated Elo: Winner G2823 -> 1156.65, Loser G2583 -> 1185.15
2025-05-31 19:23:37,819 INFO aicoscientist: Debate: G2583 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:23:37,819 INFO aicoscientist: Updated Elo: Winner G7402 -> 1233.32, Loser G2583 -> 1170.69
2025-05-31 19:23:37,819 INFO aicoscientist: Debate: G2583 (score 4) vs G4199 (score 4) => Winner: G2583
2025-05-31 19:23:37,819 INFO aicoscientist: Updated Elo: Winner G2583 -> 1188.32, Loser G4199 -> 1188.52
2025-05-31 19:23:37,820 INFO aicoscientist: Debate: G2583 (score 4) vs G3657 (score 4) => Winner: G2583
2025-05-31 19:23:37,820 INFO aicoscientist: Updated Elo: Winner G2583 -> 1205.10, Loser G3657 -> 1188.42
2025-05-31 19:23:37,820 INFO aicoscientist: Debate: G8633 (score 4) vs G7452 (score 4) => Winner: G7452
2025-05-31 19:23:37,820 INFO aicoscientist: Updated Elo: Winner G7452 -> 1195.21, Loser G8633 -> 1155.75
2025-05-31 19:23:37,821 INFO aicoscientist: Debate: G8633 (score 4) vs G2823 (score 4) => Winner: G2823
2025-05-31 19:23:37,821 INFO aicoscientist: Updated Elo: Winner G2823 -> 1172.61, Loser G8633 -> 1139.79
2025-05-31 19:23:37,821 INFO aicoscientist: Debate: G8633 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:23:37,821 INFO aicoscientist: Updated Elo: Winner G7402 -> 1245.12, Loser G8633 -> 1128.00
2025-05-31 19:23:37,822 INFO aicoscientist: Debate: G8633 (score 4) vs G4199 (score 4) => Winner: G8633
2025-05-31 19:23:37,822 INFO aicoscientist: Updated Elo: Winner G8633 -> 1146.76, Loser G4199 -> 1169.76
2025-05-31 19:23:37,822 INFO aicoscientist: Debate: G8633 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:23:37,822 INFO aicoscientist: Updated Elo: Winner G3657 -> 1202.51, Loser G8633 -> 1132.67
2025-05-31 19:23:37,822 INFO aicoscientist: Debate: G7452 (score 4) vs G2823 (score 4) => Winner: G2823
2025-05-31 19:23:37,823 INFO aicoscientist: Updated Elo: Winner G2823 -> 1189.65, Loser G7452 -> 1178.17
2025-05-31 19:23:37,823 INFO aicoscientist: Debate: G7452 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:23:37,823 INFO aicoscientist: Updated Elo: Winner G7402 -> 1258.07, Loser G7452 -> 1165.21
2025-05-31 19:23:37,823 INFO aicoscientist: Debate: G7452 (score 4) vs G4199 (score 4) => Winner: G4199
2025-05-31 19:23:37,823 INFO aicoscientist: Updated Elo: Winner G4199 -> 1185.55, Loser G7452 -> 1149.42
2025-05-31 19:23:37,824 INFO aicoscientist: Debate: G7452 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:23:37,824 INFO aicoscientist: Updated Elo: Winner G3657 -> 1216.08, Loser G7452 -> 1135.85
2025-05-31 19:23:37,824 INFO aicoscientist: Debate: G2823 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:23:37,824 INFO aicoscientist: Updated Elo: Winner G7402 -> 1270.96, Loser G2823 -> 1176.76
2025-05-31 19:23:37,825 INFO aicoscientist: Debate: G2823 (score 4) vs G4199 (score 4) => Winner: G4199
2025-05-31 19:23:37,825 INFO aicoscientist: Updated Elo: Winner G4199 -> 1201.15, Loser G2823 -> 1161.17
2025-05-31 19:23:37,825 INFO aicoscientist: Debate: G2823 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:23:37,825 INFO aicoscientist: Updated Elo: Winner G3657 -> 1229.57, Loser G2823 -> 1147.68
2025-05-31 19:23:37,826 INFO aicoscientist: Debate: G7402 (score 4) vs G4199 (score 4) => Winner: G7402
2025-05-31 19:23:37,826 INFO aicoscientist: Updated Elo: Winner G7402 -> 1283.79, Loser G4199 -> 1188.32
2025-05-31 19:23:37,826 INFO aicoscientist: Debate: G7402 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:23:37,826 INFO aicoscientist: Updated Elo: Winner G3657 -> 1248.05, Loser G7402 -> 1265.31
2025-05-31 19:23:37,826 INFO aicoscientist: Debate: G4199 (score 4) vs G3657 (score 4) => Winner: G4199
2025-05-31 19:23:37,826 INFO aicoscientist: Updated Elo: Winner G4199 -> 1207.04, Loser G3657 -> 1229.33
2025-05-31 19:23:37,827 INFO aicoscientist: Step 4: Evolution
2025-05-31 19:23:37,827 INFO aicoscientist: Combining hypotheses E5336 and G7402 into E4802
2025-05-31 19:23:37,827 INFO aicoscientist: Evolved hypothesis created: E4802 from parents ['E5336', 'G7402']
2025-05-31 19:23:37,828 INFO aicoscientist: Step 4a: Reviewing Evolved Hypotheses
2025-05-31 19:23:37,828 INFO aicoscientist: LLM reflection called with temperature: 0.50
2025-05-31 19:23:41,255 INFO aicoscientist: LLM call completed - Type: reflection, Model: google/gemini-2.0-flash-001, Success: True, Time: 3420.04ms, Tokens: 523/436, Retries: 0
2025-05-31 19:23:41,255 INFO aicoscientist: LLM reflection response for hypothesis: ```json
{
  "novelty_review": "MEDIUM",
  "feasibility_review": "MEDIUM",
  "comment": "This is a complex hypothesis combining three potentially impactful ideas. The individual components (BNNs for exploration, meta-learning for curriculum design, and symbolic reasoning in RL) have been explored separately, but their combined integration presents a novel challenge. The success hinges on effectively integrating these components without introducing excessive complexity or computational overhead. The combination of active exploration using BNN uncertainty, autonomous curriculum design via meta-learning, and symbolic reasoning for generalization is where the novelty lies. However, the feasibility is reduced by the complexity of integrating three advanced techniques.",
  "references": [
    "arXiv:1703.04977 - Variational Bayesian Neural Networks",
    "arXiv:1802.06057 - Neural Architecture Search with Reinforcement Learning",
    "arXiv:1807.05739 - Meta-Learning for Low-Resource Natural Language Processing",
    "arXiv:1901.08195 - Exploration by Random Network Distillation",
    "arXiv:1905.10982 - Learning to Explore via Meta-Reinforcement Learning",
    "arXiv:2006.05968 - Uncertainty-aware Reinforcement Learning for Safe Exploration",
    "arXiv:2010.09484 - Symbolic Planning with Neural Reward Functions for Generalizable Robot Manipulation",
    "arXiv:2103.01909 - Meta-Curriculum Learning",
    "arXiv:2106.09686 - Active Exploration via Differentiable Information Gain",
    "arXiv:2203.07657 - Combining Symbolic and Subsymbolic AI for Robust Generalization",
    "\"Learning Symbolic Rules for Reinforcement Learning\" - ICML 2019",
    "\"Meta-Learning for Curriculum Design\" - NeurIPS 2019"
  ]
}
```
2025-05-31 19:23:41,255 INFO aicoscientist: Parsed reflection data: {'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'comment': 'This is a complex hypothesis combining three potentially impactful ideas. The individual components (BNNs for exploration, meta-learning for curriculum design, and symbolic reasoning in RL) have been explored separately, but their combined integration presents a novel challenge. The success hinges on effectively integrating these components without introducing excessive complexity or computational overhead. The combination of active exploration using BNN uncertainty, autonomous curriculum design via meta-learning, and symbolic reasoning for generalization is where the novelty lies. However, the feasibility is reduced by the complexity of integrating three advanced techniques.', 'references': ['arXiv:1703.04977 - Variational Bayesian Neural Networks', 'arXiv:1802.06057 - Neural Architecture Search with Reinforcement Learning', 'arXiv:1807.05739 - Meta-Learning for Low-Resource Natural Language Processing', 'arXiv:1901.08195 - Exploration by Random Network Distillation', 'arXiv:1905.10982 - Learning to Explore via Meta-Reinforcement Learning', 'arXiv:2006.05968 - Uncertainty-aware Reinforcement Learning for Safe Exploration', 'arXiv:2010.09484 - Symbolic Planning with Neural Reward Functions for Generalizable Robot Manipulation', 'arXiv:2103.01909 - Meta-Curriculum Learning', 'arXiv:2106.09686 - Active Exploration via Differentiable Information Gain', 'arXiv:2203.07657 - Combining Symbolic and Subsymbolic AI for Robust Generalization', '"Learning Symbolic Rules for Reinforcement Learning" - ICML 2019', '"Meta-Learning for Curriculum Design" - NeurIPS 2019']}
2025-05-31 19:24:20,512 INFO aicoscientist: Reviewed hypothesis: E4802, Novelty: MEDIUM, Feasibility: MEDIUM
2025-05-31 19:24:20,512 INFO aicoscientist: Step 5: Ranking 2
2025-05-31 19:24:20,513 INFO aicoscientist: Running tournament with 91 pairs.
2025-05-31 19:24:20,513 INFO aicoscientist: Debate: G1351 (score 4) vs G1604 (score 4) => Winner: G1351
2025-05-31 19:24:20,513 INFO aicoscientist: Updated Elo: Winner G1351 -> 1190.42, Loser G1604 -> 1179.46
2025-05-31 19:24:20,516 INFO aicoscientist: Debate: G1351 (score 4) vs G2583 (score 4) => Winner: G2583
2025-05-31 19:24:20,517 INFO aicoscientist: Updated Elo: Winner G2583 -> 1220.42, Loser G1351 -> 1175.09
2025-05-31 19:24:20,519 INFO aicoscientist: Debate: G1351 (score 4) vs G4199 (score 4) => Winner: G1351
2025-05-31 19:24:20,520 INFO aicoscientist: Updated Elo: Winner G1351 -> 1192.56, Loser G4199 -> 1189.58
2025-05-31 19:24:20,521 INFO aicoscientist: Debate: G1351 (score 4) vs E4802 (score 4) => Winner: G1351
2025-05-31 19:24:20,522 INFO aicoscientist: Updated Elo: Winner G1351 -> 1208.90, Loser E4802 -> 1183.66
2025-05-31 19:24:20,523 INFO aicoscientist: Debate: G1351 (score 4) vs G5307 (score 4) => Winner: G1351
2025-05-31 19:24:20,523 INFO aicoscientist: Updated Elo: Winner G1351 -> 1223.45, Loser G5307 -> 1162.85
2025-05-31 19:24:20,524 INFO aicoscientist: Debate: G1351 (score 4) vs G2823 (score 4) => Winner: G1351
2025-05-31 19:24:20,524 INFO aicoscientist: Updated Elo: Winner G1351 -> 1236.02, Loser G2823 -> 1135.11
2025-05-31 19:24:20,525 INFO aicoscientist: Debate: G1351 (score 4) vs G7295 (score 4) => Winner: G7295
2025-05-31 19:24:20,525 INFO aicoscientist: Updated Elo: Winner G7295 -> 1173.17, Loser G1351 -> 1216.29
2025-05-31 19:24:20,526 INFO aicoscientist: Debate: G1351 (score 4) vs G3758 (score 4) => Winner: G1351
2025-05-31 19:24:20,526 INFO aicoscientist: Updated Elo: Winner G1351 -> 1233.95, Loser G3758 -> 1234.93
2025-05-31 19:24:20,527 INFO aicoscientist: Debate: G1351 (score 4) vs E5336 (score 5) => Winner: E5336
2025-05-31 19:24:20,527 INFO aicoscientist: Updated Elo: Winner E5336 -> 1335.67, Loser G1351 -> 1222.00
2025-05-31 19:24:20,528 INFO aicoscientist: Debate: G1351 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:24:20,528 INFO aicoscientist: Updated Elo: Winner G3657 -> 1244.99, Loser G1351 -> 1206.33
2025-05-31 19:24:20,529 INFO aicoscientist: Debate: G1351 (score 4) vs G7402 (score 4) => Winner: G1351
2025-05-31 19:24:20,529 INFO aicoscientist: Updated Elo: Winner G1351 -> 1225.02, Loser G7402 -> 1246.62
2025-05-31 19:24:20,529 INFO aicoscientist: Debate: G1351 (score 4) vs G7452 (score 4) => Winner: G7452
2025-05-31 19:24:20,530 INFO aicoscientist: Updated Elo: Winner G7452 -> 1155.87, Loser G1351 -> 1205.01
2025-05-31 19:24:20,530 INFO aicoscientist: Debate: G1351 (score 4) vs G8633 (score 4) => Winner: G8633
2025-05-31 19:24:20,530 INFO aicoscientist: Updated Elo: Winner G8633 -> 1151.95, Loser G1351 -> 1185.72
2025-05-31 19:24:20,531 INFO aicoscientist: Debate: G1604 (score 4) vs G2583 (score 4) => Winner: G2583
2025-05-31 19:24:20,531 INFO aicoscientist: Updated Elo: Winner G2583 -> 1234.54, Loser G1604 -> 1165.34
2025-05-31 19:24:20,531 INFO aicoscientist: Debate: G1604 (score 4) vs G4199 (score 4) => Winner: G4199
2025-05-31 19:24:20,532 INFO aicoscientist: Updated Elo: Winner G4199 -> 1204.46, Loser G1604 -> 1150.46
2025-05-31 19:24:20,532 INFO aicoscientist: Debate: G1604 (score 4) vs E4802 (score 4) => Winner: E4802
2025-05-31 19:24:20,532 INFO aicoscientist: Updated Elo: Winner E4802 -> 1198.13, Loser G1604 -> 1135.98
2025-05-31 19:24:20,533 INFO aicoscientist: Debate: G1604 (score 4) vs G5307 (score 4) => Winner: G5307
2025-05-31 19:24:20,533 INFO aicoscientist: Updated Elo: Winner G5307 -> 1177.62, Loser G1604 -> 1121.22
2025-05-31 19:24:20,534 INFO aicoscientist: Debate: G1604 (score 4) vs G2823 (score 4) => Winner: G2823
2025-05-31 19:24:20,534 INFO aicoscientist: Updated Elo: Winner G2823 -> 1150.47, Loser G1604 -> 1105.86
2025-05-31 19:24:20,535 INFO aicoscientist: Debate: G1604 (score 4) vs G7295 (score 4) => Winner: G7295
2025-05-31 19:24:20,535 INFO aicoscientist: Updated Elo: Winner G7295 -> 1186.11, Loser G1604 -> 1092.92
2025-05-31 19:24:20,535 INFO aicoscientist: Debate: G1604 (score 4) vs G3758 (score 4) => Winner: G3758
2025-05-31 19:24:20,535 INFO aicoscientist: Updated Elo: Winner G3758 -> 1244.73, Loser G1604 -> 1083.12
2025-05-31 19:24:20,536 INFO aicoscientist: Debate: G1604 (score 4) vs E5336 (score 5) => Winner: E5336
2025-05-31 19:24:20,536 INFO aicoscientist: Updated Elo: Winner E5336 -> 1341.73, Loser G1604 -> 1077.05
2025-05-31 19:24:20,536 INFO aicoscientist: Debate: G1604 (score 4) vs G3657 (score 4) => Winner: G1604
2025-05-31 19:24:20,537 INFO aicoscientist: Updated Elo: Winner G1604 -> 1100.24, Loser G3657 -> 1221.81
2025-05-31 19:24:20,537 INFO aicoscientist: Debate: G1604 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:24:20,537 INFO aicoscientist: Updated Elo: Winner G7402 -> 1256.25, Loser G1604 -> 1090.61
2025-05-31 19:24:20,538 INFO aicoscientist: Debate: G1604 (score 4) vs G7452 (score 4) => Winner: G1604
2025-05-31 19:24:20,538 INFO aicoscientist: Updated Elo: Winner G1604 -> 1109.58, Loser G7452 -> 1136.90
2025-05-31 19:24:20,538 INFO aicoscientist: Debate: G1604 (score 4) vs G8633 (score 4) => Winner: G8633
2025-05-31 19:24:20,538 INFO aicoscientist: Updated Elo: Winner G8633 -> 1166.01, Loser G1604 -> 1095.52
2025-05-31 19:24:20,539 INFO aicoscientist: Debate: G2583 (score 4) vs G4199 (score 4) => Winner: G2583
2025-05-31 19:24:20,539 INFO aicoscientist: Updated Elo: Winner G2583 -> 1249.16, Loser G4199 -> 1189.84
2025-05-31 19:24:20,539 INFO aicoscientist: Debate: G2583 (score 4) vs E4802 (score 4) => Winner: G2583
2025-05-31 19:24:20,539 INFO aicoscientist: Updated Elo: Winner G2583 -> 1262.83, Loser E4802 -> 1184.47
2025-05-31 19:24:20,540 INFO aicoscientist: Debate: G2583 (score 4) vs G5307 (score 4) => Winner: G5307
2025-05-31 19:24:20,540 INFO aicoscientist: Updated Elo: Winner G5307 -> 1197.46, Loser G2583 -> 1242.98
2025-05-31 19:24:20,541 INFO aicoscientist: Debate: G2583 (score 4) vs G2823 (score 4) => Winner: G2823
2025-05-31 19:24:20,541 INFO aicoscientist: Updated Elo: Winner G2823 -> 1170.63, Loser G2583 -> 1222.82
2025-05-31 19:24:20,541 INFO aicoscientist: Debate: G2583 (score 4) vs G7295 (score 4) => Winner: G7295
2025-05-31 19:24:20,542 INFO aicoscientist: Updated Elo: Winner G7295 -> 1203.79, Loser G2583 -> 1205.13
2025-05-31 19:24:20,542 INFO aicoscientist: Debate: G2583 (score 4) vs G3758 (score 4) => Winner: G3758
2025-05-31 19:24:20,542 INFO aicoscientist: Updated Elo: Winner G3758 -> 1258.92, Loser G2583 -> 1190.95
2025-05-31 19:24:20,543 INFO aicoscientist: Debate: G2583 (score 4) vs E5336 (score 5) => Winner: E5336
2025-05-31 19:24:20,543 INFO aicoscientist: Updated Elo: Winner E5336 -> 1351.19, Loser G2583 -> 1181.49
2025-05-31 19:24:20,543 INFO aicoscientist: Debate: G2583 (score 4) vs G3657 (score 4) => Winner: G2583
2025-05-31 19:24:20,543 INFO aicoscientist: Updated Elo: Winner G2583 -> 1199.34, Loser G3657 -> 1203.96
2025-05-31 19:24:20,544 INFO aicoscientist: Debate: G2583 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:24:20,544 INFO aicoscientist: Updated Elo: Winner G7402 -> 1269.65, Loser G2583 -> 1185.93
2025-05-31 19:24:20,544 INFO aicoscientist: Debate: G2583 (score 4) vs G7452 (score 4) => Winner: G7452
2025-05-31 19:24:20,544 INFO aicoscientist: Updated Elo: Winner G7452 -> 1155.14, Loser G2583 -> 1167.69
2025-05-31 19:24:20,545 INFO aicoscientist: Debate: G2583 (score 4) vs G8633 (score 4) => Winner: G2583
2025-05-31 19:24:20,545 INFO aicoscientist: Updated Elo: Winner G2583 -> 1183.61, Loser G8633 -> 1150.09
2025-05-31 19:24:20,545 INFO aicoscientist: Debate: G4199 (score 4) vs E4802 (score 4) => Winner: E4802
2025-05-31 19:24:20,545 INFO aicoscientist: Updated Elo: Winner E4802 -> 1200.71, Loser G4199 -> 1173.60
2025-05-31 19:24:20,546 INFO aicoscientist: Debate: G4199 (score 4) vs G5307 (score 4) => Winner: G5307
2025-05-31 19:24:20,546 INFO aicoscientist: Updated Elo: Winner G5307 -> 1212.37, Loser G4199 -> 1158.69
2025-05-31 19:24:20,546 INFO aicoscientist: Debate: G4199 (score 4) vs G2823 (score 4) => Winner: G4199
2025-05-31 19:24:20,546 INFO aicoscientist: Updated Elo: Winner G4199 -> 1175.24, Loser G2823 -> 1154.08
2025-05-31 19:24:20,547 INFO aicoscientist: Debate: G4199 (score 4) vs G7295 (score 4) => Winner: G4199
2025-05-31 19:24:20,547 INFO aicoscientist: Updated Elo: Winner G4199 -> 1192.56, Loser G7295 -> 1186.48
2025-05-31 19:24:20,547 INFO aicoscientist: Debate: G4199 (score 4) vs G3758 (score 4) => Winner: G4199
2025-05-31 19:24:20,547 INFO aicoscientist: Updated Elo: Winner G4199 -> 1211.57, Loser G3758 -> 1239.90
2025-05-31 19:24:20,548 INFO aicoscientist: Debate: G4199 (score 4) vs E5336 (score 5) => Winner: E5336
2025-05-31 19:24:20,548 INFO aicoscientist: Updated Elo: Winner E5336 -> 1361.09, Loser G4199 -> 1201.68
2025-05-31 19:24:20,548 INFO aicoscientist: Debate: G4199 (score 4) vs G3657 (score 4) => Winner: G4199
2025-05-31 19:24:20,548 INFO aicoscientist: Updated Elo: Winner G4199 -> 1217.78, Loser G3657 -> 1187.85
2025-05-31 19:24:20,549 INFO aicoscientist: Debate: G4199 (score 4) vs G7402 (score 4) => Winner: G4199
2025-05-31 19:24:20,549 INFO aicoscientist: Updated Elo: Winner G4199 -> 1236.16, Loser G7402 -> 1251.28
2025-05-31 19:24:20,549 INFO aicoscientist: Debate: G4199 (score 4) vs G7452 (score 4) => Winner: G7452
2025-05-31 19:24:20,549 INFO aicoscientist: Updated Elo: Winner G7452 -> 1174.80, Loser G4199 -> 1216.49
2025-05-31 19:24:20,550 INFO aicoscientist: Debate: G4199 (score 4) vs G8633 (score 4) => Winner: G8633
2025-05-31 19:24:20,550 INFO aicoscientist: Updated Elo: Winner G8633 -> 1169.11, Loser G4199 -> 1197.47
2025-05-31 19:24:20,550 INFO aicoscientist: Debate: E4802 (score 4) vs G5307 (score 4) => Winner: E4802
2025-05-31 19:24:20,550 INFO aicoscientist: Updated Elo: Winner E4802 -> 1217.25, Loser G5307 -> 1195.83
2025-05-31 19:24:20,551 INFO aicoscientist: Debate: E4802 (score 4) vs G2823 (score 4) => Winner: G2823
2025-05-31 19:24:20,551 INFO aicoscientist: Updated Elo: Winner G2823 -> 1172.96, Loser E4802 -> 1198.37
2025-05-31 19:24:20,552 INFO aicoscientist: Debate: E4802 (score 4) vs G7295 (score 4) => Winner: E4802
2025-05-31 19:24:20,552 INFO aicoscientist: Updated Elo: Winner E4802 -> 1213.83, Loser G7295 -> 1171.03
2025-05-31 19:24:20,552 INFO aicoscientist: Debate: E4802 (score 4) vs G3758 (score 4) => Winner: G3758
2025-05-31 19:24:20,552 INFO aicoscientist: Updated Elo: Winner G3758 -> 1254.70, Loser E4802 -> 1199.02
2025-05-31 19:24:20,552 INFO aicoscientist: Debate: E4802 (score 4) vs E5336 (score 5) => Winner: E5336
2025-05-31 19:24:20,552 INFO aicoscientist: Updated Elo: Winner E5336 -> 1370.12, Loser E4802 -> 1189.99
2025-05-31 19:24:20,553 INFO aicoscientist: Debate: E4802 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:24:20,553 INFO aicoscientist: Updated Elo: Winner G3657 -> 1203.95, Loser E4802 -> 1173.89
2025-05-31 19:24:20,553 INFO aicoscientist: Debate: E4802 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:24:20,553 INFO aicoscientist: Updated Elo: Winner G7402 -> 1263.78, Loser E4802 -> 1161.40
2025-05-31 19:24:20,554 INFO aicoscientist: Debate: E4802 (score 4) vs G7452 (score 4) => Winner: E4802
2025-05-31 19:24:20,554 INFO aicoscientist: Updated Elo: Winner E4802 -> 1178.01, Loser G7452 -> 1158.19
2025-05-31 19:24:20,554 INFO aicoscientist: Debate: E4802 (score 4) vs G8633 (score 4) => Winner: E4802
2025-05-31 19:24:20,554 INFO aicoscientist: Updated Elo: Winner E4802 -> 1193.60, Loser G8633 -> 1153.52
2025-05-31 19:24:20,555 INFO aicoscientist: Debate: G5307 (score 4) vs G2823 (score 4) => Winner: G2823
2025-05-31 19:24:20,555 INFO aicoscientist: Updated Elo: Winner G2823 -> 1190.01, Loser G5307 -> 1178.78
2025-05-31 19:24:20,555 INFO aicoscientist: Debate: G5307 (score 4) vs G7295 (score 4) => Winner: G7295
2025-05-31 19:24:20,555 INFO aicoscientist: Updated Elo: Winner G7295 -> 1187.38, Loser G5307 -> 1162.42
2025-05-31 19:24:20,556 INFO aicoscientist: Debate: G5307 (score 4) vs G3758 (score 4) => Winner: G5307
2025-05-31 19:24:20,556 INFO aicoscientist: Updated Elo: Winner G5307 -> 1182.57, Loser G3758 -> 1234.55
2025-05-31 19:24:20,556 INFO aicoscientist: Debate: G5307 (score 4) vs E5336 (score 5) => Winner: E5336
2025-05-31 19:24:20,556 INFO aicoscientist: Updated Elo: Winner E5336 -> 1378.24, Loser G5307 -> 1174.46
2025-05-31 19:24:20,557 INFO aicoscientist: Debate: G5307 (score 4) vs G3657 (score 4) => Winner: G3657
2025-05-31 19:24:20,557 INFO aicoscientist: Updated Elo: Winner G3657 -> 1218.60, Loser G5307 -> 1159.81
2025-05-31 19:24:20,557 INFO aicoscientist: Debate: G5307 (score 4) vs G7402 (score 4) => Winner: G5307
2025-05-31 19:24:20,557 INFO aicoscientist: Updated Elo: Winner G5307 -> 1180.46, Loser G7402 -> 1243.13
2025-05-31 19:24:20,558 INFO aicoscientist: Debate: G5307 (score 4) vs G7452 (score 4) => Winner: G7452
2025-05-31 19:24:20,558 INFO aicoscientist: Updated Elo: Winner G7452 -> 1175.21, Loser G5307 -> 1163.44
2025-05-31 19:24:20,558 INFO aicoscientist: Debate: G5307 (score 4) vs G8633 (score 4) => Winner: G5307
2025-05-31 19:24:20,558 INFO aicoscientist: Updated Elo: Winner G5307 -> 1178.98, Loser G8633 -> 1137.97
2025-05-31 19:24:20,559 INFO aicoscientist: Debate: G2823 (score 4) vs G7295 (score 4) => Winner: G2823
2025-05-31 19:24:20,559 INFO aicoscientist: Updated Elo: Winner G2823 -> 1205.89, Loser G7295 -> 1171.51
2025-05-31 19:24:20,559 INFO aicoscientist: Debate: G2823 (score 4) vs G3758 (score 4) => Winner: G2823
2025-05-31 19:24:20,559 INFO aicoscientist: Updated Elo: Winner G2823 -> 1223.21, Loser G3758 -> 1217.23
2025-05-31 19:24:20,559 INFO aicoscientist: Debate: G2823 (score 4) vs E5336 (score 5) => Winner: E5336
2025-05-31 19:24:20,559 INFO aicoscientist: Updated Elo: Winner E5336 -> 1387.54, Loser G2823 -> 1213.91
2025-05-31 19:24:20,560 INFO aicoscientist: Debate: G2823 (score 4) vs G3657 (score 4) => Winner: G2823
2025-05-31 19:24:20,560 INFO aicoscientist: Updated Elo: Winner G2823 -> 1230.12, Loser G3657 -> 1202.38
2025-05-31 19:24:20,560 INFO aicoscientist: Debate: G2823 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:24:20,560 INFO aicoscientist: Updated Elo: Winner G7402 -> 1258.53, Loser G2823 -> 1214.72
2025-05-31 19:24:20,561 INFO aicoscientist: Debate: G2823 (score 4) vs G7452 (score 4) => Winner: G2823
2025-05-31 19:24:20,561 INFO aicoscientist: Updated Elo: Winner G2823 -> 1228.91, Loser G7452 -> 1161.02
2025-05-31 19:24:20,561 INFO aicoscientist: Debate: G2823 (score 4) vs G8633 (score 4) => Winner: G8633
2025-05-31 19:24:20,561 INFO aicoscientist: Updated Elo: Winner G8633 -> 1158.07, Loser G2823 -> 1208.82
2025-05-31 19:24:20,562 INFO aicoscientist: Debate: G7295 (score 4) vs G3758 (score 4) => Winner: G3758
2025-05-31 19:24:20,562 INFO aicoscientist: Updated Elo: Winner G3758 -> 1231.14, Loser G7295 -> 1157.60
2025-05-31 19:24:20,562 INFO aicoscientist: Debate: G7295 (score 4) vs E5336 (score 5) => Winner: E5336
2025-05-31 19:24:20,562 INFO aicoscientist: Updated Elo: Winner E5336 -> 1394.26, Loser G7295 -> 1150.87
2025-05-31 19:24:20,563 INFO aicoscientist: Debate: G7295 (score 4) vs G3657 (score 4) => Winner: G7295
2025-05-31 19:24:20,563 INFO aicoscientist: Updated Elo: Winner G7295 -> 1169.23, Loser G3657 -> 1184.03
2025-05-31 19:24:20,563 INFO aicoscientist: Debate: G7295 (score 4) vs G7402 (score 4) => Winner: G7295
2025-05-31 19:24:20,563 INFO aicoscientist: Updated Elo: Winner G7295 -> 1189.25, Loser G7402 -> 1238.50
2025-05-31 19:24:20,563 INFO aicoscientist: Debate: G7295 (score 4) vs G7452 (score 4) => Winner: G7295
2025-05-31 19:24:20,563 INFO aicoscientist: Updated Elo: Winner G7295 -> 1203.95, Loser G7452 -> 1146.32
2025-05-31 19:24:20,564 INFO aicoscientist: Debate: G7295 (score 4) vs G8633 (score 4) => Winner: G7295
2025-05-31 19:24:20,564 INFO aicoscientist: Updated Elo: Winner G7295 -> 1217.85, Loser G8633 -> 1144.17
2025-05-31 19:24:20,564 INFO aicoscientist: Debate: G3758 (score 4) vs E5336 (score 5) => Winner: E5336
2025-05-31 19:24:20,564 INFO aicoscientist: Updated Elo: Winner E5336 -> 1403.26, Loser G3758 -> 1222.14
2025-05-31 19:24:20,565 INFO aicoscientist: Debate: G3758 (score 4) vs G3657 (score 4) => Winner: G3758
2025-05-31 19:24:20,565 INFO aicoscientist: Updated Elo: Winner G3758 -> 1236.39, Loser G3657 -> 1169.77
2025-05-31 19:24:20,565 INFO aicoscientist: Debate: G3758 (score 4) vs G7402 (score 4) => Winner: G7402
2025-05-31 19:24:20,565 INFO aicoscientist: Updated Elo: Winner G7402 -> 1254.41, Loser G3758 -> 1220.49
2025-05-31 19:24:20,566 INFO aicoscientist: Debate: G3758 (score 4) vs G7452 (score 4) => Winner: G3758
2025-05-31 19:24:20,566 INFO aicoscientist: Updated Elo: Winner G3758 -> 1233.13, Loser G7452 -> 1133.69
2025-05-31 19:24:20,566 INFO aicoscientist: Debate: G3758 (score 4) vs G8633 (score 4) => Winner: G3758
2025-05-31 19:24:20,566 INFO aicoscientist: Updated Elo: Winner G3758 -> 1245.12, Loser G8633 -> 1132.18
2025-05-31 19:24:20,567 INFO aicoscientist: Debate: E5336 (score 5) vs G3657 (score 4) => Winner: E5336
2025-05-31 19:24:20,567 INFO aicoscientist: Updated Elo: Winner E5336 -> 1409.88, Loser G3657 -> 1163.16
2025-05-31 19:24:20,567 INFO aicoscientist: Debate: E5336 (score 5) vs G7402 (score 4) => Winner: E5336
2025-05-31 19:24:20,567 INFO aicoscientist: Updated Elo: Winner E5336 -> 1419.16, Loser G7402 -> 1245.12
2025-05-31 19:24:20,567 INFO aicoscientist: Debate: E5336 (score 5) vs G7452 (score 4) => Winner: E5336
2025-05-31 19:24:20,567 INFO aicoscientist: Updated Elo: Winner E5336 -> 1424.34, Loser G7452 -> 1128.50
2025-05-31 19:24:20,568 INFO aicoscientist: Debate: E5336 (score 5) vs G8633 (score 4) => Winner: E5336
2025-05-31 19:24:20,568 INFO aicoscientist: Updated Elo: Winner E5336 -> 1429.36, Loser G8633 -> 1127.16
2025-05-31 19:24:20,568 INFO aicoscientist: Debate: G3657 (score 4) vs G7402 (score 4) => Winner: G3657
2025-05-31 19:24:20,568 INFO aicoscientist: Updated Elo: Winner G3657 -> 1182.86, Loser G7402 -> 1225.42
2025-05-31 19:24:20,569 INFO aicoscientist: Debate: G3657 (score 4) vs G7452 (score 4) => Winner: G3657
2025-05-31 19:24:20,569 INFO aicoscientist: Updated Elo: Winner G3657 -> 1196.38, Loser G7452 -> 1114.98
2025-05-31 19:24:20,569 INFO aicoscientist: Debate: G3657 (score 4) vs G8633 (score 4) => Winner: G3657
2025-05-31 19:24:20,569 INFO aicoscientist: Updated Elo: Winner G3657 -> 1209.23, Loser G8633 -> 1114.31
2025-05-31 19:24:20,569 INFO aicoscientist: Debate: G7402 (score 4) vs G7452 (score 4) => Winner: G7402
2025-05-31 19:24:20,569 INFO aicoscientist: Updated Elo: Winner G7402 -> 1236.50, Loser G7452 -> 1103.91
2025-05-31 19:24:20,570 INFO aicoscientist: Debate: G7402 (score 4) vs G8633 (score 4) => Winner: G8633
2025-05-31 19:24:20,570 INFO aicoscientist: Updated Elo: Winner G8633 -> 1135.71, Loser G7402 -> 1215.09
2025-05-31 19:24:20,570 INFO aicoscientist: Debate: G7452 (score 4) vs G8633 (score 4) => Winner: G7452
2025-05-31 19:24:20,570 INFO aicoscientist: Updated Elo: Winner G7452 -> 1121.37, Loser G8633 -> 1118.25
2025-05-31 19:24:20,571 INFO aicoscientist: Step 6: Proximity Analysis
2025-05-31 19:24:24,166 INFO aicoscientist: Built proximity graph adjacency with 14 nodes.
2025-05-31 19:24:24,166 INFO aicoscientist: Step 7: Meta-Review
2025-05-31 19:24:24,166 INFO aicoscientist: Top hypotheses for meta-review: ['E5336', 'G3758', 'G7295']
2025-05-31 19:24:24,167 INFO aicoscientist: Meta-review complete: {'meta_review_critique': ['Overall hypothesis quality seems reasonable based on automated review.'], 'research_overview': {'top_ranked_hypotheses': [{'id': 'E5336', 'title': 'Combined: Uncertainty-Aware Exploration using Bayesian Neural Networks and Active Learning & Meta-Learning Curriculum Design for Autonomous Exploration', 'text': "Combination of:\n1. Hypothesis: An agent using a Bayesian Neural Network (BNN) to represent its environment model, combined with active learning techniques to query the environment based on uncertainty estimates, can achieve more efficient exploration and learning compared to agents using deterministic models. Rationale: Traditional exploration methods often rely on heuristics or random exploration. This hypothesis leverages the uncertainty estimates provided by BNNs to guide exploration, focusing on regions of the environment where the agent's knowledge is most uncertain. Active learning is used to select the most informative actions based on these uncertainty estimates. This extends current BNN-based RL by explicitly using the uncertainty estimates for *active* exploration, leading to more targeted data acquisition and faster learning.\n2. Hypothesis: A meta-learning agent can learn to design its own learning curriculum for exploration in novel, initially unknown environments. Rationale: Current curriculum learning often relies on hand-crafted curricula or pre-defined task distributions. This hypothesis explores autonomous curriculum design using meta-learning, allowing the agent to actively select tasks and environments that maximize its learning progress and exploration efficiency. This goes beyond existing work by focusing on *true* autonomy in curriculum creation, not just task selection from a fixed set. It addresses the gap of adapting exploration strategies to completely novel and unpredictable environments.", 'novelty_review': 'HIGH', 'feasibility_review': 'MEDIUM', 'elo_score': 1429.3638424005446, 'review_comments': ['The combination of BNN-based active exploration and meta-learning for curriculum design is promising but presents significant challenges. While both components have been explored individually, their synergistic integration for autonomous exploration in novel environments is less common. The feasibility hinges on the computational cost of BNNs, the stability of meta-learning algorithms, and the design of a suitable reward structure for the meta-learner to incentivize effective curriculum design. The success will also depend on the complexity of the target environments. Demonstrating a clear advantage over simpler exploration strategies (e.g., epsilon-greedy, Thompson sampling) in sufficiently complex environments is crucial.', 'This hypothesis combines two relatively novel ideas: using BNN uncertainty for active exploration and meta-learning for autonomous curriculum design. While both components have been explored individually, their combination, especially in the context of *completely* novel environments, represents a significant step forward. The feasibility is medium because implementing and training BNNs, active learning, and meta-learning algorithms simultaneously is computationally expensive and requires careful tuning. Success hinges on effectively integrating the uncertainty estimates from the BNN into the meta-learning framework for curriculum design.'], 'references': [{'title': 'Bayesian Neural Networks', 'venue': 'Advances in Neural Information Processing Systems (NeurIPS)', 'year': 1991}, {'title': 'Active Learning Literature Survey', 'venue': 'ACM Computing Surveys', 'year': 2009}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML)', 'year': 2017}, {'title': 'Curriculum Learning', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'year': 2009}, {'title': 'Learning to Explore via Meta-Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR)', 'year': 2017}, {'title': 'Efficient Exploration through Bayesian Deep Reinforcement Learning', 'venue': 'Conference on Robot Learning (CoRL)', 'year': 2018}, {'title': 'Meta-Curriculum Learning for Domain Adaptation', 'venue': 'International Conference on Computer Vision (ICCV)', 'year': 2021}, {'arXiv_ID': '1802.04849', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'arXiv_ID': '2006.05961', 'title': 'Uncertainty-Aware Reinforcement Learning for Safe Exploration'}, {'arXiv_ID': '2106.02762', 'title': 'Meta-Learning for Curriculum Design'}, {'title': 'Bayesian Neural Networks', 'venue': 'Review Article, e.g., Journal of Machine Learning Research'}, {'title': 'Active Learning for Regression', 'venue': 'Journal of Machine Learning Research'}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'ICML 2017'}, {'title': 'Meta-Curriculum Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Exploration by Random Network Distillation', 'venue': 'ICLR 2019'}, {'arXiv_ID': '1802.09471', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'title': 'Uncertainty-Aware Deep Reinforcement Learning', 'venue': 'AAAI 2018'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'AISTATS 2016'}, {'title': 'Curriculum Learning', 'venue': 'A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems'}, {'title': 'Self-Paced Learning for Latent Variable Models', 'venue': 'NIPS 2010'}], 'is_active': True, 'parent_ids': ['G3657', 'G1351']}, {'id': 'G3758', 'title': 'Composable Skill Discovery with Task-Agnostic Pre-training and Hierarchical Reinforcement Learning', 'text': 'Hypothesis: Combining task-agnostic pre-training to learn a set of diverse, reusable skills with a hierarchical reinforcement learning framework will enable faster adaptation to new tasks and improved sample efficiency in self-improving agents. Rationale: Learning skills from scratch for each new task is inefficient. Pre-training on unsupervised data can discover useful behavioral primitives. Hierarchical reinforcement learning then allows the agent to learn how to compose these skills to solve high-level tasks. The pre-trained skills act as a foundation, accelerating learning in new environments. This hypothesis aims to bridge the gap between pre-training and hierarchical RL for creating adaptable, self-improving agents.', 'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'elo_score': 1245.1168721225129, 'review_comments': ["The hypothesis is grounded in existing research trends but the specific combination of task-agnostic pre-training for diverse skill discovery with hierarchical RL for faster adaptation presents a potentially novel and valuable approach. The novelty lies in the specific *type* of pre-training and how it's integrated within the hierarchical RL framework. Feasibility depends on the complexity of the pre-training data and the scalability of the hierarchical RL algorithm. Success hinges on effectively discovering diverse and reusable skills during pre-training and designing a hierarchical RL architecture that can leverage these skills efficiently. Defining 'faster adaptation' and 'improved sample efficiency' with specific metrics is crucial for evaluating the hypothesis."], 'references': [{'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}, {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}, {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}, {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}, {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}, {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}, {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}, {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}], 'is_active': True, 'parent_ids': []}, {'id': 'G7295', 'title': 'Hierarchical Intrinsic Motivation with Predictive Models for Skill Discovery', 'text': 'Hypothesis: A hierarchical reinforcement learning agent, driven by intrinsic motivation based on prediction error reduction at multiple levels of abstraction, can autonomously discover and learn a useful repertoire of skills. Rationale: Existing intrinsic motivation often focuses on immediate prediction error. This hypothesis proposes a hierarchical approach where higher levels predict lower-level actions and outcomes, driving the agent to learn skills that improve its ability to model its own behavior and the environment at different granularities. This differs from existing hierarchical RL by explicitly linking the hierarchy to intrinsic motivation and learning predictive models for skill abstraction, leading to more robust and adaptable skill discovery.', 'novelty_review': 'MEDIUM', 'feasibility_review': 'MEDIUM', 'elo_score': 1217.8530281990547, 'review_comments': ['The hypothesis builds upon existing work in hierarchical reinforcement learning and intrinsic motivation, but the explicit connection between hierarchical levels, prediction error reduction at *multiple* levels, and autonomous skill discovery makes it reasonably novel. The feasibility is moderate because implementing and training such a system, especially with multiple levels of abstraction, can be computationally expensive and require careful design of the reward functions and state representations. The success hinges on the specific implementation details and the ability to effectively learn predictive models at each level of the hierarchy.', "The hypothesis has merit. While hierarchical reinforcement learning and intrinsic motivation are established fields, the specific combination of hierarchical prediction error reduction across multiple levels of abstraction for autonomous skill discovery is not entirely novel, but has potential for significant advancement. The success hinges on the effective design of the hierarchical architecture, the intrinsic reward function, and the scalability of the learning algorithm. The claim of 'more robust and adaptable skill discovery' needs strong empirical validation. The connection between prediction error and skill abstraction is a key element that needs to be well-defined and implemented."], 'references': [{'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}, {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}, {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}, {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1604.00077', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204503', 'title': 'Intrinsic Motivation for Hierarchical Reinforcement Learning'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2019'}, {'type': 'arXiv', 'id': '1511.09292', 'title': 'Learning to Predict the Internal State of Agents'}, {'type': 'Paper', 'title': 'Unsupervised Meta-Learning for Few-Shot Image Classification', 'venue': 'ICLR 2017'}, {'type': 'Paper', 'title': 'Intrinsic Motivation via Prediction Gain', 'venue': 'International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011'}], 'is_active': True, 'parent_ids': []}], 'suggested_next_steps': ['Refine top hypotheses based on review comments.', 'Consider exploring areas with fewer, less connected hypotheses (if any).', 'Seek external expert feedback on top candidates.']}}
2025-05-31 19:24:24,168 INFO aicoscientist: --- Cycle 2 Complete ---
2025-05-31 19:24:24,168 INFO aicoscientist: Supervisor run_cycle completed for iteration 2.
2025-05-31 19:24:24,168 INFO aicoscientist: --- Endpoint /run_cycle END (Success) ---
2025-05-31 19:24:24,175 INFO aicoscientist: [FRONTEND-INFO] Starting references section update | Client Time: 2025-06-01T02:24:24.174Z
2025-05-31 19:24:24,178 INFO aicoscientist: [FRONTEND-INFO] Extracting references from hypotheses | Data: {'researchGoal': 'make self-improving ai systems', 'hasSteps': True} | Client Time: 2025-06-01T02:24:24.177Z
2025-05-31 19:24:24,182 INFO aicoscientist: [FRONTEND-INFO] References extraction complete | Data: {'totalReferences': 478, 'references': [{'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}, {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}, {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}, {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}, {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}, {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}, {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}, {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}, {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}, {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}, {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}, {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}, {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}, {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}, {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}, {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}, {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}, {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}, {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}, {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}, {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}, {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}, {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}, {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}, {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}, {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}, {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}, {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}, {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}, {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}, {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}, {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}, {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}, {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}, {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}, {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}, {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}, {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}, {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Active Learning for Reinforcement Learning', 'venue': 'Machine Learning 2005'}, {'arXiv_ID': '1802.06866', 'title': 'Exploration with Bayesian Neural Networks for Reinforcement Learning'}, {'arXiv_ID': '1901.08753', 'title': 'Deep Exploration via Bootstrapped DQN'}, {'title': 'Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Variational Bayesian Reinforcement Learning', 'venue': 'AISTATS 2018'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Deep Exploration via Bootstrapped DQN', 'venue': 'NeurIPS 2016'}, {'title': 'Variational Information Bottleneck for Reinforcement Learning', 'venue': 'ICLR 2019'}, {'title': 'Active Model Learning', 'venue': 'arXiv:1802.08941'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'arXiv:1606.04670'}, {'title': 'Active Learning for Deep Reinforcement Learning', 'venue': 'arXiv:1804.08474'}, {'title': 'Model-Based Active Exploration', 'venue': 'arXiv:1703.07604'}, 'arXiv:1801.10127 (Elastic Weight Consolidation)', 'arXiv:1606.07141 (Learning without Forgetting)', 'arXiv:1602.04902 (Distilling the Knowledge in a Neural Network)', 'arXiv:1705.07129 (PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning)', 'arXiv:1812.00424 (Synaptic Intelligence: Continual Learning with Synaptic Intelligence)', 'arXiv:2002.00810 (Gradient Episodic Memory for Continual Learning)', 'arXiv:2007.00894 (Continual Learning via Neural Pruning)', 'arXiv:2106.04497 (Model Merging via Uniform Weight Averaging)', 'arXiv:2202.02012 (Task-Agnostic Continual Learning via Meta-Learning)', 'Tevfik Koşar, Mustafa Sert, and Pinar Duygulu. 2023. Model Merging: A Survey. ACM Comput. Surv. 55, 11, Article 223 (December 2023), 33 pages. DOI: https://doi.org/10.1145/3574243', {'title': 'Model Soups: Averaging Weights Leads to Wider Optima and Better Generalization', 'venue': 'NeurIPS 2022'}, {'title': 'Knowledge Distillation', 'venue': 'NIPS 2015'}, {'arXiv_ID': '1812.00424', 'title': 'Elastic Weight Consolidation'}, {'arXiv_ID': '1606.07792', 'title': 'Learning without Forgetting'}, {'title': 'Continual Learning via Modular Networks with Overlapping Representation', 'venue': 'ICLR 2020'}, {'arXiv_ID': '2012.00822', 'title': 'Task-Agnostic Continual Learning via Meta-Learning'}, {'arXiv_ID': '2106.08943', 'title': 'MERLIN: Parameter-Efficient Transfer Learning for NLP'}, {'title': 'Distilling Knowledge from Ensembles of Neural Networks', 'venue': 'CVPR 2014'}, {'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}, {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}, {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}, {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}, {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}, {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}, {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}, {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}, {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}, {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}, {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}, {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}, {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}, {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}, {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}, {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}, {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}, {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}, {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}, {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}, {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}, {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}, {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}, {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}, {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}, {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}, {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}, {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}, {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}, {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}, {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}, {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}, {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}, {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}, {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}, {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}, {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}, {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}, {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}, {'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}, {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}, {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}, {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1604.00077', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204503', 'title': 'Intrinsic Motivation for Hierarchical Reinforcement Learning'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2019'}, {'type': 'arXiv', 'id': '1511.09292', 'title': 'Learning to Predict the Internal State of Agents'}, {'type': 'Paper', 'title': 'Unsupervised Meta-Learning for Few-Shot Image Classification', 'venue': 'ICLR 2017'}, {'type': 'Paper', 'title': 'Intrinsic Motivation via Prediction Gain', 'venue': 'International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems', 'venue': 'Foundations and Trends in Machine Learning, 2020'}, {'title': 'Communication-Efficient Distributed Deep Reinforcement Learning', 'arXiv_ID': '1802.01528'}, {'title': 'TarMAC: Targeted Multi-Agent Communication', 'venue': 'ICML 2018'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'AAMAS 2020'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning', 'venue': 'ICML 2018'}, {'title': 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'venue': 'NeurIPS 2017'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems: A Survey', 'venue': 'Journal of Artificial Intelligence Research 72 (2021) 1-93'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'ICML 2018'}, {'arXiv_ID': '1605.06673', 'comment': 'Foerster et al. - Learning to Communicate with Deep Multi-Agent Reinforcement Learning'}, {'arXiv_ID': '1802.08780', 'comment': 'Lazaridou et al. - Emergence of Linguistic Communication from Referential Games'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Exploration', 'venue': 'ICLR 2019'}, {'title': 'Model-Based Communication for Multi-Agent Cooperation', 'venue': 'AAMAS 2020'}, {'title': 'Neuro-Symbolic Reinforcement Learning: A Survey', 'venue': 'arXiv preprint arXiv:2010.06278'}, {'title': 'Learning Programs for Abstraction and Reasoning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'ICLR 2017'}, {'title': 'SRL: Symbolic Reinforcement Learning', 'venue': 'International Conference on Automated Planning and Scheduling (ICAPS) 2018'}, {'arXiv_ID': '2106.09689'}, {'DOI': '10.1109/ICRA46639.2022.9811713'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'NeurIPS 2017'}, {'title': 'Neuro-Symbolic Reinforcement Learning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Learning Symbolic Representations for Generalization in Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR) 2018'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'Association for Computational Linguistics (ACL) 2017'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'International Conference on Learning Representations (ICLR) 2017'}, {'title': 'Relational Reinforcement Learning', 'venue': 'Machine Learning, 57(1-3), 5-43, 2004'}, {'arXiv_ID': '2106.06275', 'title': 'Symbolic Planning with Neural Reward Functions'}, {'arXiv_ID': '2203.02071', 'title': 'Learning to Abstract and Reason with Symbolic Planning'}, {'title': 'STRIPS Learning through Reinforcement', 'venue': 'AAAI Conference on Artificial Intelligence (AAAI), 2023'}, {'type': 'arXiv', 'id': '1905.10976', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04140', 'title': 'Teacher-Student Curriculum Learning'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460635', 'title': 'Automatic Curriculum Learning for Deep RL: A Case Study on Robotic Manipulation', 'venue': 'ICRA 2018'}, {'type': 'Paper Title', 'title': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Curricula via Paired Open-Ended Trailblazing', 'venue': 'GECCO 2019'}, {'type': 'arXiv', 'id': '2010.08453', 'title': 'Self-Supervised Curriculum Learning'}, {'type': 'Paper Title', 'title': 'Curriculum Learning by Transferring Latent Task Structures', 'venue': 'NeurIPS 2020'}, {'type': 'arXiv', 'id': '1810.11903', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04170', 'title': 'Curriculum Learning by Prediction-Error Minimization'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204463', 'title': 'Autonomous Curriculum Generation for Reinforcement Learning'}, {'type': 'Paper Title', 'title': 'POET: End-to-End Parametric Optimization through Evolution', 'venue': 'Genetic and Evolutionary Computation Conference (GECCO) 2019'}, {'type': 'Paper Title', 'title': 'Teacher-Student Curriculum Learning', 'venue': 'Neural Information Processing Systems (NeurIPS) 2016'}, {'type': 'arXiv', 'id': '1802.09562', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'title': 'Bayesian Neural Networks', 'venue': 'Advances in Neural Information Processing Systems (NeurIPS)', 'year': 1991}, {'title': 'Active Learning Literature Survey', 'venue': 'ACM Computing Surveys', 'year': 2009}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML)', 'year': 2017}, {'title': 'Curriculum Learning', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'year': 2009}, {'title': 'Learning to Explore via Meta-Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR)', 'year': 2017}, {'title': 'Efficient Exploration through Bayesian Deep Reinforcement Learning', 'venue': 'Conference on Robot Learning (CoRL)', 'year': 2018}, {'title': 'Meta-Curriculum Learning for Domain Adaptation', 'venue': 'International Conference on Computer Vision (ICCV)', 'year': 2021}, {'arXiv_ID': '1802.04849', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'arXiv_ID': '2006.05961', 'title': 'Uncertainty-Aware Reinforcement Learning for Safe Exploration'}, {'arXiv_ID': '2106.02762', 'title': 'Meta-Learning for Curriculum Design'}, {'title': 'Bayesian Neural Networks', 'venue': 'Review Article, e.g., Journal of Machine Learning Research'}, {'title': 'Active Learning for Regression', 'venue': 'Journal of Machine Learning Research'}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'ICML 2017'}, {'title': 'Meta-Curriculum Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Exploration by Random Network Distillation', 'venue': 'ICLR 2019'}, {'arXiv_ID': '1802.09471', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'title': 'Uncertainty-Aware Deep Reinforcement Learning', 'venue': 'AAAI 2018'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'AISTATS 2016'}, {'title': 'Curriculum Learning', 'venue': 'A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems'}, {'title': 'Self-Paced Learning for Latent Variable Models', 'venue': 'NIPS 2010'}, {'title': 'Bayesian Neural Networks', 'venue': 'Advances in Neural Information Processing Systems (NeurIPS)', 'year': 1991}, {'title': 'Active Learning Literature Survey', 'venue': 'ACM Computing Surveys', 'year': 2009}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML)', 'year': 2017}, {'title': 'Curriculum Learning', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'year': 2009}, {'title': 'Learning to Explore via Meta-Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR)', 'year': 2017}, {'title': 'Efficient Exploration through Bayesian Deep Reinforcement Learning', 'venue': 'Conference on Robot Learning (CoRL)', 'year': 2018}, {'title': 'Meta-Curriculum Learning for Domain Adaptation', 'venue': 'International Conference on Computer Vision (ICCV)', 'year': 2021}, {'arXiv_ID': '1802.04849', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'arXiv_ID': '2006.05961', 'title': 'Uncertainty-Aware Reinforcement Learning for Safe Exploration'}, {'arXiv_ID': '2106.02762', 'title': 'Meta-Learning for Curriculum Design'}, {'title': 'Bayesian Neural Networks', 'venue': 'Review Article, e.g., Journal of Machine Learning Research'}, {'title': 'Active Learning for Regression', 'venue': 'Journal of Machine Learning Research'}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'ICML 2017'}, {'title': 'Meta-Curriculum Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Exploration by Random Network Distillation', 'venue': 'ICLR 2019'}, {'arXiv_ID': '1802.09471', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'title': 'Uncertainty-Aware Deep Reinforcement Learning', 'venue': 'AAAI 2018'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'AISTATS 2016'}, {'title': 'Curriculum Learning', 'venue': 'A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems'}, {'title': 'Self-Paced Learning for Latent Variable Models', 'venue': 'NIPS 2010'}, {'title': 'Neuro-Symbolic Reinforcement Learning: A Survey', 'venue': 'arXiv preprint arXiv:2010.06278'}, {'title': 'Learning Programs for Abstraction and Reasoning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'ICLR 2017'}, {'title': 'SRL: Symbolic Reinforcement Learning', 'venue': 'International Conference on Automated Planning and Scheduling (ICAPS) 2018'}, {'arXiv_ID': '2106.09689'}, {'DOI': '10.1109/ICRA46639.2022.9811713'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'NeurIPS 2017'}, {'title': 'Neuro-Symbolic Reinforcement Learning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Learning Symbolic Representations for Generalization in Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR) 2018'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'Association for Computational Linguistics (ACL) 2017'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'International Conference on Learning Representations (ICLR) 2017'}, {'title': 'Relational Reinforcement Learning', 'venue': 'Machine Learning, 57(1-3), 5-43, 2004'}, {'arXiv_ID': '2106.06275', 'title': 'Symbolic Planning with Neural Reward Functions'}, {'arXiv_ID': '2203.02071', 'title': 'Learning to Abstract and Reason with Symbolic Planning'}, {'title': 'STRIPS Learning through Reinforcement', 'venue': 'AAAI Conference on Artificial Intelligence (AAAI), 2023'}, {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}, {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}, {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}, {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}, {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}, {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}, {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}, {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Active Learning for Reinforcement Learning', 'venue': 'Machine Learning 2005'}, {'arXiv_ID': '1802.06866', 'title': 'Exploration with Bayesian Neural Networks for Reinforcement Learning'}, {'arXiv_ID': '1901.08753', 'title': 'Deep Exploration via Bootstrapped DQN'}, {'title': 'Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Variational Bayesian Reinforcement Learning', 'venue': 'AISTATS 2018'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Deep Exploration via Bootstrapped DQN', 'venue': 'NeurIPS 2016'}, {'title': 'Variational Information Bottleneck for Reinforcement Learning', 'venue': 'ICLR 2019'}, {'title': 'Active Model Learning', 'venue': 'arXiv:1802.08941'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'arXiv:1606.04670'}, {'title': 'Active Learning for Deep Reinforcement Learning', 'venue': 'arXiv:1804.08474'}, {'title': 'Model-Based Active Exploration', 'venue': 'arXiv:1703.07604'}, {'title': 'Model Soups: Averaging Weights Leads to Wider Optima and Better Generalization', 'venue': 'NeurIPS 2022'}, {'title': 'Knowledge Distillation', 'venue': 'NIPS 2015'}, {'arXiv_ID': '1812.00424', 'title': 'Elastic Weight Consolidation'}, {'arXiv_ID': '1606.07792', 'title': 'Learning without Forgetting'}, {'title': 'Continual Learning via Modular Networks with Overlapping Representation', 'venue': 'ICLR 2020'}, {'arXiv_ID': '2012.00822', 'title': 'Task-Agnostic Continual Learning via Meta-Learning'}, {'arXiv_ID': '2106.08943', 'title': 'MERLIN: Parameter-Efficient Transfer Learning for NLP'}, {'title': 'Distilling Knowledge from Ensembles of Neural Networks', 'venue': 'CVPR 2014'}, {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}, {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}, {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}, {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}, {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}, {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}, {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}, {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}, {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}, {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}, {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}, {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}, {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}, {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}, {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}, {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}, {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}, {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}, {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}, {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1905.10976', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04140', 'title': 'Teacher-Student Curriculum Learning'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460635', 'title': 'Automatic Curriculum Learning for Deep RL: A Case Study on Robotic Manipulation', 'venue': 'ICRA 2018'}, {'type': 'Paper Title', 'title': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Curricula via Paired Open-Ended Trailblazing', 'venue': 'GECCO 2019'}, {'type': 'arXiv', 'id': '2010.08453', 'title': 'Self-Supervised Curriculum Learning'}, {'type': 'Paper Title', 'title': 'Curriculum Learning by Transferring Latent Task Structures', 'venue': 'NeurIPS 2020'}, {'type': 'arXiv', 'id': '1810.11903', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04170', 'title': 'Curriculum Learning by Prediction-Error Minimization'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204463', 'title': 'Autonomous Curriculum Generation for Reinforcement Learning'}, {'type': 'Paper Title', 'title': 'POET: End-to-End Parametric Optimization through Evolution', 'venue': 'Genetic and Evolutionary Computation Conference (GECCO) 2019'}, {'type': 'Paper Title', 'title': 'Teacher-Student Curriculum Learning', 'venue': 'Neural Information Processing Systems (NeurIPS) 2016'}, {'type': 'arXiv', 'id': '1802.09562', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}, {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}, {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}, {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1604.00077', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204503', 'title': 'Intrinsic Motivation for Hierarchical Reinforcement Learning'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2019'}, {'type': 'arXiv', 'id': '1511.09292', 'title': 'Learning to Predict the Internal State of Agents'}, {'type': 'Paper', 'title': 'Unsupervised Meta-Learning for Few-Shot Image Classification', 'venue': 'ICLR 2017'}, {'type': 'Paper', 'title': 'Intrinsic Motivation via Prediction Gain', 'venue': 'International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011'}, {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}, {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}, {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}, {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}, {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}, {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}, {'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}, {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}, {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}, {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}, {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}, {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems', 'venue': 'Foundations and Trends in Machine Learning, 2020'}, {'title': 'Communication-Efficient Distributed Deep Reinforcement Learning', 'arXiv_ID': '1802.01528'}, {'title': 'TarMAC: Targeted Multi-Agent Communication', 'venue': 'ICML 2018'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'AAMAS 2020'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning', 'venue': 'ICML 2018'}, {'title': 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'venue': 'NeurIPS 2017'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems: A Survey', 'venue': 'Journal of Artificial Intelligence Research 72 (2021) 1-93'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'ICML 2018'}, {'arXiv_ID': '1605.06673', 'comment': 'Foerster et al. - Learning to Communicate with Deep Multi-Agent Reinforcement Learning'}, {'arXiv_ID': '1802.08780', 'comment': 'Lazaridou et al. - Emergence of Linguistic Communication from Referential Games'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Exploration', 'venue': 'ICLR 2019'}, {'title': 'Model-Based Communication for Multi-Agent Cooperation', 'venue': 'AAMAS 2020'}, 'arXiv:1703.04977 - Variational Bayesian Neural Networks', 'arXiv:1802.06057 - Neural Architecture Search with Reinforcement Learning', 'arXiv:1807.05739 - Meta-Learning for Low-Resource Natural Language Processing', 'arXiv:1901.08195 - Exploration by Random Network Distillation', 'arXiv:1905.10982 - Learning to Explore via Meta-Reinforcement Learning', 'arXiv:2006.05968 - Uncertainty-aware Reinforcement Learning for Safe Exploration', 'arXiv:2010.09484 - Symbolic Planning with Neural Reward Functions for Generalizable Robot Manipulation', 'arXiv:2103.01909 - Meta-Curriculum Learning', 'arXiv:2106.09686 - Active Exploration via Differentiable Information Gain', 'arXiv:2203.07657 - Combining Symbolic and Subsymbolic AI for Robust Generalization', '"Learning Symbolic Rules for Reinforcement Learning" - ICML 2019', '"Meta-Learning for Curriculum Design" - NeurIPS 2019', {'title': 'Bayesian Neural Networks', 'venue': 'Advances in Neural Information Processing Systems (NeurIPS)', 'year': 1991}, {'title': 'Active Learning Literature Survey', 'venue': 'ACM Computing Surveys', 'year': 2009}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML)', 'year': 2017}, {'title': 'Curriculum Learning', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'year': 2009}, {'title': 'Learning to Explore via Meta-Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR)', 'year': 2017}, {'title': 'Efficient Exploration through Bayesian Deep Reinforcement Learning', 'venue': 'Conference on Robot Learning (CoRL)', 'year': 2018}, {'title': 'Meta-Curriculum Learning for Domain Adaptation', 'venue': 'International Conference on Computer Vision (ICCV)', 'year': 2021}, {'arXiv_ID': '1802.04849', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'arXiv_ID': '2006.05961', 'title': 'Uncertainty-Aware Reinforcement Learning for Safe Exploration'}, {'arXiv_ID': '2106.02762', 'title': 'Meta-Learning for Curriculum Design'}, {'title': 'Bayesian Neural Networks', 'venue': 'Review Article, e.g., Journal of Machine Learning Research'}, {'title': 'Active Learning for Regression', 'venue': 'Journal of Machine Learning Research'}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'ICML 2017'}, {'title': 'Meta-Curriculum Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Exploration by Random Network Distillation', 'venue': 'ICLR 2019'}, {'arXiv_ID': '1802.09471', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'title': 'Uncertainty-Aware Deep Reinforcement Learning', 'venue': 'AAAI 2018'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'AISTATS 2016'}, {'title': 'Curriculum Learning', 'venue': 'A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems'}, {'title': 'Self-Paced Learning for Latent Variable Models', 'venue': 'NIPS 2010'}, {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}, {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}, {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}, {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}, {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}, {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}, {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}, {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}, {'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}, {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}, {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}, {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1604.00077', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204503', 'title': 'Intrinsic Motivation for Hierarchical Reinforcement Learning'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2019'}, {'type': 'arXiv', 'id': '1511.09292', 'title': 'Learning to Predict the Internal State of Agents'}, {'type': 'Paper', 'title': 'Unsupervised Meta-Learning for Few-Shot Image Classification', 'venue': 'ICLR 2017'}, {'type': 'Paper', 'title': 'Intrinsic Motivation via Prediction Gain', 'venue': 'International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011'}, {'title': 'Neuro-Symbolic Reinforcement Learning: A Survey', 'venue': 'arXiv preprint arXiv:2010.06278'}, {'title': 'Learning Programs for Abstraction and Reasoning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'ICLR 2017'}, {'title': 'SRL: Symbolic Reinforcement Learning', 'venue': 'International Conference on Automated Planning and Scheduling (ICAPS) 2018'}, {'arXiv_ID': '2106.09689'}, {'DOI': '10.1109/ICRA46639.2022.9811713'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'NeurIPS 2017'}, {'title': 'Neuro-Symbolic Reinforcement Learning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Learning Symbolic Representations for Generalization in Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR) 2018'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'Association for Computational Linguistics (ACL) 2017'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'International Conference on Learning Representations (ICLR) 2017'}, {'title': 'Relational Reinforcement Learning', 'venue': 'Machine Learning, 57(1-3), 5-43, 2004'}, {'arXiv_ID': '2106.06275', 'title': 'Symbolic Planning with Neural Reward Functions'}, {'arXiv_ID': '2203.02071', 'title': 'Learning to Abstract and Reason with Symbolic Planning'}, {'title': 'STRIPS Learning through Reinforcement', 'venue': 'AAAI Conference on Artificial Intelligence (AAAI), 2023'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Active Learning for Reinforcement Learning', 'venue': 'Machine Learning 2005'}, {'arXiv_ID': '1802.06866', 'title': 'Exploration with Bayesian Neural Networks for Reinforcement Learning'}, {'arXiv_ID': '1901.08753', 'title': 'Deep Exploration via Bootstrapped DQN'}, {'title': 'Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Variational Bayesian Reinforcement Learning', 'venue': 'AISTATS 2018'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Deep Exploration via Bootstrapped DQN', 'venue': 'NeurIPS 2016'}, {'title': 'Variational Information Bottleneck for Reinforcement Learning', 'venue': 'ICLR 2019'}, {'title': 'Active Model Learning', 'venue': 'arXiv:1802.08941'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'arXiv:1606.04670'}, {'title': 'Active Learning for Deep Reinforcement Learning', 'venue': 'arXiv:1804.08474'}, {'title': 'Model-Based Active Exploration', 'venue': 'arXiv:1703.07604'}, {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}, {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}, {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}, {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}, {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}, {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}, {'title': 'Model Soups: Averaging Weights Leads to Wider Optima and Better Generalization', 'venue': 'NeurIPS 2022'}, {'title': 'Knowledge Distillation', 'venue': 'NIPS 2015'}, {'arXiv_ID': '1812.00424', 'title': 'Elastic Weight Consolidation'}, {'arXiv_ID': '1606.07792', 'title': 'Learning without Forgetting'}, {'title': 'Continual Learning via Modular Networks with Overlapping Representation', 'venue': 'ICLR 2020'}, {'arXiv_ID': '2012.00822', 'title': 'Task-Agnostic Continual Learning via Meta-Learning'}, {'arXiv_ID': '2106.08943', 'title': 'MERLIN: Parameter-Efficient Transfer Learning for NLP'}, {'title': 'Distilling Knowledge from Ensembles of Neural Networks', 'venue': 'CVPR 2014'}, {'type': 'arXiv', 'id': '1905.10976', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04140', 'title': 'Teacher-Student Curriculum Learning'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460635', 'title': 'Automatic Curriculum Learning for Deep RL: A Case Study on Robotic Manipulation', 'venue': 'ICRA 2018'}, {'type': 'Paper Title', 'title': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Curricula via Paired Open-Ended Trailblazing', 'venue': 'GECCO 2019'}, {'type': 'arXiv', 'id': '2010.08453', 'title': 'Self-Supervised Curriculum Learning'}, {'type': 'Paper Title', 'title': 'Curriculum Learning by Transferring Latent Task Structures', 'venue': 'NeurIPS 2020'}, {'type': 'arXiv', 'id': '1810.11903', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04170', 'title': 'Curriculum Learning by Prediction-Error Minimization'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204463', 'title': 'Autonomous Curriculum Generation for Reinforcement Learning'}, {'type': 'Paper Title', 'title': 'POET: End-to-End Parametric Optimization through Evolution', 'venue': 'Genetic and Evolutionary Computation Conference (GECCO) 2019'}, {'type': 'Paper Title', 'title': 'Teacher-Student Curriculum Learning', 'venue': 'Neural Information Processing Systems (NeurIPS) 2016'}, {'type': 'arXiv', 'id': '1802.09562', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}, {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}, {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}, {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}, {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}, {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}, {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}, {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}, {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}, {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}, {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}, {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}, {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}, {'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}, {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}, {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}, {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}, {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}, {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems', 'venue': 'Foundations and Trends in Machine Learning, 2020'}, {'title': 'Communication-Efficient Distributed Deep Reinforcement Learning', 'arXiv_ID': '1802.01528'}, {'title': 'TarMAC: Targeted Multi-Agent Communication', 'venue': 'ICML 2018'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'AAMAS 2020'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning', 'venue': 'ICML 2018'}, {'title': 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'venue': 'NeurIPS 2017'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems: A Survey', 'venue': 'Journal of Artificial Intelligence Research 72 (2021) 1-93'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'ICML 2018'}, {'arXiv_ID': '1605.06673', 'comment': 'Foerster et al. - Learning to Communicate with Deep Multi-Agent Reinforcement Learning'}, {'arXiv_ID': '1802.08780', 'comment': 'Lazaridou et al. - Emergence of Linguistic Communication from Referential Games'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Exploration', 'venue': 'ICLR 2019'}, {'title': 'Model-Based Communication for Multi-Agent Cooperation', 'venue': 'AAMAS 2020'}, {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}, {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}, {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}, {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}, {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}, {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}, {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}]} | Client Time: 2025-06-01T02:24:24.180Z
2025-05-31 19:24:24,186 INFO aicoscientist: [FRONTEND-INFO] Starting arXiv search | Data: {'query': 'make self-improving ai systems'} | Client Time: 2025-06-01T02:24:24.185Z
2025-05-31 19:24:30,835 INFO aicoscientist: ArXiv search for 'make self-improving ai systems' returned 0 papers in 6646.27ms
2025-05-31 19:24:30,843 INFO aicoscientist: [FRONTEND-INFO] arXiv search response received | Data: {'status': 200, 'ok': True} | Client Time: 2025-06-01T02:24:30.839Z
2025-05-31 19:24:30,854 INFO aicoscientist: [FRONTEND-INFO] arXiv papers found | Data: {'count': 0, 'paperTitles': []} | Client Time: 2025-06-01T02:24:30.851Z
2025-05-31 19:24:30,860 INFO aicoscientist: [FRONTEND-INFO] Calling displayReferences function | Client Time: 2025-06-01T02:24:30.857Z
2025-05-31 19:24:30,865 INFO aicoscientist: [FRONTEND-INFO] Starting displayReferences function | Data: {'arxivPapersCount': 0, 'additionalReferencesCount': 478} | Client Time: 2025-06-01T02:24:30.863Z
2025-05-31 19:24:30,871 INFO aicoscientist: [FRONTEND-INFO] Processing additional references | Data: {'count': 478, 'references': [{'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}, {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}, {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}, {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}, {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}, {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}, {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}, {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}, {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}, {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}, {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}, {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}, {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}, {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}, {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}, {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}, {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}, {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}, {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}, {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}, {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}, {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}, {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}, {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}, {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}, {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}, {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}, {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}, {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}, {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}, {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}, {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}, {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}, {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}, {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}, {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}, {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}, {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}, {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Active Learning for Reinforcement Learning', 'venue': 'Machine Learning 2005'}, {'arXiv_ID': '1802.06866', 'title': 'Exploration with Bayesian Neural Networks for Reinforcement Learning'}, {'arXiv_ID': '1901.08753', 'title': 'Deep Exploration via Bootstrapped DQN'}, {'title': 'Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Variational Bayesian Reinforcement Learning', 'venue': 'AISTATS 2018'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Deep Exploration via Bootstrapped DQN', 'venue': 'NeurIPS 2016'}, {'title': 'Variational Information Bottleneck for Reinforcement Learning', 'venue': 'ICLR 2019'}, {'title': 'Active Model Learning', 'venue': 'arXiv:1802.08941'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'arXiv:1606.04670'}, {'title': 'Active Learning for Deep Reinforcement Learning', 'venue': 'arXiv:1804.08474'}, {'title': 'Model-Based Active Exploration', 'venue': 'arXiv:1703.07604'}, 'arXiv:1801.10127 (Elastic Weight Consolidation)', 'arXiv:1606.07141 (Learning without Forgetting)', 'arXiv:1602.04902 (Distilling the Knowledge in a Neural Network)', 'arXiv:1705.07129 (PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning)', 'arXiv:1812.00424 (Synaptic Intelligence: Continual Learning with Synaptic Intelligence)', 'arXiv:2002.00810 (Gradient Episodic Memory for Continual Learning)', 'arXiv:2007.00894 (Continual Learning via Neural Pruning)', 'arXiv:2106.04497 (Model Merging via Uniform Weight Averaging)', 'arXiv:2202.02012 (Task-Agnostic Continual Learning via Meta-Learning)', 'Tevfik Koşar, Mustafa Sert, and Pinar Duygulu. 2023. Model Merging: A Survey. ACM Comput. Surv. 55, 11, Article 223 (December 2023), 33 pages. DOI: https://doi.org/10.1145/3574243', {'title': 'Model Soups: Averaging Weights Leads to Wider Optima and Better Generalization', 'venue': 'NeurIPS 2022'}, {'title': 'Knowledge Distillation', 'venue': 'NIPS 2015'}, {'arXiv_ID': '1812.00424', 'title': 'Elastic Weight Consolidation'}, {'arXiv_ID': '1606.07792', 'title': 'Learning without Forgetting'}, {'title': 'Continual Learning via Modular Networks with Overlapping Representation', 'venue': 'ICLR 2020'}, {'arXiv_ID': '2012.00822', 'title': 'Task-Agnostic Continual Learning via Meta-Learning'}, {'arXiv_ID': '2106.08943', 'title': 'MERLIN: Parameter-Efficient Transfer Learning for NLP'}, {'title': 'Distilling Knowledge from Ensembles of Neural Networks', 'venue': 'CVPR 2014'}, {'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}, {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}, {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}, {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}, {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}, {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}, {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}, {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}, {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}, {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}, {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}, {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}, {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}, {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}, {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}, {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}, {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}, {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}, {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}, {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}, {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}, {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}, {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}, {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}, {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}, {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}, {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}, {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}, {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}, {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}, {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}, {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}, {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}, {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}, {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}, {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}, {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}, {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}, {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}, {'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}, {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}, {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}, {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1604.00077', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204503', 'title': 'Intrinsic Motivation for Hierarchical Reinforcement Learning'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2019'}, {'type': 'arXiv', 'id': '1511.09292', 'title': 'Learning to Predict the Internal State of Agents'}, {'type': 'Paper', 'title': 'Unsupervised Meta-Learning for Few-Shot Image Classification', 'venue': 'ICLR 2017'}, {'type': 'Paper', 'title': 'Intrinsic Motivation via Prediction Gain', 'venue': 'International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems', 'venue': 'Foundations and Trends in Machine Learning, 2020'}, {'title': 'Communication-Efficient Distributed Deep Reinforcement Learning', 'arXiv_ID': '1802.01528'}, {'title': 'TarMAC: Targeted Multi-Agent Communication', 'venue': 'ICML 2018'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'AAMAS 2020'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning', 'venue': 'ICML 2018'}, {'title': 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'venue': 'NeurIPS 2017'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems: A Survey', 'venue': 'Journal of Artificial Intelligence Research 72 (2021) 1-93'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'ICML 2018'}, {'arXiv_ID': '1605.06673', 'comment': 'Foerster et al. - Learning to Communicate with Deep Multi-Agent Reinforcement Learning'}, {'arXiv_ID': '1802.08780', 'comment': 'Lazaridou et al. - Emergence of Linguistic Communication from Referential Games'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Exploration', 'venue': 'ICLR 2019'}, {'title': 'Model-Based Communication for Multi-Agent Cooperation', 'venue': 'AAMAS 2020'}, {'title': 'Neuro-Symbolic Reinforcement Learning: A Survey', 'venue': 'arXiv preprint arXiv:2010.06278'}, {'title': 'Learning Programs for Abstraction and Reasoning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'ICLR 2017'}, {'title': 'SRL: Symbolic Reinforcement Learning', 'venue': 'International Conference on Automated Planning and Scheduling (ICAPS) 2018'}, {'arXiv_ID': '2106.09689'}, {'DOI': '10.1109/ICRA46639.2022.9811713'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'NeurIPS 2017'}, {'title': 'Neuro-Symbolic Reinforcement Learning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Learning Symbolic Representations for Generalization in Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR) 2018'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'Association for Computational Linguistics (ACL) 2017'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'International Conference on Learning Representations (ICLR) 2017'}, {'title': 'Relational Reinforcement Learning', 'venue': 'Machine Learning, 57(1-3), 5-43, 2004'}, {'arXiv_ID': '2106.06275', 'title': 'Symbolic Planning with Neural Reward Functions'}, {'arXiv_ID': '2203.02071', 'title': 'Learning to Abstract and Reason with Symbolic Planning'}, {'title': 'STRIPS Learning through Reinforcement', 'venue': 'AAAI Conference on Artificial Intelligence (AAAI), 2023'}, {'type': 'arXiv', 'id': '1905.10976', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04140', 'title': 'Teacher-Student Curriculum Learning'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460635', 'title': 'Automatic Curriculum Learning for Deep RL: A Case Study on Robotic Manipulation', 'venue': 'ICRA 2018'}, {'type': 'Paper Title', 'title': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Curricula via Paired Open-Ended Trailblazing', 'venue': 'GECCO 2019'}, {'type': 'arXiv', 'id': '2010.08453', 'title': 'Self-Supervised Curriculum Learning'}, {'type': 'Paper Title', 'title': 'Curriculum Learning by Transferring Latent Task Structures', 'venue': 'NeurIPS 2020'}, {'type': 'arXiv', 'id': '1810.11903', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04170', 'title': 'Curriculum Learning by Prediction-Error Minimization'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204463', 'title': 'Autonomous Curriculum Generation for Reinforcement Learning'}, {'type': 'Paper Title', 'title': 'POET: End-to-End Parametric Optimization through Evolution', 'venue': 'Genetic and Evolutionary Computation Conference (GECCO) 2019'}, {'type': 'Paper Title', 'title': 'Teacher-Student Curriculum Learning', 'venue': 'Neural Information Processing Systems (NeurIPS) 2016'}, {'type': 'arXiv', 'id': '1802.09562', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'title': 'Bayesian Neural Networks', 'venue': 'Advances in Neural Information Processing Systems (NeurIPS)', 'year': 1991}, {'title': 'Active Learning Literature Survey', 'venue': 'ACM Computing Surveys', 'year': 2009}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML)', 'year': 2017}, {'title': 'Curriculum Learning', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'year': 2009}, {'title': 'Learning to Explore via Meta-Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR)', 'year': 2017}, {'title': 'Efficient Exploration through Bayesian Deep Reinforcement Learning', 'venue': 'Conference on Robot Learning (CoRL)', 'year': 2018}, {'title': 'Meta-Curriculum Learning for Domain Adaptation', 'venue': 'International Conference on Computer Vision (ICCV)', 'year': 2021}, {'arXiv_ID': '1802.04849', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'arXiv_ID': '2006.05961', 'title': 'Uncertainty-Aware Reinforcement Learning for Safe Exploration'}, {'arXiv_ID': '2106.02762', 'title': 'Meta-Learning for Curriculum Design'}, {'title': 'Bayesian Neural Networks', 'venue': 'Review Article, e.g., Journal of Machine Learning Research'}, {'title': 'Active Learning for Regression', 'venue': 'Journal of Machine Learning Research'}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'ICML 2017'}, {'title': 'Meta-Curriculum Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Exploration by Random Network Distillation', 'venue': 'ICLR 2019'}, {'arXiv_ID': '1802.09471', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'title': 'Uncertainty-Aware Deep Reinforcement Learning', 'venue': 'AAAI 2018'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'AISTATS 2016'}, {'title': 'Curriculum Learning', 'venue': 'A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems'}, {'title': 'Self-Paced Learning for Latent Variable Models', 'venue': 'NIPS 2010'}, {'title': 'Bayesian Neural Networks', 'venue': 'Advances in Neural Information Processing Systems (NeurIPS)', 'year': 1991}, {'title': 'Active Learning Literature Survey', 'venue': 'ACM Computing Surveys', 'year': 2009}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML)', 'year': 2017}, {'title': 'Curriculum Learning', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'year': 2009}, {'title': 'Learning to Explore via Meta-Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR)', 'year': 2017}, {'title': 'Efficient Exploration through Bayesian Deep Reinforcement Learning', 'venue': 'Conference on Robot Learning (CoRL)', 'year': 2018}, {'title': 'Meta-Curriculum Learning for Domain Adaptation', 'venue': 'International Conference on Computer Vision (ICCV)', 'year': 2021}, {'arXiv_ID': '1802.04849', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'arXiv_ID': '2006.05961', 'title': 'Uncertainty-Aware Reinforcement Learning for Safe Exploration'}, {'arXiv_ID': '2106.02762', 'title': 'Meta-Learning for Curriculum Design'}, {'title': 'Bayesian Neural Networks', 'venue': 'Review Article, e.g., Journal of Machine Learning Research'}, {'title': 'Active Learning for Regression', 'venue': 'Journal of Machine Learning Research'}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'ICML 2017'}, {'title': 'Meta-Curriculum Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Exploration by Random Network Distillation', 'venue': 'ICLR 2019'}, {'arXiv_ID': '1802.09471', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'title': 'Uncertainty-Aware Deep Reinforcement Learning', 'venue': 'AAAI 2018'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'AISTATS 2016'}, {'title': 'Curriculum Learning', 'venue': 'A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems'}, {'title': 'Self-Paced Learning for Latent Variable Models', 'venue': 'NIPS 2010'}, {'title': 'Neuro-Symbolic Reinforcement Learning: A Survey', 'venue': 'arXiv preprint arXiv:2010.06278'}, {'title': 'Learning Programs for Abstraction and Reasoning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'ICLR 2017'}, {'title': 'SRL: Symbolic Reinforcement Learning', 'venue': 'International Conference on Automated Planning and Scheduling (ICAPS) 2018'}, {'arXiv_ID': '2106.09689'}, {'DOI': '10.1109/ICRA46639.2022.9811713'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'NeurIPS 2017'}, {'title': 'Neuro-Symbolic Reinforcement Learning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Learning Symbolic Representations for Generalization in Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR) 2018'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'Association for Computational Linguistics (ACL) 2017'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'International Conference on Learning Representations (ICLR) 2017'}, {'title': 'Relational Reinforcement Learning', 'venue': 'Machine Learning, 57(1-3), 5-43, 2004'}, {'arXiv_ID': '2106.06275', 'title': 'Symbolic Planning with Neural Reward Functions'}, {'arXiv_ID': '2203.02071', 'title': 'Learning to Abstract and Reason with Symbolic Planning'}, {'title': 'STRIPS Learning through Reinforcement', 'venue': 'AAAI Conference on Artificial Intelligence (AAAI), 2023'}, {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}, {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}, {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}, {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}, {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}, {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}, {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}, {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Active Learning for Reinforcement Learning', 'venue': 'Machine Learning 2005'}, {'arXiv_ID': '1802.06866', 'title': 'Exploration with Bayesian Neural Networks for Reinforcement Learning'}, {'arXiv_ID': '1901.08753', 'title': 'Deep Exploration via Bootstrapped DQN'}, {'title': 'Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Variational Bayesian Reinforcement Learning', 'venue': 'AISTATS 2018'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Deep Exploration via Bootstrapped DQN', 'venue': 'NeurIPS 2016'}, {'title': 'Variational Information Bottleneck for Reinforcement Learning', 'venue': 'ICLR 2019'}, {'title': 'Active Model Learning', 'venue': 'arXiv:1802.08941'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'arXiv:1606.04670'}, {'title': 'Active Learning for Deep Reinforcement Learning', 'venue': 'arXiv:1804.08474'}, {'title': 'Model-Based Active Exploration', 'venue': 'arXiv:1703.07604'}, {'title': 'Model Soups: Averaging Weights Leads to Wider Optima and Better Generalization', 'venue': 'NeurIPS 2022'}, {'title': 'Knowledge Distillation', 'venue': 'NIPS 2015'}, {'arXiv_ID': '1812.00424', 'title': 'Elastic Weight Consolidation'}, {'arXiv_ID': '1606.07792', 'title': 'Learning without Forgetting'}, {'title': 'Continual Learning via Modular Networks with Overlapping Representation', 'venue': 'ICLR 2020'}, {'arXiv_ID': '2012.00822', 'title': 'Task-Agnostic Continual Learning via Meta-Learning'}, {'arXiv_ID': '2106.08943', 'title': 'MERLIN: Parameter-Efficient Transfer Learning for NLP'}, {'title': 'Distilling Knowledge from Ensembles of Neural Networks', 'venue': 'CVPR 2014'}, {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}, {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}, {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}, {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}, {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}, {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}, {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}, {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}, {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}, {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}, {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}, {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}, {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}, {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}, {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}, {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}, {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}, {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}, {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}, {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1905.10976', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04140', 'title': 'Teacher-Student Curriculum Learning'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460635', 'title': 'Automatic Curriculum Learning for Deep RL: A Case Study on Robotic Manipulation', 'venue': 'ICRA 2018'}, {'type': 'Paper Title', 'title': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Curricula via Paired Open-Ended Trailblazing', 'venue': 'GECCO 2019'}, {'type': 'arXiv', 'id': '2010.08453', 'title': 'Self-Supervised Curriculum Learning'}, {'type': 'Paper Title', 'title': 'Curriculum Learning by Transferring Latent Task Structures', 'venue': 'NeurIPS 2020'}, {'type': 'arXiv', 'id': '1810.11903', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04170', 'title': 'Curriculum Learning by Prediction-Error Minimization'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204463', 'title': 'Autonomous Curriculum Generation for Reinforcement Learning'}, {'type': 'Paper Title', 'title': 'POET: End-to-End Parametric Optimization through Evolution', 'venue': 'Genetic and Evolutionary Computation Conference (GECCO) 2019'}, {'type': 'Paper Title', 'title': 'Teacher-Student Curriculum Learning', 'venue': 'Neural Information Processing Systems (NeurIPS) 2016'}, {'type': 'arXiv', 'id': '1802.09562', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}, {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}, {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}, {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1604.00077', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204503', 'title': 'Intrinsic Motivation for Hierarchical Reinforcement Learning'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2019'}, {'type': 'arXiv', 'id': '1511.09292', 'title': 'Learning to Predict the Internal State of Agents'}, {'type': 'Paper', 'title': 'Unsupervised Meta-Learning for Few-Shot Image Classification', 'venue': 'ICLR 2017'}, {'type': 'Paper', 'title': 'Intrinsic Motivation via Prediction Gain', 'venue': 'International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011'}, {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}, {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}, {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}, {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}, {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}, {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}, {'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}, {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}, {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}, {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}, {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}, {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems', 'venue': 'Foundations and Trends in Machine Learning, 2020'}, {'title': 'Communication-Efficient Distributed Deep Reinforcement Learning', 'arXiv_ID': '1802.01528'}, {'title': 'TarMAC: Targeted Multi-Agent Communication', 'venue': 'ICML 2018'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'AAMAS 2020'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning', 'venue': 'ICML 2018'}, {'title': 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'venue': 'NeurIPS 2017'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems: A Survey', 'venue': 'Journal of Artificial Intelligence Research 72 (2021) 1-93'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'ICML 2018'}, {'arXiv_ID': '1605.06673', 'comment': 'Foerster et al. - Learning to Communicate with Deep Multi-Agent Reinforcement Learning'}, {'arXiv_ID': '1802.08780', 'comment': 'Lazaridou et al. - Emergence of Linguistic Communication from Referential Games'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Exploration', 'venue': 'ICLR 2019'}, {'title': 'Model-Based Communication for Multi-Agent Cooperation', 'venue': 'AAMAS 2020'}, 'arXiv:1703.04977 - Variational Bayesian Neural Networks', 'arXiv:1802.06057 - Neural Architecture Search with Reinforcement Learning', 'arXiv:1807.05739 - Meta-Learning for Low-Resource Natural Language Processing', 'arXiv:1901.08195 - Exploration by Random Network Distillation', 'arXiv:1905.10982 - Learning to Explore via Meta-Reinforcement Learning', 'arXiv:2006.05968 - Uncertainty-aware Reinforcement Learning for Safe Exploration', 'arXiv:2010.09484 - Symbolic Planning with Neural Reward Functions for Generalizable Robot Manipulation', 'arXiv:2103.01909 - Meta-Curriculum Learning', 'arXiv:2106.09686 - Active Exploration via Differentiable Information Gain', 'arXiv:2203.07657 - Combining Symbolic and Subsymbolic AI for Robust Generalization', '"Learning Symbolic Rules for Reinforcement Learning" - ICML 2019', '"Meta-Learning for Curriculum Design" - NeurIPS 2019', {'title': 'Bayesian Neural Networks', 'venue': 'Advances in Neural Information Processing Systems (NeurIPS)', 'year': 1991}, {'title': 'Active Learning Literature Survey', 'venue': 'ACM Computing Surveys', 'year': 2009}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML)', 'year': 2017}, {'title': 'Curriculum Learning', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'year': 2009}, {'title': 'Learning to Explore via Meta-Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR)', 'year': 2017}, {'title': 'Efficient Exploration through Bayesian Deep Reinforcement Learning', 'venue': 'Conference on Robot Learning (CoRL)', 'year': 2018}, {'title': 'Meta-Curriculum Learning for Domain Adaptation', 'venue': 'International Conference on Computer Vision (ICCV)', 'year': 2021}, {'arXiv_ID': '1802.04849', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'arXiv_ID': '2006.05961', 'title': 'Uncertainty-Aware Reinforcement Learning for Safe Exploration'}, {'arXiv_ID': '2106.02762', 'title': 'Meta-Learning for Curriculum Design'}, {'title': 'Bayesian Neural Networks', 'venue': 'Review Article, e.g., Journal of Machine Learning Research'}, {'title': 'Active Learning for Regression', 'venue': 'Journal of Machine Learning Research'}, {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'ICML 2017'}, {'title': 'Meta-Curriculum Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Exploration by Random Network Distillation', 'venue': 'ICLR 2019'}, {'arXiv_ID': '1802.09471', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}, {'title': 'Uncertainty-Aware Deep Reinforcement Learning', 'venue': 'AAAI 2018'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'AISTATS 2016'}, {'title': 'Curriculum Learning', 'venue': 'A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems'}, {'title': 'Self-Paced Learning for Latent Variable Models', 'venue': 'NIPS 2010'}, {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}, {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}, {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}, {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}, {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}, {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}, {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}, {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}, {'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}, {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}, {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}, {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}, {'type': 'arXiv', 'id': '1604.00077', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204503', 'title': 'Intrinsic Motivation for Hierarchical Reinforcement Learning'}, {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}, {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2019'}, {'type': 'arXiv', 'id': '1511.09292', 'title': 'Learning to Predict the Internal State of Agents'}, {'type': 'Paper', 'title': 'Unsupervised Meta-Learning for Few-Shot Image Classification', 'venue': 'ICLR 2017'}, {'type': 'Paper', 'title': 'Intrinsic Motivation via Prediction Gain', 'venue': 'International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011'}, {'title': 'Neuro-Symbolic Reinforcement Learning: A Survey', 'venue': 'arXiv preprint arXiv:2010.06278'}, {'title': 'Learning Programs for Abstraction and Reasoning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'ICLR 2017'}, {'title': 'SRL: Symbolic Reinforcement Learning', 'venue': 'International Conference on Automated Planning and Scheduling (ICAPS) 2018'}, {'arXiv_ID': '2106.09689'}, {'DOI': '10.1109/ICRA46639.2022.9811713'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'NeurIPS 2017'}, {'title': 'Neuro-Symbolic Reinforcement Learning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}, {'title': 'Learning Symbolic Representations for Generalization in Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR) 2018'}, {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'Association for Computational Linguistics (ACL) 2017'}, {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'International Conference on Learning Representations (ICLR) 2017'}, {'title': 'Relational Reinforcement Learning', 'venue': 'Machine Learning, 57(1-3), 5-43, 2004'}, {'arXiv_ID': '2106.06275', 'title': 'Symbolic Planning with Neural Reward Functions'}, {'arXiv_ID': '2203.02071', 'title': 'Learning to Abstract and Reason with Symbolic Planning'}, {'title': 'STRIPS Learning through Reinforcement', 'venue': 'AAAI Conference on Artificial Intelligence (AAAI), 2023'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Active Learning for Reinforcement Learning', 'venue': 'Machine Learning 2005'}, {'arXiv_ID': '1802.06866', 'title': 'Exploration with Bayesian Neural Networks for Reinforcement Learning'}, {'arXiv_ID': '1901.08753', 'title': 'Deep Exploration via Bootstrapped DQN'}, {'title': 'Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning', 'venue': 'NeurIPS 2019'}, {'title': 'Variational Bayesian Reinforcement Learning', 'venue': 'AISTATS 2018'}, {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}, {'title': 'Deep Exploration via Bootstrapped DQN', 'venue': 'NeurIPS 2016'}, {'title': 'Variational Information Bottleneck for Reinforcement Learning', 'venue': 'ICLR 2019'}, {'title': 'Active Model Learning', 'venue': 'arXiv:1802.08941'}, {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'arXiv:1606.04670'}, {'title': 'Active Learning for Deep Reinforcement Learning', 'venue': 'arXiv:1804.08474'}, {'title': 'Model-Based Active Exploration', 'venue': 'arXiv:1703.07604'}, {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}, {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}, {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}, {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}, {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}, {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}, {'title': 'Model Soups: Averaging Weights Leads to Wider Optima and Better Generalization', 'venue': 'NeurIPS 2022'}, {'title': 'Knowledge Distillation', 'venue': 'NIPS 2015'}, {'arXiv_ID': '1812.00424', 'title': 'Elastic Weight Consolidation'}, {'arXiv_ID': '1606.07792', 'title': 'Learning without Forgetting'}, {'title': 'Continual Learning via Modular Networks with Overlapping Representation', 'venue': 'ICLR 2020'}, {'arXiv_ID': '2012.00822', 'title': 'Task-Agnostic Continual Learning via Meta-Learning'}, {'arXiv_ID': '2106.08943', 'title': 'MERLIN: Parameter-Efficient Transfer Learning for NLP'}, {'title': 'Distilling Knowledge from Ensembles of Neural Networks', 'venue': 'CVPR 2014'}, {'type': 'arXiv', 'id': '1905.10976', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04140', 'title': 'Teacher-Student Curriculum Learning'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460635', 'title': 'Automatic Curriculum Learning for Deep RL: A Case Study on Robotic Manipulation', 'venue': 'ICRA 2018'}, {'type': 'Paper Title', 'title': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Curricula via Paired Open-Ended Trailblazing', 'venue': 'GECCO 2019'}, {'type': 'arXiv', 'id': '2010.08453', 'title': 'Self-Supervised Curriculum Learning'}, {'type': 'Paper Title', 'title': 'Curriculum Learning by Transferring Latent Task Structures', 'venue': 'NeurIPS 2020'}, {'type': 'arXiv', 'id': '1810.11903', 'title': 'Meta-Curriculum Learning'}, {'type': 'arXiv', 'id': '1707.04170', 'title': 'Curriculum Learning by Prediction-Error Minimization'}, {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204463', 'title': 'Autonomous Curriculum Generation for Reinforcement Learning'}, {'type': 'Paper Title', 'title': 'POET: End-to-End Parametric Optimization through Evolution', 'venue': 'Genetic and Evolutionary Computation Conference (GECCO) 2019'}, {'type': 'Paper Title', 'title': 'Teacher-Student Curriculum Learning', 'venue': 'Neural Information Processing Systems (NeurIPS) 2016'}, {'type': 'arXiv', 'id': '1802.09562', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}, {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}, {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}, {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}, {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}, {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}, {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}, {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}, {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}, {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}, {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}, {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}, {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}, {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}, {'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}, {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}, {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}, {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}, {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}, {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}, {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems', 'venue': 'Foundations and Trends in Machine Learning, 2020'}, {'title': 'Communication-Efficient Distributed Deep Reinforcement Learning', 'arXiv_ID': '1802.01528'}, {'title': 'TarMAC: Targeted Multi-Agent Communication', 'venue': 'ICML 2018'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'AAMAS 2020'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning', 'venue': 'ICML 2018'}, {'title': 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'venue': 'NeurIPS 2017'}, {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}, {'title': 'Emergent Communication in Multi-Agent Systems: A Survey', 'venue': 'Journal of Artificial Intelligence Research 72 (2021) 1-93'}, {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'ICML 2018'}, {'arXiv_ID': '1605.06673', 'comment': 'Foerster et al. - Learning to Communicate with Deep Multi-Agent Reinforcement Learning'}, {'arXiv_ID': '1802.08780', 'comment': 'Lazaridou et al. - Emergence of Linguistic Communication from Referential Games'}, {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Exploration', 'venue': 'ICLR 2019'}, {'title': 'Model-Based Communication for Multi-Agent Cooperation', 'venue': 'AAMAS 2020'}, {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}, {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}, {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}, {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}, {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}, {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}, {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}]} | Client Time: 2025-06-01T02:24:30.867Z
2025-05-31 19:24:30,879 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 0 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.876Z
2025-05-31 19:24:30,891 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 1 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.876Z
2025-05-31 19:24:30,891 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 2 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}} | Client Time: 2025-06-01T02:24:30.876Z
2025-05-31 19:24:30,892 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 4 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}} | Client Time: 2025-06-01T02:24:30.877Z
2025-05-31 19:24:30,892 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 3 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}} | Client Time: 2025-06-01T02:24:30.877Z
2025-05-31 19:24:30,893 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 5 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}} | Client Time: 2025-06-01T02:24:30.877Z
2025-05-31 19:24:30,894 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 6 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.877Z
2025-05-31 19:24:30,897 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 7 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}} | Client Time: 2025-06-01T02:24:30.877Z
2025-05-31 19:24:30,901 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 8 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.878Z
2025-05-31 19:24:30,902 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 9 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}} | Client Time: 2025-06-01T02:24:30.878Z
2025-05-31 19:24:30,905 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 10 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}} | Client Time: 2025-06-01T02:24:30.878Z
2025-05-31 19:24:30,906 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 11 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}} | Client Time: 2025-06-01T02:24:30.879Z
2025-05-31 19:24:30,907 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 12 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}} | Client Time: 2025-06-01T02:24:30.879Z
2025-05-31 19:24:30,908 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 13 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.879Z
2025-05-31 19:24:30,909 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 14 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.879Z
2025-05-31 19:24:30,911 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 15 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}} | Client Time: 2025-06-01T02:24:30.879Z
2025-05-31 19:24:30,911 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 16 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}} | Client Time: 2025-06-01T02:24:30.879Z
2025-05-31 19:24:30,912 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 17 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}} | Client Time: 2025-06-01T02:24:30.880Z
2025-05-31 19:24:30,913 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 18 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}} | Client Time: 2025-06-01T02:24:30.880Z
2025-05-31 19:24:30,914 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 19 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.880Z
2025-05-31 19:24:30,914 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 20 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.880Z
2025-05-31 19:24:30,915 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 21 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.880Z
2025-05-31 19:24:30,917 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 22 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}} | Client Time: 2025-06-01T02:24:30.880Z
2025-05-31 19:24:30,919 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 23 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}} | Client Time: 2025-06-01T02:24:30.881Z
2025-05-31 19:24:30,920 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 24 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}} | Client Time: 2025-06-01T02:24:30.881Z
2025-05-31 19:24:30,920 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 25 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}} | Client Time: 2025-06-01T02:24:30.881Z
2025-05-31 19:24:30,921 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 26 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}} | Client Time: 2025-06-01T02:24:30.881Z
2025-05-31 19:24:30,922 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 27 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.881Z
2025-05-31 19:24:30,922 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 28 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}} | Client Time: 2025-06-01T02:24:30.881Z
2025-05-31 19:24:30,923 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 29 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}} | Client Time: 2025-06-01T02:24:30.881Z
2025-05-31 19:24:30,924 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 30 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}} | Client Time: 2025-06-01T02:24:30.881Z
2025-05-31 19:24:30,926 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 31 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.882Z
2025-05-31 19:24:30,927 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 32 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}} | Client Time: 2025-06-01T02:24:30.882Z
2025-05-31 19:24:30,928 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 33 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}} | Client Time: 2025-06-01T02:24:30.882Z
2025-05-31 19:24:30,929 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 34 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.882Z
2025-05-31 19:24:30,930 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 35 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}} | Client Time: 2025-06-01T02:24:30.882Z
2025-05-31 19:24:30,930 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 36 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}} | Client Time: 2025-06-01T02:24:30.882Z
2025-05-31 19:24:30,931 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 37 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}} | Client Time: 2025-06-01T02:24:30.882Z
2025-05-31 19:24:30,932 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 38 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}} | Client Time: 2025-06-01T02:24:30.883Z
2025-05-31 19:24:30,933 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 39 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}} | Client Time: 2025-06-01T02:24:30.883Z
2025-05-31 19:24:30,934 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 40 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}} | Client Time: 2025-06-01T02:24:30.883Z
2025-05-31 19:24:30,935 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 41 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}} | Client Time: 2025-06-01T02:24:30.883Z
2025-05-31 19:24:30,936 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 42 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}} | Client Time: 2025-06-01T02:24:30.883Z
2025-05-31 19:24:30,936 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 43 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning for Reinforcement Learning', 'venue': 'Machine Learning 2005'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,937 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 44 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.06866', 'title': 'Exploration with Bayesian Neural Networks for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,937 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 45 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1901.08753', 'title': 'Deep Exploration via Bootstrapped DQN'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,939 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 46 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning', 'venue': 'NeurIPS 2019'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,940 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 47 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Variational Bayesian Reinforcement Learning', 'venue': 'AISTATS 2018'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,940 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 48 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,941 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 49 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Deep Exploration via Bootstrapped DQN', 'venue': 'NeurIPS 2016'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,941 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 50 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Variational Information Bottleneck for Reinforcement Learning', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,943 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 51 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Model Learning', 'venue': 'arXiv:1802.08941'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,943 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 52 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'arXiv:1606.04670'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,944 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 53 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning for Deep Reinforcement Learning', 'venue': 'arXiv:1804.08474'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,946 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 54 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Based Active Exploration', 'venue': 'arXiv:1703.07604'}} | Client Time: 2025-06-01T02:24:30.884Z
2025-05-31 19:24:30,947 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 65 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model Soups: Averaging Weights Leads to Wider Optima and Better Generalization', 'venue': 'NeurIPS 2022'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,948 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 66 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Knowledge Distillation', 'venue': 'NIPS 2015'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,949 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 67 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1812.00424', 'title': 'Elastic Weight Consolidation'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,949 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 68 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1606.07792', 'title': 'Learning without Forgetting'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,950 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 69 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Continual Learning via Modular Networks with Overlapping Representation', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,951 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 70 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2012.00822', 'title': 'Task-Agnostic Continual Learning via Meta-Learning'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,953 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 71 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.08943', 'title': 'MERLIN: Parameter-Efficient Transfer Learning for NLP'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,953 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 72 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Distilling Knowledge from Ensembles of Neural Networks', 'venue': 'CVPR 2014'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,953 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 73 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,954 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 74 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,955 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 75 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,956 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 76 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,956 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 77 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,957 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 78 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,958 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 79 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,959 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 80 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,960 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 81 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,960 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 82 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,961 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 83 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}} | Client Time: 2025-06-01T02:24:30.885Z
2025-05-31 19:24:30,963 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 84 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,964 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 85 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,964 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 86 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,965 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 87 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,966 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 88 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,968 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 89 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,968 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 90 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,969 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 91 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,970 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 92 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,971 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 93 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,971 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 94 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,973 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 95 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,974 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 96 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,974 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 97 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,975 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 98 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,976 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 99 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,976 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 100 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,977 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 101 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,978 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 102 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,978 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 103 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,979 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 104 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,981 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 105 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,981 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 106 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}} | Client Time: 2025-06-01T02:24:30.886Z
2025-05-31 19:24:30,982 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 107 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,982 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 108 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,983 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 109 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,984 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 110 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,985 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 111 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,986 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 112 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,987 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 113 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,988 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 114 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,989 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 115 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,989 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 116 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,990 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 117 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,990 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 118 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,991 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 119 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,992 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 120 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,993 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 121 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,994 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 122 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1604.00077', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,995 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 123 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204503', 'title': 'Intrinsic Motivation for Hierarchical Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,995 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 124 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,996 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 125 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2019'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,997 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 126 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1511.09292', 'title': 'Learning to Predict the Internal State of Agents'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:30,998 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 127 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Unsupervised Meta-Learning for Few-Shot Image Classification', 'venue': 'ICLR 2017'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:31,000 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 128 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Intrinsic Motivation via Prediction Gain', 'venue': 'International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011'}} | Client Time: 2025-06-01T02:24:30.887Z
2025-05-31 19:24:31,001 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 129 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,003 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 130 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Emergent Communication in Multi-Agent Systems', 'venue': 'Foundations and Trends in Machine Learning, 2020'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,004 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 131 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Communication-Efficient Distributed Deep Reinforcement Learning', 'arXiv_ID': '1802.01528'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,004 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 132 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'TarMAC: Targeted Multi-Agent Communication', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,005 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 133 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'AAMAS 2020'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,005 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 134 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,005 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 135 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'venue': 'NeurIPS 2017'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,006 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 136 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,007 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 137 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Emergent Communication in Multi-Agent Systems: A Survey', 'venue': 'Journal of Artificial Intelligence Research 72 (2021) 1-93'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,009 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 138 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,010 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 139 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1605.06673', 'comment': 'Foerster et al. - Learning to Communicate with Deep Multi-Agent Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,010 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 140 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.08780', 'comment': 'Lazaridou et al. - Emergence of Linguistic Communication from Referential Games'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,011 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 141 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Exploration', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,012 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 142 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Based Communication for Multi-Agent Cooperation', 'venue': 'AAMAS 2020'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,013 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 143 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Neuro-Symbolic Reinforcement Learning: A Survey', 'venue': 'arXiv preprint arXiv:2010.06278'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,013 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 144 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning Programs for Abstraction and Reasoning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,013 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 145 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'ICLR 2017'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,014 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 146 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'SRL: Symbolic Reinforcement Learning', 'venue': 'International Conference on Automated Planning and Scheduling (ICAPS) 2018'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,014 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 147 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.09689'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,015 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 148 | Data: {'error': 'str.replace is not a function', 'reference': {'DOI': '10.1109/ICRA46639.2022.9811713'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,016 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 149 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'NeurIPS 2017'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,017 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 150 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Neuro-Symbolic Reinforcement Learning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,017 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 151 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning Symbolic Representations for Generalization in Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR) 2018'}} | Client Time: 2025-06-01T02:24:30.888Z
2025-05-31 19:24:31,018 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 152 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'Association for Computational Linguistics (ACL) 2017'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,018 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 153 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'International Conference on Learning Representations (ICLR) 2017'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,019 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 154 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Relational Reinforcement Learning', 'venue': 'Machine Learning, 57(1-3), 5-43, 2004'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,020 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 155 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.06275', 'title': 'Symbolic Planning with Neural Reward Functions'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,020 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 156 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2203.02071', 'title': 'Learning to Abstract and Reason with Symbolic Planning'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,021 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 157 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'STRIPS Learning through Reinforcement', 'venue': 'AAAI Conference on Artificial Intelligence (AAAI), 2023'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,022 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 158 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1905.10976', 'title': 'Meta-Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,022 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 159 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1707.04140', 'title': 'Teacher-Student Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,023 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 160 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460635', 'title': 'Automatic Curriculum Learning for Deep RL: A Case Study on Robotic Manipulation', 'venue': 'ICRA 2018'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,025 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 161 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Curricula via Paired Open-Ended Trailblazing', 'venue': 'GECCO 2019'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,026 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 162 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2010.08453', 'title': 'Self-Supervised Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,027 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 163 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Curriculum Learning by Transferring Latent Task Structures', 'venue': 'NeurIPS 2020'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,027 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 164 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1810.11903', 'title': 'Meta-Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,028 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 165 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1707.04170', 'title': 'Curriculum Learning by Prediction-Error Minimization'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,029 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 166 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204463', 'title': 'Autonomous Curriculum Generation for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,029 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 167 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'POET: End-to-End Parametric Optimization through Evolution', 'venue': 'Genetic and Evolutionary Computation Conference (GECCO) 2019'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,030 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 168 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Teacher-Student Curriculum Learning', 'venue': 'Neural Information Processing Systems (NeurIPS) 2016'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,031 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 169 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1802.09562', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,033 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 170 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Neural Networks', 'venue': 'Advances in Neural Information Processing Systems (NeurIPS)', 'year': 1991}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,034 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 171 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning Literature Survey', 'venue': 'ACM Computing Surveys', 'year': 2009}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,035 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 172 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML)', 'year': 2017}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,036 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 173 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Curriculum Learning', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'year': 2009}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,037 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 174 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Explore via Meta-Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR)', 'year': 2017}} | Client Time: 2025-06-01T02:24:30.889Z
2025-05-31 19:24:31,037 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 175 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Efficient Exploration through Bayesian Deep Reinforcement Learning', 'venue': 'Conference on Robot Learning (CoRL)', 'year': 2018}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,038 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 176 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Meta-Curriculum Learning for Domain Adaptation', 'venue': 'International Conference on Computer Vision (ICCV)', 'year': 2021}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,038 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 177 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.04849', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,039 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 178 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2006.05961', 'title': 'Uncertainty-Aware Reinforcement Learning for Safe Exploration'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,040 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 179 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.02762', 'title': 'Meta-Learning for Curriculum Design'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,042 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 180 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Neural Networks', 'venue': 'Review Article, e.g., Journal of Machine Learning Research'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,043 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 181 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning for Regression', 'venue': 'Journal of Machine Learning Research'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,043 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 182 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'ICML 2017'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,045 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 183 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Meta-Curriculum Learning', 'venue': 'NeurIPS 2019'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,046 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 184 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Exploration by Random Network Distillation', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,047 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 185 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.09471', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,048 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 186 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Uncertainty-Aware Deep Reinforcement Learning', 'venue': 'AAAI 2018'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,048 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 187 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'AISTATS 2016'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,049 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 188 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Curriculum Learning', 'venue': 'A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,051 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 189 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Self-Paced Learning for Latent Variable Models', 'venue': 'NIPS 2010'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,051 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 190 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Neural Networks', 'venue': 'Advances in Neural Information Processing Systems (NeurIPS)', 'year': 1991}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,052 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 191 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning Literature Survey', 'venue': 'ACM Computing Surveys', 'year': 2009}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,053 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 192 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML)', 'year': 2017}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,054 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 193 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Curriculum Learning', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'year': 2009}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,055 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 194 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Explore via Meta-Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR)', 'year': 2017}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,056 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 195 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Efficient Exploration through Bayesian Deep Reinforcement Learning', 'venue': 'Conference on Robot Learning (CoRL)', 'year': 2018}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,057 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 196 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Meta-Curriculum Learning for Domain Adaptation', 'venue': 'International Conference on Computer Vision (ICCV)', 'year': 2021}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,057 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 197 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.04849', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,058 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 198 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2006.05961', 'title': 'Uncertainty-Aware Reinforcement Learning for Safe Exploration'}} | Client Time: 2025-06-01T02:24:30.890Z
2025-05-31 19:24:31,059 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 199 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.02762', 'title': 'Meta-Learning for Curriculum Design'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,060 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 200 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Neural Networks', 'venue': 'Review Article, e.g., Journal of Machine Learning Research'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,060 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 201 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning for Regression', 'venue': 'Journal of Machine Learning Research'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,060 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 202 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'ICML 2017'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,061 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 203 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Meta-Curriculum Learning', 'venue': 'NeurIPS 2019'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,062 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 204 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Exploration by Random Network Distillation', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,062 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 205 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.09471', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,063 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 206 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Uncertainty-Aware Deep Reinforcement Learning', 'venue': 'AAAI 2018'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,063 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 207 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'AISTATS 2016'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,064 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 208 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Curriculum Learning', 'venue': 'A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,065 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 209 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Self-Paced Learning for Latent Variable Models', 'venue': 'NIPS 2010'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,066 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 210 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Neuro-Symbolic Reinforcement Learning: A Survey', 'venue': 'arXiv preprint arXiv:2010.06278'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,067 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 211 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning Programs for Abstraction and Reasoning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,067 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 212 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'ICLR 2017'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,068 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 213 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'SRL: Symbolic Reinforcement Learning', 'venue': 'International Conference on Automated Planning and Scheduling (ICAPS) 2018'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,069 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 214 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.09689'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,071 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 215 | Data: {'error': 'str.replace is not a function', 'reference': {'DOI': '10.1109/ICRA46639.2022.9811713'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,072 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 216 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'NeurIPS 2017'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,073 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 217 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Neuro-Symbolic Reinforcement Learning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,074 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 218 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning Symbolic Representations for Generalization in Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR) 2018'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,075 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 219 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'Association for Computational Linguistics (ACL) 2017'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,076 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 220 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'International Conference on Learning Representations (ICLR) 2017'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,077 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 221 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Relational Reinforcement Learning', 'venue': 'Machine Learning, 57(1-3), 5-43, 2004'}} | Client Time: 2025-06-01T02:24:30.891Z
2025-05-31 19:24:31,078 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 222 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.06275', 'title': 'Symbolic Planning with Neural Reward Functions'}} | Client Time: 2025-06-01T02:24:30.892Z
2025-05-31 19:24:31,080 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 223 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2203.02071', 'title': 'Learning to Abstract and Reason with Symbolic Planning'}} | Client Time: 2025-06-01T02:24:30.892Z
2025-05-31 19:24:31,081 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 224 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'STRIPS Learning through Reinforcement', 'venue': 'AAAI Conference on Artificial Intelligence (AAAI), 2023'}} | Client Time: 2025-06-01T02:24:30.892Z
2025-05-31 19:24:31,082 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 225 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.892Z
2025-05-31 19:24:31,083 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 226 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}} | Client Time: 2025-06-01T02:24:30.892Z
2025-05-31 19:24:31,084 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 227 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}} | Client Time: 2025-06-01T02:24:30.892Z
2025-05-31 19:24:31,084 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 228 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}} | Client Time: 2025-06-01T02:24:30.892Z
2025-05-31 19:24:31,085 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 229 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}} | Client Time: 2025-06-01T02:24:30.892Z
2025-05-31 19:24:31,086 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 230 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,087 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 231 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,087 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 232 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,088 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 233 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,088 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 234 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning for Reinforcement Learning', 'venue': 'Machine Learning 2005'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,089 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 235 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.06866', 'title': 'Exploration with Bayesian Neural Networks for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,089 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 236 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1901.08753', 'title': 'Deep Exploration via Bootstrapped DQN'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,090 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 237 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning', 'venue': 'NeurIPS 2019'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,091 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 238 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Variational Bayesian Reinforcement Learning', 'venue': 'AISTATS 2018'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,091 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 239 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,092 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 240 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Deep Exploration via Bootstrapped DQN', 'venue': 'NeurIPS 2016'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,093 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 241 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Variational Information Bottleneck for Reinforcement Learning', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,093 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 242 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Model Learning', 'venue': 'arXiv:1802.08941'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,094 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 243 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'arXiv:1606.04670'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,094 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 244 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning for Deep Reinforcement Learning', 'venue': 'arXiv:1804.08474'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,095 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 245 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Based Active Exploration', 'venue': 'arXiv:1703.07604'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,096 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 246 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model Soups: Averaging Weights Leads to Wider Optima and Better Generalization', 'venue': 'NeurIPS 2022'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,096 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 247 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Knowledge Distillation', 'venue': 'NIPS 2015'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,097 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 248 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1812.00424', 'title': 'Elastic Weight Consolidation'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,098 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 249 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1606.07792', 'title': 'Learning without Forgetting'}} | Client Time: 2025-06-01T02:24:30.893Z
2025-05-31 19:24:31,099 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 250 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Continual Learning via Modular Networks with Overlapping Representation', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,100 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 251 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2012.00822', 'title': 'Task-Agnostic Continual Learning via Meta-Learning'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,102 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 252 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.08943', 'title': 'MERLIN: Parameter-Efficient Transfer Learning for NLP'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,103 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 253 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Distilling Knowledge from Ensembles of Neural Networks', 'venue': 'CVPR 2014'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,103 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 254 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,104 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 255 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,106 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 256 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,106 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 257 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,107 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 258 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,108 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 259 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,108 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 260 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,110 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 261 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,112 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 262 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,114 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 263 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,114 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 264 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,116 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 265 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,116 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 266 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,117 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 267 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,117 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 268 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,118 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 269 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,120 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 270 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,121 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 271 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,122 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 272 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}} | Client Time: 2025-06-01T02:24:30.894Z
2025-05-31 19:24:31,123 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 273 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,124 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 274 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,126 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 275 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1905.10976', 'title': 'Meta-Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,127 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 276 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1707.04140', 'title': 'Teacher-Student Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,128 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 277 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460635', 'title': 'Automatic Curriculum Learning for Deep RL: A Case Study on Robotic Manipulation', 'venue': 'ICRA 2018'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,129 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 278 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Curricula via Paired Open-Ended Trailblazing', 'venue': 'GECCO 2019'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,129 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 279 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2010.08453', 'title': 'Self-Supervised Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,129 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 280 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Curriculum Learning by Transferring Latent Task Structures', 'venue': 'NeurIPS 2020'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,131 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 281 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1810.11903', 'title': 'Meta-Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,132 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 282 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1707.04170', 'title': 'Curriculum Learning by Prediction-Error Minimization'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,134 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 283 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204463', 'title': 'Autonomous Curriculum Generation for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,134 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 284 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'POET: End-to-End Parametric Optimization through Evolution', 'venue': 'Genetic and Evolutionary Computation Conference (GECCO) 2019'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,135 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 285 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Teacher-Student Curriculum Learning', 'venue': 'Neural Information Processing Systems (NeurIPS) 2016'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,136 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 286 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1802.09562', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,137 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 287 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,139 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 288 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,140 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 289 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,142 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 290 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,142 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 291 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,144 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 292 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,145 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 293 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,145 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 294 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1604.00077', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,145 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 295 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204503', 'title': 'Intrinsic Motivation for Hierarchical Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,146 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 296 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.895Z
2025-05-31 19:24:31,147 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 297 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2019'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,149 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 298 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1511.09292', 'title': 'Learning to Predict the Internal State of Agents'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,149 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 299 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Unsupervised Meta-Learning for Few-Shot Image Classification', 'venue': 'ICLR 2017'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,151 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 300 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Intrinsic Motivation via Prediction Gain', 'venue': 'International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,153 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 301 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,153 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 302 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,154 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 303 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,154 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 304 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,155 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 305 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,157 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 306 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,159 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 307 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,160 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 308 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,161 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 309 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,161 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 310 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,162 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 311 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,162 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 312 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,164 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 313 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,165 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 314 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,166 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 315 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Emergent Communication in Multi-Agent Systems', 'venue': 'Foundations and Trends in Machine Learning, 2020'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,167 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 316 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Communication-Efficient Distributed Deep Reinforcement Learning', 'arXiv_ID': '1802.01528'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,168 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 317 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'TarMAC: Targeted Multi-Agent Communication', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,169 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 318 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'AAMAS 2020'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,170 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 319 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,171 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 320 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'venue': 'NeurIPS 2017'}} | Client Time: 2025-06-01T02:24:30.896Z
2025-05-31 19:24:31,171 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 321 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}} | Client Time: 2025-06-01T02:24:30.897Z
2025-05-31 19:24:31,172 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 322 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Emergent Communication in Multi-Agent Systems: A Survey', 'venue': 'Journal of Artificial Intelligence Research 72 (2021) 1-93'}} | Client Time: 2025-06-01T02:24:30.897Z
2025-05-31 19:24:31,172 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 323 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.897Z
2025-05-31 19:24:31,173 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 324 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1605.06673', 'comment': 'Foerster et al. - Learning to Communicate with Deep Multi-Agent Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.897Z
2025-05-31 19:24:31,175 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 325 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.08780', 'comment': 'Lazaridou et al. - Emergence of Linguistic Communication from Referential Games'}} | Client Time: 2025-06-01T02:24:30.897Z
2025-05-31 19:24:31,175 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 326 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Exploration', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.897Z
2025-05-31 19:24:31,178 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 327 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Based Communication for Multi-Agent Cooperation', 'venue': 'AAMAS 2020'}} | Client Time: 2025-06-01T02:24:30.897Z
2025-05-31 19:24:31,178 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 340 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Neural Networks', 'venue': 'Advances in Neural Information Processing Systems (NeurIPS)', 'year': 1991}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,178 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 341 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning Literature Survey', 'venue': 'ACM Computing Surveys', 'year': 2009}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,179 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 342 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML)', 'year': 2017}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,181 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 343 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Curriculum Learning', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'year': 2009}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,181 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 344 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Explore via Meta-Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR)', 'year': 2017}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,182 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 345 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Efficient Exploration through Bayesian Deep Reinforcement Learning', 'venue': 'Conference on Robot Learning (CoRL)', 'year': 2018}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,199 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 346 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Meta-Curriculum Learning for Domain Adaptation', 'venue': 'International Conference on Computer Vision (ICCV)', 'year': 2021}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,199 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 347 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.04849', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,205 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 348 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2006.05961', 'title': 'Uncertainty-Aware Reinforcement Learning for Safe Exploration'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,205 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 349 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.02762', 'title': 'Meta-Learning for Curriculum Design'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,205 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 350 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Neural Networks', 'venue': 'Review Article, e.g., Journal of Machine Learning Research'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,206 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 351 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning for Regression', 'venue': 'Journal of Machine Learning Research'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,207 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 352 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'ICML 2017'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,210 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 353 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Meta-Curriculum Learning', 'venue': 'NeurIPS 2019'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,211 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 354 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Exploration by Random Network Distillation', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,212 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 355 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.09471', 'title': 'Variational Information Bottleneck for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,212 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 356 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Uncertainty-Aware Deep Reinforcement Learning', 'venue': 'AAAI 2018'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,215 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 357 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'AISTATS 2016'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,216 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 358 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Curriculum Learning', 'venue': 'A comprehensive survey, e.g., IEEE Transactions on Neural Networks and Learning Systems'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,218 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 359 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Self-Paced Learning for Latent Variable Models', 'venue': 'NIPS 2010'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,218 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 360 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1602.06070', 'title': 'Hierarchical Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,220 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 361 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1802.09403', 'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'}} | Client Time: 2025-06-01T02:24:30.898Z
2025-05-31 19:24:31,222 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 362 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Hindsight Experience Replay', 'venue': 'NeurIPS 2017'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,223 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 363 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Data-Efficient Hierarchical Reinforcement Learning', 'venue': 'ICML 2019'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,224 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 364 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Learning latent options', 'venue': 'ICML 2016'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,225 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 365 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2007.14950', 'title': 'Contrastive unsupervised representation learning with noise for reinforcement learning'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,226 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 366 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2106.04487', 'title': 'Pre-training Representations for Data-Efficient Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,227 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 367 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/LRA.2021.3051378', 'title': 'Skill-Based Hierarchical Reinforcement Learning for Complex Manipulation Tasks'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,229 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 368 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1604.00772', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,231 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 369 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2013.6673656', 'title': 'Intrinsic Motivation for Hierarchical Skill Acquisition'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,232 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 370 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,234 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 371 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,235 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 372 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Learning Options via Spectral Clustering', 'venue': 'NeurIPS 2007'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,236 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 373 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'venue': 'ICML 2017'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,236 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 374 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Self-Supervised Hindsight Planning', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,237 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 375 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1604.00077', 'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,238 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 376 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204503', 'title': 'Intrinsic Motivation for Hierarchical Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,238 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 377 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'HAC: Hierarchical Actor-Critic', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,238 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 378 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Variational Option Discovery using Hierarchical Intrinsic Rewards', 'venue': 'ICML 2019'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,239 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 379 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1511.09292', 'title': 'Learning to Predict the Internal State of Agents'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,239 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 380 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Unsupervised Meta-Learning for Few-Shot Image Classification', 'venue': 'ICLR 2017'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,240 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 381 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Intrinsic Motivation via Prediction Gain', 'venue': 'International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) 2011'}} | Client Time: 2025-06-01T02:24:30.899Z
2025-05-31 19:24:31,242 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 382 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Neuro-Symbolic Reinforcement Learning: A Survey', 'venue': 'arXiv preprint arXiv:2010.06278'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,242 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 383 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning Programs for Abstraction and Reasoning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,242 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 384 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'ICLR 2017'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,243 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 385 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'SRL: Symbolic Reinforcement Learning', 'venue': 'International Conference on Automated Planning and Scheduling (ICAPS) 2018'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,244 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 386 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.09689'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,244 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 387 | Data: {'error': 'str.replace is not a function', 'reference': {'DOI': '10.1109/ICRA46639.2022.9811713'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,246 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 388 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'NeurIPS 2017'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,246 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 389 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Neuro-Symbolic Reinforcement Learning', 'venue': 'Advances in Neural Information Processing Systems 31 (NeurIPS 2018)'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,247 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 390 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning Symbolic Representations for Generalization in Reinforcement Learning', 'venue': 'International Conference on Learning Representations (ICLR) 2018'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,248 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 391 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Grounded Language Learning in a Simulated 3D World', 'venue': 'Association for Computational Linguistics (ACL) 2017'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,249 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 392 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Differentiable Reasoning over Symbolic Knowledge', 'venue': 'International Conference on Learning Representations (ICLR) 2017'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,251 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 393 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Relational Reinforcement Learning', 'venue': 'Machine Learning, 57(1-3), 5-43, 2004'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,252 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 394 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.06275', 'title': 'Symbolic Planning with Neural Reward Functions'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,254 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 395 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2203.02071', 'title': 'Learning to Abstract and Reason with Symbolic Planning'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,254 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 396 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'STRIPS Learning through Reinforcement', 'venue': 'AAAI Conference on Artificial Intelligence (AAAI), 2023'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,255 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 397 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,255 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 398 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning for Reinforcement Learning', 'venue': 'Machine Learning 2005'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,256 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 399 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.06866', 'title': 'Exploration with Bayesian Neural Networks for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,257 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 400 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1901.08753', 'title': 'Deep Exploration via Bootstrapped DQN'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,259 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 401 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Uncertainty-Aware Value Functions for Off-Policy Reinforcement Learning', 'venue': 'NeurIPS 2019'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,259 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 402 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Variational Bayesian Reinforcement Learning', 'venue': 'AISTATS 2018'}} | Client Time: 2025-06-01T02:24:30.900Z
2025-05-31 19:24:31,260 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 403 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Bayesian Deep Reinforcement Learning', 'venue': 'ICML 2016'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,260 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 404 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Deep Exploration via Bootstrapped DQN', 'venue': 'NeurIPS 2016'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,262 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 405 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Variational Information Bottleneck for Reinforcement Learning', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,263 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 406 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Model Learning', 'venue': 'arXiv:1802.08941'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,265 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 407 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Efficient Exploration through Bayesian Deep Q-Networks', 'venue': 'arXiv:1606.04670'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,266 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 408 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Active Learning for Deep Reinforcement Learning', 'venue': 'arXiv:1804.08474'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,268 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 409 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Based Active Exploration', 'venue': 'arXiv:1703.07604'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,268 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 410 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1511.00421', 'title': 'Distilling the Knowledge in a Neural Network'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,268 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 411 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.48550/arXiv.1706.08974', 'title': 'Adversarial Training Methods for Semi-Supervised Text Classification', 'venue': 'ICLR 2017'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,269 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 412 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2106.05237', 'title': 'Self-Distillation as Instance-Specific Label Smoothing'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,269 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 413 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Regularizing Neural Networks with Parameter-Dependent Noise', 'venue': 'NIPS 2016'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,270 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 414 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Improving Generalization via Scalable Neighborhood Mixing', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,271 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 415 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper', 'title': 'Curriculum by Smoothing', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,272 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 416 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model Soups: Averaging Weights Leads to Wider Optima and Better Generalization', 'venue': 'NeurIPS 2022'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,272 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 417 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Knowledge Distillation', 'venue': 'NIPS 2015'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,274 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 418 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1812.00424', 'title': 'Elastic Weight Consolidation'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,274 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 419 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1606.07792', 'title': 'Learning without Forgetting'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,274 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 420 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Continual Learning via Modular Networks with Overlapping Representation', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,277 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 421 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2012.00822', 'title': 'Task-Agnostic Continual Learning via Meta-Learning'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,277 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 422 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '2106.08943', 'title': 'MERLIN: Parameter-Efficient Transfer Learning for NLP'}} | Client Time: 2025-06-01T02:24:30.901Z
2025-05-31 19:24:31,278 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 423 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Distilling Knowledge from Ensembles of Neural Networks', 'venue': 'CVPR 2014'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,278 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 424 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1905.10976', 'title': 'Meta-Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,279 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 425 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1707.04140', 'title': 'Teacher-Student Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,281 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 426 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460635', 'title': 'Automatic Curriculum Learning for Deep RL: A Case Study on Robotic Manipulation', 'venue': 'ICRA 2018'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,284 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 427 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Curricula via Paired Open-Ended Trailblazing', 'venue': 'GECCO 2019'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,285 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 428 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '2010.08453', 'title': 'Self-Supervised Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,286 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 429 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Curriculum Learning by Transferring Latent Task Structures', 'venue': 'NeurIPS 2020'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,287 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 430 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1810.11903', 'title': 'Meta-Curriculum Learning'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,288 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 431 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1707.04170', 'title': 'Curriculum Learning by Prediction-Error Minimization'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,289 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 432 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICDL-EpiRob.2017.8204463', 'title': 'Autonomous Curriculum Generation for Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,289 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 433 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'POET: End-to-End Parametric Optimization through Evolution', 'venue': 'Genetic and Evolutionary Computation Conference (GECCO) 2019'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,292 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 434 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title', 'title': 'Teacher-Student Curriculum Learning', 'venue': 'Neural Information Processing Systems (NeurIPS) 2016'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,292 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 435 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1802.09562', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,293 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 436 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1703.03400', 'comment': 'Meta-Learning with Memory-Augmented Neural Networks'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,295 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 437 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1606.01545', 'comment': 'Overcoming Catastrophic Forgetting in Neural Networks'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,295 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 438 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'DOI', 'id': '10.1109/ICRA.2018.8460811', 'comment': 'Lifelong Robotic Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,296 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 439 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title & Venue', 'title': 'Learning without Forgetting', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,297 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 440 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'Paper Title & Venue', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'venue': 'International Conference on Machine Learning (ICML), 2017'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,298 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 441 | Data: {'error': 'str.replace is not a function', 'reference': {'type': 'arXiv', 'id': '1802.07592', 'comment': 'Regularizing Representations to Generalize in Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,298 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 442 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Curiosity-driven Exploration by Self-supervised Prediction', 'venue': 'ICML 2017'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,299 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 443 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Exploration via Disagreement', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,299 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 444 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Variational Intrinsic Control', 'venue': 'ICLR 2016'}} | Client Time: 2025-06-01T02:24:30.902Z
2025-05-31 19:24:31,300 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 445 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Multi-Modal Deep Reinforcement Learning', 'venue': 'IJCAI 2016'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,302 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 446 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Explore via Meta-Curiosity', 'venue': 'ICLR 2018'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,303 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 447 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'A Survey of Exploration in Reinforcement Learning', 'venue': 'Journal of Artificial Intelligence Research 2020'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,305 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 448 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Diversity is All You Need: Learning Skills without Rewards', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,307 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 449 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Episodic Curiosity through Reachability', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,308 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 450 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Meta-Curriculum Learning', 'venue': 'ICML 2020'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,309 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 451 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Hierarchical Meta-Reinforcement Learning', 'venue': 'NeurIPS 2018'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,312 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 452 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Teach with Dynamic Loss Functions', 'venue': 'ICLR 2018'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,313 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 453 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1705.10410', 'title': 'Learning to Learn by Gradient Descent by Gradient Descent'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,316 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 454 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1606.04474', 'title': 'Optimization as a Model for Few-Shot Learning'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,317 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 455 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Automatic Curriculum Learning for Deep RL: A Survey', 'venue': 'Journal of Artificial Intelligence Research 2022'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,318 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 456 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Self-Supervised Curriculum Learning', 'venue': 'ICLR 2020'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,320 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 457 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,320 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 458 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Emergent Communication in Multi-Agent Systems', 'venue': 'Foundations and Trends in Machine Learning, 2020'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,322 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 459 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Communication-Efficient Distributed Deep Reinforcement Learning', 'arXiv_ID': '1802.01528'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,322 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 460 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'TarMAC: Targeted Multi-Agent Communication', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,322 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 461 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'AAMAS 2020'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,324 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 462 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,325 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 463 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'venue': 'NeurIPS 2017'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,325 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 464 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Communicate with Deep Multi-Agent Reinforcement Learning', 'venue': 'NeurIPS 2016'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,326 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 465 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Emergent Communication in Multi-Agent Systems: A Survey', 'venue': 'Journal of Artificial Intelligence Research 72 (2021) 1-93'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,328 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 466 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Learning to Share in Multi-Agent Reinforcement Learning', 'venue': 'ICML 2018'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,329 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 467 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1605.06673', 'comment': 'Foerster et al. - Learning to Communicate with Deep Multi-Agent Reinforcement Learning'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,330 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 468 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1802.08780', 'comment': 'Lazaridou et al. - Emergence of Linguistic Communication from Referential Games'}} | Client Time: 2025-06-01T02:24:30.903Z
2025-05-31 19:24:31,331 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 469 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Social Influence as Intrinsic Motivation for Multi-Agent Exploration', 'venue': 'ICLR 2019'}} | Client Time: 2025-06-01T02:24:30.904Z
2025-05-31 19:24:31,331 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 470 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Model-Based Communication for Multi-Agent Cooperation', 'venue': 'AAMAS 2020'}} | Client Time: 2025-06-01T02:24:30.904Z
2025-05-31 19:24:31,331 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 471 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Trust Region Policy Optimization', 'venue': 'International Conference on Machine Learning (ICML), 2015'}} | Client Time: 2025-06-01T02:24:30.904Z
2025-05-31 19:24:31,332 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 472 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Proximal Policy Optimization Algorithms', 'venue': 'arXiv preprint arXiv:1707.06347'}} | Client Time: 2025-06-01T02:24:30.904Z
2025-05-31 19:24:31,335 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 473 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Policy Distillation', 'venue': 'International Conference on Knowledge Discovery and Data Mining (KDD), 2006'}} | Client Time: 2025-06-01T02:24:30.904Z
2025-05-31 19:24:31,337 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 474 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1606.05256', 'title': 'Explaining and Harnessing Adversarial Examples'}} | Client Time: 2025-06-01T02:24:30.904Z
2025-05-31 19:24:31,339 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 475 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1602.04938', 'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation'}} | Client Time: 2025-06-01T02:24:30.904Z
2025-05-31 19:24:31,339 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 476 | Data: {'error': 'str.replace is not a function', 'reference': {'arXiv_ID': '1810.00633', 'title': 'Interpretable Policies for Reinforcement Learning by Extracting Decision Trees'}} | Client Time: 2025-06-01T02:24:30.904Z
2025-05-31 19:24:31,340 ERROR aicoscientist: [FRONTEND-ERROR] Error processing additional reference 477 | Data: {'error': 'str.replace is not a function', 'reference': {'title': 'Explainable Reinforcement Learning: A Survey', 'venue': 'Journal of Artificial Intelligence Research (JAIR), 2022'}} | Client Time: 2025-06-01T02:24:30.904Z
2025-05-31 19:24:31,340 INFO aicoscientist: [FRONTEND-INFO] Additional references processing completed | Client Time: 2025-06-01T02:24:30.904Z
2025-05-31 19:24:31,344 INFO aicoscientist: [FRONTEND-INFO] displayReferences function completed successfully | Client Time: 2025-06-01T02:24:31.343Z
2025-05-31 19:24:31,346 INFO aicoscientist: [FRONTEND-INFO] References section update completed successfully | Client Time: 2025-06-01T02:24:31.345Z
